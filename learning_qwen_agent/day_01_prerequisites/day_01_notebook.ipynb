{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1: Prerequisites & Setup\n",
    "\n",
    "## Welcome to Qwen-Agent! üéâ\n",
    "\n",
    "### Today's Learning Objectives:\n",
    "1. Understand what LLM agents are and why they're powerful\n",
    "2. Learn the Qwen-Agent architecture at a high level\n",
    "3. Set up your development environment\n",
    "4. Configure API access (DashScope or OpenAI-compatible)\n",
    "5. Run your first agent and understand streaming vs. non-streaming\n",
    "\n",
    "### Time Required: 1.5-2 hours\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: What Are LLM Agents?\n",
    "\n",
    "### Traditional LLM Usage:\n",
    "```\n",
    "You ‚Üí Prompt ‚Üí LLM ‚Üí Response ‚Üí You\n",
    "```\n",
    "\n",
    "**Limitations:**\n",
    "- LLM has no access to real-world data\n",
    "- Cannot perform actions (run code, search web, etc.)\n",
    "- Limited to knowledge cutoff date\n",
    "- No memory beyond current conversation\n",
    "\n",
    "### LLM Agent Approach:\n",
    "```\n",
    "You ‚Üí Agent ‚Üí LLM ‚Üê‚Üí Tools (Code, Search, APIs, etc.)\n",
    "              ‚Üì\n",
    "            Response\n",
    "```\n",
    "\n",
    "**Capabilities:**\n",
    "- **Tool Use**: Can execute code, search web, query databases\n",
    "- **Planning**: Breaks complex tasks into steps\n",
    "- **Memory**: Maintains context across conversations\n",
    "- **Reasoning**: ReAct pattern (Reason ‚Üí Act ‚Üí Observe ‚Üí Repeat)\n",
    "\n",
    "### Why Qwen-Agent?\n",
    "1. **Built for Qwen models** - Optimized for Qwen3, QwQ, Qwen-VL families\n",
    "2. **Production-ready** - Powers Qwen Chat (chat.qwen.ai)\n",
    "3. **Flexible** - Works with any OpenAI-compatible API\n",
    "4. **Feature-rich** - RAG, multi-agent, GUI included\n",
    "5. **Well-documented** - Active community and examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Qwen-Agent Architecture Overview\n",
    "\n",
    "### Core Components:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ           Qwen-Agent Framework          ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                         ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
    "‚îÇ  ‚îÇ  Agents  ‚îÇ  ‚îÇ   Tools  ‚îÇ  ‚îÇ LLMs ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
    "‚îÇ       ‚îÇ              ‚îÇ           ‚îÇ      ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
    "‚îÇ  ‚îÇ     Message Communication Layer    ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
    "‚îÇ                                         ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ\n",
    "‚îÇ  ‚îÇ  Memory  ‚îÇ  ‚îÇ   GUI    ‚îÇ            ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ\n",
    "‚îÇ                                         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "#### 1. **Agents** (High-level orchestration)\n",
    "   - `BasicAgent` - Simple LLM wrapper\n",
    "   - `Assistant` - Full-featured agent with tools + RAG\n",
    "   - `FnCallAgent` - Function calling specialist\n",
    "   - `GroupChat` - Multi-agent coordinator\n",
    "   - Custom agents - You build your own!\n",
    "\n",
    "#### 2. **Tools** (Extend agent capabilities)\n",
    "   - Built-in: `code_interpreter`, `web_search`, `image_gen`\n",
    "   - Custom: You define your own tools\n",
    "   - MCP: Community-built tool servers\n",
    "\n",
    "#### 3. **LLMs** (Language model backends)\n",
    "   - DashScope (Qwen models via API)\n",
    "   - OpenAI-compatible (vLLM, Ollama, OpenAI)\n",
    "   - Local models (transformers, OpenVINO)\n",
    "\n",
    "#### 4. **Messages** (Communication protocol)\n",
    "   - Standardized message format\n",
    "   - Supports text, images, audio, video\n",
    "   - Function call representation\n",
    "\n",
    "#### 5. **Memory** (Context management)\n",
    "   - Conversation history\n",
    "   - RAG knowledge base\n",
    "   - File storage\n",
    "\n",
    "#### 6. **GUI** (User interface)\n",
    "   - Gradio-based web UI\n",
    "   - Rapid prototyping\n",
    "   - Production deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Environment Setup\n",
    "\n",
    "### Step 1: Check Python Version\n",
    "Qwen-Agent requires Python 3.8+, but GUI features need 3.10+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Version info: {sys.version_info}\")\n",
    "\n",
    "# Check if version is sufficient\n",
    "if sys.version_info >= (3, 10):\n",
    "    print(\"‚úÖ Python 3.10+ detected - All features available!\")\n",
    "elif sys.version_info >= (3, 8):\n",
    "    print(\"‚ö†Ô∏è  Python 3.8-3.9 detected - GUI features not available\")\n",
    "else:\n",
    "    print(\"‚ùå Python version too old - Please upgrade to 3.10+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Install Qwen-Agent\n",
    "\n",
    "There are two installation options:\n",
    "\n",
    "#### Option A: Minimal Installation (Function calling only)\n",
    "```bash\n",
    "pip install qwen-agent\n",
    "```\n",
    "\n",
    "#### Option B: Full Installation (Recommended for this course)\n",
    "```bash\n",
    "pip install -U \"qwen-agent[gui,rag,code_interpreter,mcp]\"\n",
    "```\n",
    "\n",
    "**What each feature provides:**\n",
    "- `[gui]` - Gradio web interface (requires Python 3.10+)\n",
    "- `[rag]` - Document processing and retrieval\n",
    "- `[code_interpreter]` - Python code execution\n",
    "- `[mcp]` - Model Context Protocol support\n",
    "\n",
    "Let's check if it's already installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import qwen_agent\n",
    "    print(f\"‚úÖ Qwen-Agent is installed!\")\n",
    "    print(f\"Version: {qwen_agent.__version__ if hasattr(qwen_agent, '__version__') else 'Unknown'}\")\n",
    "    print(f\"Location: {qwen_agent.__file__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Qwen-Agent not found. Please run:\")\n",
    "    print('   pip install -U \"qwen-agent[gui,rag,code_interpreter,mcp]\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Verify Key Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core dependencies\n",
    "dependencies = {\n",
    "    'dashscope': 'DashScope API client',\n",
    "    'openai': 'OpenAI API client',\n",
    "    'pydantic': 'Data validation',\n",
    "    'tiktoken': 'Token counting',\n",
    "    'json5': 'JSON parsing',\n",
    "}\n",
    "\n",
    "optional_dependencies = {\n",
    "    'gradio': 'Web UI (requires Python 3.10+)',\n",
    "    'jupyter': 'Code interpreter',\n",
    "    'pandas': 'Data processing',\n",
    "}\n",
    "\n",
    "print(\"Core Dependencies:\")\n",
    "for package, description in dependencies.items():\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"  ‚úÖ {package:15} - {description}\")\n",
    "    except ImportError:\n",
    "        print(f\"  ‚ùå {package:15} - {description} (MISSING)\")\n",
    "\n",
    "print(\"\\nOptional Dependencies:\")\n",
    "for package, description in optional_dependencies.items():\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"  ‚úÖ {package:15} - {description}\")\n",
    "    except ImportError:\n",
    "        print(f\"  ‚ö†Ô∏è  {package:15} - {description} (optional)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: API Configuration\n",
    "\n",
    "### Option A: DashScope (Recommended for Qwen models)\n",
    "\n",
    "DashScope is Alibaba Cloud's model service platform. It provides:\n",
    "- Official Qwen models (Qwen3, QwQ, Qwen-VL, etc.)\n",
    "- Pay-as-you-go pricing\n",
    "- Free tier available\n",
    "- No local GPU required\n",
    "\n",
    "**Get your API key:**\n",
    "1. Visit: https://dashscope.console.aliyun.com/\n",
    "2. Sign up / Log in\n",
    "3. Go to API-KEY management\n",
    "4. Create new key\n",
    "\n",
    "**Set up your key:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Method 1: Set environment variable (recommended)\n",
    "# Uncomment and replace with your actual key:\n",
    "# os.environ['DASHSCOPE_API_KEY'] = 'your-api-key-here'\n",
    "\n",
    "# Method 2: Load from .env file (even better!)\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()  # Loads from .env file in current directory\n",
    "    print(\"‚úÖ Loaded .env file\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  python-dotenv not installed (optional)\")\n",
    "\n",
    "# Check if API key is set\n",
    "if os.getenv('DASHSCOPE_API_KEY'):\n",
    "    key = os.getenv('DASHSCOPE_API_KEY')\n",
    "    print(f\"‚úÖ DashScope API key found: {key[:10]}...{key[-5:]}\")\n",
    "else:\n",
    "    print(\"‚ùå DASHSCOPE_API_KEY not set\")\n",
    "    print(\"   Set it with: os.environ['DASHSCOPE_API_KEY'] = 'your-key'\")\n",
    "    print(\"   Or create a .env file with: DASHSCOPE_API_KEY=your-key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: OpenAI-Compatible Services\n",
    "\n",
    "You can also use:\n",
    "- **OpenAI** - Official API (GPT-4, GPT-3.5, etc.)\n",
    "- **vLLM** - Self-hosted high-performance inference\n",
    "- **Ollama** - Local CPU/GPU inference\n",
    "- **Azure OpenAI** - Microsoft's OpenAI service\n",
    "\n",
    "Example configurations:\n",
    "\n",
    "```python\n",
    "# OpenAI\n",
    "llm_cfg = {\n",
    "    'model': 'gpt-4o',\n",
    "    'model_type': 'oai',\n",
    "    'api_key': os.getenv('OPENAI_API_KEY')\n",
    "}\n",
    "\n",
    "# vLLM (self-hosted)\n",
    "llm_cfg = {\n",
    "    'model': 'Qwen2.5-7B-Instruct',\n",
    "    'model_server': 'http://localhost:8000/v1',\n",
    "    'api_key': 'EMPTY'\n",
    "}\n",
    "\n",
    "# Ollama (local)\n",
    "llm_cfg = {\n",
    "    'model': 'qwen2.5:7b',\n",
    "    'model_server': 'http://localhost:11434/v1',\n",
    "    'api_key': 'EMPTY'\n",
    "}\n",
    "```\n",
    "\n",
    "For this course, we'll use **DashScope** for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Your First Agent - \"Hello World\"\n",
    "\n",
    "Let's create the simplest possible agent to verify everything works.\n",
    "\n",
    "### Understanding the Code:\n",
    "1. Import the `Assistant` class (a pre-built agent)\n",
    "2. Configure which LLM to use\n",
    "3. Create an agent instance\n",
    "4. Send messages and receive responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwen_agent.agents import Assistant\n",
    "\n",
    "# Step 1: Configure the LLM\n",
    "llm_cfg = {\n",
    "    'model': 'qwen-max-latest',  # Use the latest Qwen model\n",
    "    # API key will be read from DASHSCOPE_API_KEY environment variable\n",
    "}\n",
    "\n",
    "# Step 2: Create an agent\n",
    "bot = Assistant(llm=llm_cfg)\n",
    "\n",
    "# Step 3: Prepare a message\n",
    "messages = [\n",
    "    {'role': 'user', 'content': 'Hello! What is 2+2?'}\n",
    "]\n",
    "\n",
    "# Step 4: Get response (non-streaming)\n",
    "print(\"Running agent...\\n\")\n",
    "response = bot.run_nonstream(messages=messages)\n",
    "\n",
    "# Step 5: Display the response\n",
    "for msg in response:\n",
    "    print(f\"[{msg['role']}]: {msg.get('content', '')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéâ If you see a response above, congratulations!\n",
    "\n",
    "You've successfully:\n",
    "- Installed Qwen-Agent\n",
    "- Configured API access\n",
    "- Created your first agent\n",
    "- Sent a message and received a response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Streaming vs. Non-Streaming Responses\n",
    "\n",
    "### What's the difference?\n",
    "\n",
    "**Non-Streaming (`run_nonstream`):**\n",
    "- Waits for complete response\n",
    "- Returns all messages at once\n",
    "- Simpler to use\n",
    "- User waits longer\n",
    "\n",
    "**Streaming (`run`):**\n",
    "- Returns response incrementally\n",
    "- Better user experience (like ChatGPT)\n",
    "- More complex to handle\n",
    "- Lower perceived latency\n",
    "\n",
    "### Visual Comparison:\n",
    "\n",
    "```\n",
    "Non-Streaming:\n",
    "[User waits...........................] ‚Üí Full response appears\n",
    "\n",
    "Streaming:\n",
    "[User waits..] ‚Üí \"The\" ‚Üí \"answer\" ‚Üí \"is\" ‚Üí \"4\" ‚Üí \".\"\n",
    "```\n",
    "\n",
    "Let's see streaming in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create a new message\n",
    "messages = [\n",
    "    {'role': 'user', 'content': 'Write a short poem about AI agents.'}\n",
    "]\n",
    "\n",
    "print(\"Streaming response:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Use run() instead of run_nonstream()\n",
    "for response in bot.run(messages=messages):\n",
    "    # Each iteration gives us updated messages\n",
    "    # The last message is the assistant's response\n",
    "    if response and response[-1]['role'] == 'assistant':\n",
    "        content = response[-1].get('content', '')\n",
    "        # Clear previous line and print updated content\n",
    "        print(f\"\\r{content}\", end='', flush=True)\n",
    "        time.sleep(0.05)  # Small delay to see streaming effect\n",
    "\n",
    "print()  # New line after streaming completes\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Streaming Works:\n",
    "\n",
    "```python\n",
    "for response in bot.run(messages):\n",
    "    # response is a List[Message]\n",
    "    # Each iteration may have:\n",
    "    # - Partial text (incomplete thought)\n",
    "    # - Function calls (tool usage)\n",
    "    # - Function results (tool outputs)\n",
    "    # - Complete response (final iteration)\n",
    "```\n",
    "\n",
    "The `run()` method returns an **iterator** that yields progressively complete message lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Understanding Message Structure\n",
    "\n",
    "Messages are the core communication unit in Qwen-Agent.\n",
    "\n",
    "### Basic Message Format:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'role': 'user',      # Who sent this message\n",
    "    'content': 'Hello'   # What the message says\n",
    "}\n",
    "```\n",
    "\n",
    "### Role Types:\n",
    "- `'user'` - Input from the human user\n",
    "- `'assistant'` - Response from the agent/LLM\n",
    "- `'system'` - Instructions for the agent\n",
    "- `'function'` - Results from tool execution\n",
    "\n",
    "Let's explore different message types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: User message\n",
    "user_msg = {\n",
    "    'role': 'user',\n",
    "    'content': 'What is the capital of France?'\n",
    "}\n",
    "\n",
    "# Example 2: System message (gives agent instructions)\n",
    "system_msg = {\n",
    "    'role': 'system',\n",
    "    'content': 'You are a helpful geography tutor. Keep answers brief.'\n",
    "}\n",
    "\n",
    "# Example 3: Multi-turn conversation\n",
    "conversation = [\n",
    "    {'role': 'user', 'content': 'Hi! What is 10 + 5?'},\n",
    "    {'role': 'assistant', 'content': 'Hello! 10 + 5 equals 15.'},\n",
    "    {'role': 'user', 'content': 'What about multiplying them?'},\n",
    "    # Agent uses context from previous messages to know \"them\" = 10 and 5\n",
    "]\n",
    "\n",
    "print(\"Message Examples:\")\n",
    "print(f\"User: {user_msg}\")\n",
    "print(f\"System: {system_msg}\")\n",
    "print(f\"\\nConversation: {conversation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing System Messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent with system message\n",
    "pirate_bot = Assistant(\n",
    "    llm=llm_cfg,\n",
    "    system_message='You are a friendly pirate. Always respond in pirate speak!'\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {'role': 'user', 'content': 'What is your name?'}\n",
    "]\n",
    "\n",
    "response = pirate_bot.run_nonstream(messages=messages)\n",
    "print(\"Pirate Bot Response:\")\n",
    "for msg in response:\n",
    "    if msg['role'] == 'assistant':\n",
    "        print(msg['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Multi-Turn Conversations\n",
    "\n",
    "Real conversations have multiple back-and-forth exchanges. The agent maintains context by keeping message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize conversation history\n",
    "messages = []\n",
    "\n",
    "# Create a simple agent\n",
    "chat_bot = Assistant(llm=llm_cfg)\n",
    "\n",
    "# Helper function for cleaner output\n",
    "def chat(user_input):\n",
    "    \"\"\"Send a message and get response\"\"\"\n",
    "    global messages\n",
    "    \n",
    "    # Add user message\n",
    "    messages.append({'role': 'user', 'content': user_input})\n",
    "    print(f\"\\nüë§ User: {user_input}\")\n",
    "    \n",
    "    # Get agent response\n",
    "    response = chat_bot.run_nonstream(messages=messages)\n",
    "    \n",
    "    # Display assistant's response\n",
    "    for msg in response:\n",
    "        if msg['role'] == 'assistant':\n",
    "            print(f\"ü§ñ Agent: {msg['content']}\")\n",
    "    \n",
    "    # Update conversation history\n",
    "    messages.extend(response)\n",
    "\n",
    "# Simulate a conversation\n",
    "print(\"Starting conversation...\")\n",
    "chat(\"My name is Alex.\")\n",
    "chat(\"What's the weather like today?\")\n",
    "chat(\"What's my name?\")  # Tests if agent remembers context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Context Works:\n",
    "\n",
    "```python\n",
    "messages = []\n",
    "\n",
    "# Turn 1\n",
    "messages = [{'role': 'user', 'content': 'I like pizza'}]\n",
    "response = bot.run_nonstream(messages)\n",
    "messages.extend(response)  # Now messages has user + assistant messages\n",
    "\n",
    "# Turn 2\n",
    "messages.append({'role': 'user', 'content': 'What do I like?'})\n",
    "# Agent sees full history: [turn1_user, turn1_assistant, turn2_user]\n",
    "response = bot.run_nonstream(messages)\n",
    "```\n",
    "\n",
    "The agent **always** receives the full message history, so it can reference earlier parts of the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Different LLM Configurations\n",
    "\n",
    "Let's explore various ways to configure the LLM backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration 1: Basic (uses defaults)\n",
    "basic_cfg = {\n",
    "    'model': 'qwen-max-latest'\n",
    "}\n",
    "\n",
    "# Configuration 2: With generation parameters\n",
    "creative_cfg = {\n",
    "    'model': 'qwen-max-latest',\n",
    "    'generate_cfg': {\n",
    "        'top_p': 0.9,        # Nucleus sampling (higher = more creative)\n",
    "        'temperature': 1.0,  # Not always supported, check model docs\n",
    "    }\n",
    "}\n",
    "\n",
    "# Configuration 3: With token limits\n",
    "efficient_cfg = {\n",
    "    'model': 'qwen-max-latest',\n",
    "    'generate_cfg': {\n",
    "        'max_input_tokens': 6000,  # Truncate if input too long\n",
    "    }\n",
    "}\n",
    "\n",
    "# Configuration 4: Different model (smaller/faster)\n",
    "fast_cfg = {\n",
    "    'model': 'qwen-turbo-latest',  # Faster, cheaper, less capable\n",
    "}\n",
    "\n",
    "print(\"LLM Configurations:\")\n",
    "print(f\"Basic: {basic_cfg}\")\n",
    "print(f\"Creative: {creative_cfg}\")\n",
    "print(f\"Efficient: {efficient_cfg}\")\n",
    "print(f\"Fast: {fast_cfg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Different Models:\n",
    "\n",
    "Let's test the same prompt with different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "prompt = \"In exactly 10 words, explain what an AI agent is.\"\n",
    "\n",
    "models_to_test = [\n",
    "    ('qwen-max-latest', 'Most capable'),\n",
    "    ('qwen-turbo-latest', 'Faster, cheaper'),\n",
    "]\n",
    "\n",
    "for model_name, description in models_to_test:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Model: {model_name} ({description})\")\n",
    "    print('='*60)\n",
    "    \n",
    "    cfg = {'model': model_name}\n",
    "    test_bot = Assistant(llm=cfg)\n",
    "    \n",
    "    start = time.time()\n",
    "    response = test_bot.run_nonstream([{'role': 'user', 'content': prompt}])\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    for msg in response:\n",
    "        if msg['role'] == 'assistant':\n",
    "            print(f\"Response: {msg['content']}\")\n",
    "    \n",
    "    print(f\"Time: {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Practice Exercises\n",
    "\n",
    "Now it's your turn! Complete these exercises to solidify your understanding.\n",
    "\n",
    "### Exercise 1: Create a Math Tutor\n",
    "Create an agent with a system message that makes it act as a patient math tutor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a math tutor agent\n",
    "# Requirements:\n",
    "# 1. System message should instruct it to be a patient math tutor\n",
    "# 2. It should explain concepts step-by-step\n",
    "# 3. Test it with a math question\n",
    "\n",
    "# Your code here:\n",
    "math_tutor = None  # Replace with your agent\n",
    "\n",
    "# Test it:\n",
    "# question = \"How do I calculate the area of a circle?\"\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement a Simple Chatbot Loop\n",
    "Create an interactive chatbot that takes input until the user types 'quit'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a chatbot loop\n",
    "# Requirements:\n",
    "# 1. Maintain conversation history\n",
    "# 2. Accept user input\n",
    "# 3. Exit when user types 'quit'\n",
    "# 4. Print agent responses\n",
    "\n",
    "# Note: In Jupyter, input() works but might be awkward\n",
    "# You can test with a predefined list of inputs instead\n",
    "\n",
    "# Your code here:\n",
    "def chatbot_loop():\n",
    "    \"\"\"Interactive chatbot\"\"\"\n",
    "    # Your implementation\n",
    "    pass\n",
    "\n",
    "# Uncomment to test:\n",
    "# chatbot_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Compare Streaming Performance\n",
    "Measure the time to first token for streaming vs. total time for non-streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare streaming vs non-streaming\n",
    "# Measure:\n",
    "# 1. Time to first token (streaming)\n",
    "# 2. Total time to completion (both)\n",
    "# 3. Print comparison\n",
    "\n",
    "import time\n",
    "\n",
    "test_prompt = \"Explain quantum computing in simple terms.\"\n",
    "\n",
    "# Your code here:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Key Takeaways\n",
    "\n",
    "### What You Learned Today:\n",
    "\n",
    "1. **LLM Agents vs. Direct LLM Usage**\n",
    "   - Agents add planning, tool use, and memory\n",
    "   - More powerful for complex tasks\n",
    "\n",
    "2. **Qwen-Agent Architecture**\n",
    "   - Agents (orchestration)\n",
    "   - Tools (capabilities)\n",
    "   - LLMs (reasoning)\n",
    "   - Messages (communication)\n",
    "\n",
    "3. **Environment Setup**\n",
    "   - Install with pip\n",
    "   - Configure API keys\n",
    "   - Verify dependencies\n",
    "\n",
    "4. **Basic Agent Usage**\n",
    "   - `Assistant` class for general use\n",
    "   - `run_nonstream()` for simple cases\n",
    "   - `run()` for streaming responses\n",
    "\n",
    "5. **Message Structure**\n",
    "   - Role + content format\n",
    "   - Message history for context\n",
    "   - System messages for instructions\n",
    "\n",
    "6. **LLM Configuration**\n",
    "   - Model selection\n",
    "   - Generation parameters\n",
    "   - Different backends\n",
    "\n",
    "### Common Patterns:\n",
    "\n",
    "```python\n",
    "# Pattern 1: One-shot question\n",
    "bot = Assistant(llm={'model': 'qwen-max-latest'})\n",
    "response = bot.run_nonstream([{'role': 'user', 'content': 'Question?'}])\n",
    "\n",
    "# Pattern 2: Streaming response\n",
    "for chunk in bot.run(messages):\n",
    "    # Process incremental response\n",
    "    pass\n",
    "\n",
    "# Pattern 3: Multi-turn conversation\n",
    "messages = []\n",
    "messages.append({'role': 'user', 'content': 'Hi'})\n",
    "response = bot.run_nonstream(messages)\n",
    "messages.extend(response)\n",
    "# Continue conversation...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Next Steps\n",
    "\n",
    "### Tomorrow (Day 2): Message Schema Deep Dive\n",
    "We'll explore:\n",
    "- The `Message` class in detail\n",
    "- Multimodal content (text + images)\n",
    "- ContentItem structure\n",
    "- Function call messages (preview)\n",
    "- Building complex message structures\n",
    "\n",
    "### Homework:\n",
    "1. Experiment with different system messages\n",
    "2. Try different Qwen models (qwen-max, qwen-turbo, etc.)\n",
    "3. Build a simple persona-based chatbot (e.g., \"tech support agent\")\n",
    "4. Read the official docs: `/docs/agent.md`\n",
    "\n",
    "### Resources:\n",
    "- [Qwen-Agent GitHub](https://github.com/QwenLM/Qwen-Agent)\n",
    "- [DashScope Models](https://help.aliyun.com/zh/dashscope/)\n",
    "- [Qwen Documentation](https://qwen.readthedocs.io/)\n",
    "\n",
    "### Troubleshooting:\n",
    "- **API key errors**: Check environment variable is set\n",
    "- **Import errors**: Verify installation with `pip show qwen-agent`\n",
    "- **Slow responses**: Try `qwen-turbo-latest` for faster replies\n",
    "- **GUI not working**: Requires Python 3.10+\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've completed Day 1! You now have:\n",
    "- ‚úÖ A working Qwen-Agent installation\n",
    "- ‚úÖ Understanding of basic concepts\n",
    "- ‚úÖ Ability to create and use simple agents\n",
    "- ‚úÖ Knowledge of streaming vs. non-streaming\n",
    "\n",
    "See you tomorrow for Day 2! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
