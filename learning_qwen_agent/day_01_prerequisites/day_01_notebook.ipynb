{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Day 1: Prerequisites & First Steps with Qwen-Agent\n",
    "\n",
    "## üéâ Welcome to Your Qwen-Agent Learning Journey!\n",
    "\n",
    "### What You'll Learn Today:\n",
    "1. **What are LLM Agents?** - Understanding the power beyond simple chatbots\n",
    "2. **Qwen-Agent Architecture** - How everything fits together\n",
    "3. **Environment Setup** - Getting your development environment ready\n",
    "4. **First Agent** - Run your first working example\n",
    "5. **Thinking Models** - Special feature of Qwen3 235B Thinking model\n",
    "6. **Streaming Responses** - Real-time vs batch responses\n",
    "\n",
    "### Time Required: 1.5-2 hours\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-intro",
   "metadata": {},
   "source": [
    "## Part 1: What Are LLM Agents?\n",
    "\n",
    "### Let's Start with an Analogy:\n",
    "\n",
    "Imagine you're a manager:\n",
    "- **Traditional LLM**: You have an expert consultant who can *only* give advice based on their knowledge\n",
    "- **LLM Agent**: You have an assistant who can:\n",
    "  - Give advice (like the consultant)\n",
    "  - Use tools (calculator, internet, run code)\n",
    "  - Remember previous conversations\n",
    "  - Break complex tasks into steps\n",
    "  - Learn from results and try again\n",
    "\n",
    "### Traditional LLM Flow:\n",
    "```\n",
    "You: \"What's the weather in Paris?\"\n",
    " ‚Üì\n",
    "LLM: \"I don't have access to real-time weather data.\"\n",
    " ‚Üì\n",
    "(Dead end - LLM can't help)\n",
    "```\n",
    "\n",
    "### LLM Agent Flow:\n",
    "```\n",
    "You: \"What's the weather in Paris?\"\n",
    " ‚Üì\n",
    "Agent: \"I'll check the weather API for you\"\n",
    " ‚Üì\n",
    "Agent uses weather API tool\n",
    " ‚Üì\n",
    "Agent: \"It's currently 18¬∞C and partly cloudy in Paris!\"\n",
    " ‚Üì\n",
    "(Success - Agent solved the problem)\n",
    "```\n",
    "\n",
    "### Why Qwen-Agent?\n",
    "\n",
    "**Qwen-Agent** is a framework that makes building these \"AI assistants\" easy:\n",
    "\n",
    "1. **üß† Built for Qwen Models** - Optimized for Qwen3, QwQ (reasoning models)\n",
    "2. **üè≠ Production-Ready** - Powers Qwen's own chat service (chat.qwen.ai)\n",
    "3. **üîß Tool System** - Easy to add capabilities (code execution, web search, custom tools)\n",
    "4. **üìö RAG Support** - Built-in document intelligence\n",
    "5. **üé® GUI Included** - Beautiful web interface with zero effort\n",
    "6. **üåç Flexible** - Works with any OpenAI-compatible API\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architecture",
   "metadata": {},
   "source": [
    "## Part 2: Qwen-Agent Architecture\n",
    "\n",
    "### The Big Picture:\n",
    "\n",
    "Think of Qwen-Agent as a toolkit with different modules:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ         üèóÔ∏è  Qwen-Agent Framework                ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                 ‚îÇ\n",
    "‚îÇ   üë• AGENTS (The Brains)                        ‚îÇ\n",
    "‚îÇ   ‚îú‚îÄ Assistant      ‚Üí All-in-one agent         ‚îÇ\n",
    "‚îÇ   ‚îú‚îÄ FnCallAgent    ‚Üí Function calling expert  ‚îÇ\n",
    "‚îÇ   ‚îú‚îÄ GroupChat      ‚Üí Multi-agent teams        ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ Custom Agents  ‚Üí You build your own!      ‚îÇ\n",
    "‚îÇ                                                 ‚îÇ\n",
    "‚îÇ   üîß TOOLS (The Capabilities)                   ‚îÇ\n",
    "‚îÇ   ‚îú‚îÄ code_interpreter ‚Üí Run Python code        ‚îÇ\n",
    "‚îÇ   ‚îú‚îÄ doc_parser       ‚Üí Parse PDFs/docs        ‚îÇ\n",
    "‚îÇ   ‚îú‚îÄ image_gen        ‚Üí Generate images        ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ Custom Tools     ‚Üí Your own tools         ‚îÇ\n",
    "‚îÇ                                                 ‚îÇ\n",
    "‚îÇ   ü§ñ LLM BACKENDS (The Intelligence)           ‚îÇ\n",
    "‚îÇ   ‚îú‚îÄ DashScope API   ‚Üí Official Qwen models    ‚îÇ\n",
    "‚îÇ   ‚îú‚îÄ Fireworks AI    ‚Üí Fast inference (we use) ‚îÇ\n",
    "‚îÇ   ‚îú‚îÄ OpenAI API      ‚Üí GPT models              ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ Local (vLLM)    ‚Üí Self-hosted             ‚îÇ\n",
    "‚îÇ                                                 ‚îÇ\n",
    "‚îÇ   üí¨ MESSAGES (The Communication)              ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ Standardized format for all interactions  ‚îÇ\n",
    "‚îÇ                                                 ‚îÇ\n",
    "‚îÇ   üìù MEMORY (The Context)                       ‚îÇ\n",
    "‚îÇ   ‚îú‚îÄ Conversation history                      ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ RAG knowledge base                        ‚îÇ\n",
    "‚îÇ                                                 ‚îÇ\n",
    "‚îÇ   üé® GUI (The Interface)                        ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ Gradio web UI (just 1 line of code!)      ‚îÇ\n",
    "‚îÇ                                                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### How They Work Together:\n",
    "\n",
    "1. **You** send a message via the GUI or code\n",
    "2. **Agent** receives it and decides what to do\n",
    "3. **LLM** analyzes the message and plans actions\n",
    "4. **Tools** execute actions (if needed)\n",
    "5. **Memory** keeps track of everything\n",
    "6. **Agent** sends back the final answer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-intro",
   "metadata": {},
   "source": [
    "## Part 3: Environment Setup\n",
    "\n",
    "Let's make sure everything is ready to go!\n",
    "\n",
    "### Step 1: Check Python Version\n",
    "\n",
    "Qwen-Agent requires:\n",
    "- **Python 3.8+** for basic features\n",
    "- **Python 3.10+** for GUI features (recommended)\n",
    "\n",
    "Let's check your version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PYTHON VERSION CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìç Python version: {sys.version}\")\n",
    "print(f\"üìç Version info: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")\n",
    "\n",
    "# Check requirements\n",
    "if sys.version_info >= (3, 10):\n",
    "    print(\"\\n‚úÖ Perfect! Python 3.10+ detected\")\n",
    "    print(\"   All features available (including GUI)\")\n",
    "elif sys.version_info >= (3, 8):\n",
    "    print(\"\\n‚ö†Ô∏è  Python 3.8-3.9 detected\")\n",
    "    print(\"   Basic features work, but GUI requires 3.10+\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Python version too old!\")\n",
    "    print(\"   Please upgrade to Python 3.10+\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-qwen-agent",
   "metadata": {},
   "source": [
    "### Step 2: Verify Qwen-Agent Installation\n",
    "\n",
    "#### Installation Command (if needed):\n",
    "\n",
    "```bash\n",
    "# Full installation with all features\n",
    "pip install -U \"qwen-agent[gui,rag,code_interpreter,mcp]\"\n",
    "```\n",
    "\n",
    "**What each feature provides:**\n",
    "- `[gui]` - Gradio web interface\n",
    "- `[rag]` - Document processing and retrieval\n",
    "- `[code_interpreter]` - Python code execution capability\n",
    "- `[mcp]` - Model Context Protocol support\n",
    "\n",
    "Let's check if it's installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"QWEN-AGENT INSTALLATION CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    import qwen_agent\n",
    "    print(\"\\n‚úÖ Qwen-Agent is installed!\")\n",
    "    \n",
    "    # Try to get version\n",
    "    version = getattr(qwen_agent, '__version__', 'Unknown')\n",
    "    print(f\"   Version: {version}\")\n",
    "    print(f\"   Location: {qwen_agent.__file__}\")\n",
    "    \n",
    "    # Check key imports\n",
    "    print(\"\\nüì¶ Checking key components:\")\n",
    "    from qwen_agent.agents import Assistant\n",
    "    print(\"   ‚úÖ Assistant class available\")\n",
    "    \n",
    "    from qwen_agent.llm import BaseChatModel\n",
    "    print(\"   ‚úÖ LLM classes available\")\n",
    "    \n",
    "    try:\n",
    "        from qwen_agent.gui import WebUI\n",
    "        print(\"   ‚úÖ WebUI available (GUI features enabled)\")\n",
    "    except ImportError:\n",
    "        print(\"   ‚ö†Ô∏è  WebUI not available (requires Python 3.10+)\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(\"\\n‚ùå Qwen-Agent not found!\")\n",
    "    print(\"\\n   Please install with:\")\n",
    "    print('   pip install -U \"qwen-agent[gui,rag,code_interpreter,mcp]\"')\n",
    "    print(f\"\\n   Error details: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check-deps",
   "metadata": {},
   "source": [
    "### Step 3: Verify Dependencies\n",
    "\n",
    "Let's make sure all required packages are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-dependencies",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DEPENDENCY CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Core dependencies\n",
    "core_deps = {\n",
    "    'openai': 'OpenAI API client (for API calls)',\n",
    "    'pydantic': 'Data validation',\n",
    "    'requests': 'HTTP requests',\n",
    "    'json5': 'JSON parsing',\n",
    "}\n",
    "\n",
    "# Optional but useful\n",
    "optional_deps = {\n",
    "    'python-dotenv': 'Load environment variables from .env',\n",
    "    'gradio': 'Web UI (requires Python 3.10+)',\n",
    "    'pandas': 'Data processing',\n",
    "    'numpy': 'Numerical computing',\n",
    "}\n",
    "\n",
    "print(\"\\nüì¶ Core Dependencies:\")\n",
    "for package, description in core_deps.items():\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f\"   ‚úÖ {package:20} - {description}\")\n",
    "    except ImportError:\n",
    "        print(f\"   ‚ùå {package:20} - {description} (MISSING - REQUIRED!)\")\n",
    "\n",
    "print(\"\\nüì¶ Optional Dependencies:\")\n",
    "for package, description in optional_deps.items():\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f\"   ‚úÖ {package:20} - {description}\")\n",
    "    except ImportError:\n",
    "        print(f\"   ‚ö†Ô∏è  {package:20} - {description} (optional)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "api-config-intro",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: API Configuration - Fireworks AI\n",
    "\n",
    "### Why Fireworks AI?\n",
    "\n",
    "For this course, we're using **Fireworks AI** because:\n",
    "- ‚úÖ **Fast inference** - Optimized for speed\n",
    "- ‚úÖ **Qwen3 235B Thinking model** - Latest reasoning model\n",
    "- ‚úÖ **OpenAI-compatible** - Works with standard tools\n",
    "- ‚úÖ **Pay-as-you-go** - Only pay for what you use ($0.22/1M input tokens)\n",
    "\n",
    "### API Configuration:\n",
    "\n",
    "```python\n",
    "Model: accounts/fireworks/models/qwen3-235b-a22b-thinking-2507\n",
    "Endpoint: https://api.fireworks.ai/inference/v1\n",
    "```\n",
    "\n",
    "### Best Practice: Use .env File\n",
    "\n",
    "**Why `.env` file?**\n",
    "- ‚úÖ Keeps API keys secure\n",
    "- ‚úÖ Never accidentally commit keys to git\n",
    "- ‚úÖ Easy to manage different environments\n",
    "\n",
    "**Our `.env` file contains:**\n",
    "```\n",
    "FIREWORKS_API_KEY=fw_3ZSpUnVR78vs38jJtyewjcWk\n",
    "```\n",
    "\n",
    "Let's load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-env",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"API CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Method 1: Load from .env file (BEST PRACTICE)\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    \n",
    "    # Load .env from project root\n",
    "    env_path = '/home/user/Qwen-Agent/.env'\n",
    "    if load_dotenv(env_path):\n",
    "        print(f\"\\n‚úÖ Loaded .env file from: {env_path}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  .env file not found at: {env_path}\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  python-dotenv not installed\")\n",
    "    print(\"   Install with: pip install python-dotenv\")\n",
    "    \n",
    "    # Fallback: Set directly (NOT recommended for production)\n",
    "    print(\"\\n   Setting API key directly (fallback)...\")\n",
    "    os.environ['FIREWORKS_API_KEY'] = 'fw_3ZSpUnVR78vs38jJtyewjcWk'\n",
    "\n",
    "# Verify API key is loaded\n",
    "api_key = os.getenv('FIREWORKS_API_KEY')\n",
    "if api_key:\n",
    "    # Show partial key for security\n",
    "    print(f\"\\nüîë API Key loaded: {api_key[:15]}...{api_key[-10:]}\")\n",
    "    print(f\"   Length: {len(api_key)} characters\")\n",
    "else:\n",
    "    print(\"\\n‚ùå FIREWORKS_API_KEY not set!\")\n",
    "    print(\"   Please check your .env file\")\n",
    "\n",
    "print(\"\\nüì° API Details:\")\n",
    "print(\"   Model: Qwen3-235B-A22B-Thinking-2507\")\n",
    "print(\"   Provider: Fireworks AI\")\n",
    "print(\"   Endpoint: https://api.fireworks.ai/inference/v1\")\n",
    "print(\"   Type: OpenAI-compatible\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-agent-intro",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Your First Agent! üéâ\n",
    "\n",
    "### The Moment of Truth\n",
    "\n",
    "Let's create your first AI agent and make it do something!\n",
    "\n",
    "### What We're Doing:\n",
    "\n",
    "1. **Import** the `Assistant` class - pre-built agent from Qwen-Agent\n",
    "2. **Configure** which LLM to use - Fireworks Qwen3 235B Thinking\n",
    "3. **Create** an agent instance - your AI assistant\n",
    "4. **Send** a message - ask it to do something\n",
    "5. **Get** response - see what it says!\n",
    "\n",
    "### Simple Example First:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-agent-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwen_agent.agents import Assistant\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CREATING YOUR FIRST AGENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Configure the LLM\n",
    "print(\"\\nüìù Step 1: Configuring LLM...\")\n",
    "llm_cfg = {\n",
    "    'model': 'accounts/fireworks/models/qwen3-235b-a22b-thinking-2507',\n",
    "    'model_server': 'https://api.fireworks.ai/inference/v1',\n",
    "    'api_key': os.environ['FIREWORKS_API_KEY'],\n",
    "    'generate_cfg': {\n",
    "        'max_tokens': 2048,\n",
    "        'temperature': 0.6  # 0=deterministic, 1=creative\n",
    "    }\n",
    "}\n",
    "print(\"   ‚úÖ LLM configuration ready\")\n",
    "\n",
    "# Step 2: Create an agent\n",
    "print(\"\\nüìù Step 2: Creating Assistant agent...\")\n",
    "bot = Assistant(llm=llm_cfg)\n",
    "print(\"   ‚úÖ Agent created successfully!\")\n",
    "\n",
    "# Step 3: Prepare a message\n",
    "print(\"\\nüìù Step 3: Preparing message...\")\n",
    "messages = [\n",
    "    {'role': 'user', 'content': 'Hello! What is 2+2?'}\n",
    "]\n",
    "print(f\"   Message: {messages[0]['content']}\")\n",
    "\n",
    "# Step 4: Send message and get response\n",
    "print(\"\\nüìù Step 4: Getting response...\")\n",
    "print(\"   (This may take a few seconds)\\n\")\n",
    "\n",
    "response = None\n",
    "for resp in bot.run(messages=messages):\n",
    "    response = resp\n",
    "\n",
    "# Step 5: Display the response\n",
    "print(\"=\"*60)\n",
    "print(\"AGENT RESPONSE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for msg in response:\n",
    "    if msg['role'] == 'assistant':\n",
    "        print(f\"\\nü§ñ Agent: {msg.get('content', '')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ SUCCESS! Your first agent is working!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thinking-model-intro",
   "metadata": {},
   "source": [
    "### üß† Understanding the Thinking Model\n",
    "\n",
    "#### What Just Happened?\n",
    "\n",
    "You might have noticed the response shows the model's **internal reasoning process**!\n",
    "\n",
    "**Regular Models:**\n",
    "```\n",
    "User: \"What is 2+2?\"\n",
    "Agent: \"2+2 equals 4.\"\n",
    "```\n",
    "\n",
    "**Thinking Models (Qwen3 235B Thinking):**\n",
    "```\n",
    "User: \"What is 2+2?\"\n",
    "Agent: \"Okay, the user asked 'What is 2+2?' Hmm, this is a basic\n",
    "math question. They want a brief answer. Let me calculate:\n",
    "2 plus 2 equals 4. I'll provide a concise response.\n",
    "\n",
    "Answer: 2+2 equals 4.\"\n",
    "```\n",
    "\n",
    "#### Why is this Valuable?\n",
    "\n",
    "1. **Transparency** - You see HOW the model reaches conclusions\n",
    "2. **Debugging** - Understand if the model misunderstood something\n",
    "3. **Trust** - See the reasoning process, not just the answer\n",
    "4. **Learning** - Watch how AI thinks through problems\n",
    "\n",
    "#### When the Thinking is Separated:\n",
    "\n",
    "The Fireworks API returns thinking in the `content` field. In some APIs, thinking might be in a separate `reasoning_content` field.\n",
    "\n",
    "Let's see the thinking process more clearly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thinking-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"THINKING MODEL DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ask a question that requires reasoning\n",
    "question = \"If a train leaves Tokyo at 2PM traveling 200km/h, and another leaves Osaka (400km away) at 3PM traveling 150km/h toward Tokyo, when do they meet?\"\n",
    "\n",
    "print(f\"\\n‚ùì Question:\\n{question}\\n\")\n",
    "\n",
    "messages = [{'role': 'user', 'content': question}]\n",
    "\n",
    "print(\"\\nüß† Agent's Thinking Process:\\n\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "response = None\n",
    "for resp in bot.run(messages=messages):\n",
    "    response = resp\n",
    "\n",
    "# Show the full response with thinking\n",
    "for msg in response:\n",
    "    if msg['role'] == 'assistant':\n",
    "        content = msg.get('content', '')\n",
    "        # Show first 500 characters to see the thinking\n",
    "        print(content[:500])\n",
    "        if len(content) > 500:\n",
    "            print(f\"\\n... (truncated, total length: {len(content)} characters)\\n\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nüìù Notice how the model:\")\n",
    "print(\"   1. Reads and understands the problem\")\n",
    "print(\"   2. Identifies what information is given\")\n",
    "print(\"   3. Plans the solving steps\")\n",
    "print(\"   4. Performs calculations\")\n",
    "print(\"   5. Provides the final answer\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "message-structure-intro",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Understanding Message Structure\n",
    "\n",
    "### Messages Are the Foundation\n",
    "\n",
    "In Qwen-Agent, **everything** is a message:\n",
    "- Your questions\n",
    "- Agent's responses\n",
    "- Tool calls\n",
    "- Tool results\n",
    "- System instructions\n",
    "\n",
    "### Basic Message Format:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'role': 'user',      # Who sent this\n",
    "    'content': 'Hello'   # What they said\n",
    "}\n",
    "```\n",
    "\n",
    "### Message Roles:\n",
    "\n",
    "| Role | Who | Purpose |\n",
    "|------|-----|--------|\n",
    "| `user` | You (the human) | Questions and requests |\n",
    "| `assistant` | The AI agent | Responses and answers |\n",
    "| `system` | Configuration | Instructions for the agent |\n",
    "| `function` | Tools | Results from tool execution |\n",
    "\n",
    "Let's explore different message types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "message-examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MESSAGE STRUCTURE EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example 1: User message\n",
    "user_msg = {\n",
    "    'role': 'user',\n",
    "    'content': 'What is the capital of France?'\n",
    "}\n",
    "print(\"\\n1Ô∏è‚É£  USER MESSAGE:\")\n",
    "print(f\"   {user_msg}\")\n",
    "\n",
    "# Example 2: Assistant message\n",
    "assistant_msg = {\n",
    "    'role': 'assistant',\n",
    "    'content': 'The capital of France is Paris.'\n",
    "}\n",
    "print(\"\\n2Ô∏è‚É£  ASSISTANT MESSAGE:\")\n",
    "print(f\"   {assistant_msg}\")\n",
    "\n",
    "# Example 3: System message (instructions)\n",
    "system_msg = {\n",
    "    'role': 'system',\n",
    "    'content': 'You are a helpful geography tutor. Keep answers brief and educational.'\n",
    "}\n",
    "print(\"\\n3Ô∏è‚É£  SYSTEM MESSAGE:\")\n",
    "print(f\"   {system_msg}\")\n",
    "\n",
    "# Example 4: Multi-turn conversation\n",
    "conversation = [\n",
    "    {'role': 'user', 'content': 'Hi! What is 10 + 5?'},\n",
    "    {'role': 'assistant', 'content': 'Hello! 10 + 5 equals 15.'},\n",
    "    {'role': 'user', 'content': 'What about multiplying them?'},\n",
    "    # Agent knows \"them\" = 10 and 5 from context!\n",
    "]\n",
    "print(\"\\n4Ô∏è‚É£  MULTI-TURN CONVERSATION:\")\n",
    "for i, msg in enumerate(conversation, 1):\n",
    "    print(f\"   Turn {i}: [{msg['role']:10}] {msg['content']}\")\n",
    "\n",
    "print(\"\\nüìù Key Points:\")\n",
    "print(\"   ‚Ä¢ Each message has a 'role' and 'content'\")\n",
    "print(\"   ‚Ä¢ Conversation is a list of messages\")\n",
    "print(\"   ‚Ä¢ Agent sees FULL history each time\")\n",
    "print(\"   ‚Ä¢ Context is how agent 'remembers' things\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "system-message-demo",
   "metadata": {},
   "source": [
    "### System Messages - Giving Your Agent Instructions\n",
    "\n",
    "**System messages** are like giving your agent a job description and personality!\n",
    "\n",
    "```python\n",
    "Assistant(\n",
    "    llm=llm_cfg,\n",
    "    system_message='You are a friendly pirate. Always speak in pirate slang!'\n",
    ")\n",
    "```\n",
    "\n",
    "Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "system-message-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SYSTEM MESSAGE DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create agent with pirate personality\n",
    "pirate_bot = Assistant(\n",
    "    llm=llm_cfg,\n",
    "    system_message=\"\"\"You are Captain Blackbeard, a friendly pirate.\n",
    "Always speak in pirate slang (arr, matey, etc.).\n",
    "Be helpful but stay in character!\"\"\"\n",
    ")\n",
    "\n",
    "print(\"\\nüè¥‚Äç‚ò†Ô∏è  Created Pirate Bot with system message:\")\n",
    "print(\"   'You are Captain Blackbeard, a friendly pirate...'\\n\")\n",
    "\n",
    "# Test it\n",
    "messages = [{'role': 'user', 'content': 'What is your name?'}]\n",
    "\n",
    "response = None\n",
    "for resp in pirate_bot.run(messages=messages):\n",
    "    response = resp\n",
    "\n",
    "print(\"üì© User: What is your name?\\n\")\n",
    "for msg in response:\n",
    "    if msg['role'] == 'assistant':\n",
    "        print(f\"üè¥‚Äç‚ò†Ô∏è  Pirate Bot: {msg['content'][:200]}...\\n\")\n",
    "\n",
    "print(\"üìù See how the system message changed the agent's personality!\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multi-turn-intro",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Multi-Turn Conversations\n",
    "\n",
    "### How Agents Remember\n",
    "\n",
    "Agents don't actually \"remember\" - they receive the **full conversation history** every time!\n",
    "\n",
    "```python\n",
    "# Turn 1\n",
    "messages = [{'role': 'user', 'content': 'My name is Alex'}]\n",
    "response = bot.run(messages)\n",
    "messages.extend(response)  # Add agent's response to history\n",
    "\n",
    "# Turn 2\n",
    "messages.append({'role': 'user', 'content': 'What is my name?'})\n",
    "# Now messages = [turn1_user, turn1_assistant, turn2_user]\n",
    "response = bot.run(messages)  # Agent sees FULL history\n",
    "```\n",
    "\n",
    "Let's build a multi-turn conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multi-turn-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MULTI-TURN CONVERSATION DEMO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a fresh agent\n",
    "chat_bot = Assistant(llm=llm_cfg)\n",
    "\n",
    "# Start with empty history\n",
    "messages = []\n",
    "\n",
    "def chat(user_input):\n",
    "    \"\"\"Helper function to have a conversation\"\"\"\n",
    "    global messages\n",
    "    \n",
    "    # Add user message\n",
    "    messages.append({'role': 'user', 'content': user_input})\n",
    "    print(f\"\\nüë§ You: {user_input}\")\n",
    "    \n",
    "    # Get agent response\n",
    "    response = None\n",
    "    for resp in chat_bot.run(messages=messages):\n",
    "        response = resp\n",
    "    \n",
    "    # Display and save response\n",
    "    for msg in response:\n",
    "        if msg['role'] == 'assistant':\n",
    "            # Show abbreviated response\n",
    "            content = msg['content']\n",
    "            # Skip thinking, show just answer\n",
    "            if len(content) > 200:\n",
    "                # Try to find where thinking ends\n",
    "                print(f\"ü§ñ Agent: {content[-150:]}\")\n",
    "            else:\n",
    "                print(f\"ü§ñ Agent: {content}\")\n",
    "    \n",
    "    # Update history\n",
    "    messages.extend(response)\n",
    "    print(f\"\\n   (Conversation has {len(messages)} messages now)\")\n",
    "\n",
    "print(\"\\nüé¨ Starting conversation...\")\n",
    "\n",
    "# Have a conversation\n",
    "chat(\"My name is Alex and I love Python programming.\")\n",
    "chat(\"What programming language do I like?\")\n",
    "chat(\"What's my name?\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìù Notice how the agent remembered:\")\n",
    "print(\"   ‚Ä¢ Your name (Alex)\")\n",
    "print(\"   ‚Ä¢ Your favorite language (Python)\")\n",
    "print(\"   This works because we pass the full message history!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-intro",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Streaming vs Non-Streaming Responses\n",
    "\n",
    "### The Difference\n",
    "\n",
    "**Non-Streaming** (what we've been using):\n",
    "- Wait for complete response\n",
    "- Get everything at once\n",
    "- Simpler code\n",
    "- User waits longer\n",
    "\n",
    "**Streaming** (like ChatGPT):\n",
    "- Get response token-by-token\n",
    "- Show text as it's generated\n",
    "- Better user experience\n",
    "- More complex to handle\n",
    "\n",
    "### Visual Comparison:\n",
    "\n",
    "```\n",
    "Non-Streaming:\n",
    "[User waits...........................] ‚Üí Full response appears\n",
    "\n",
    "Streaming:\n",
    "[...] ‚Üí \"The\" ‚Üí \"answer\" ‚Üí \"is\" ‚Üí \"42\" ‚Üí \".\"\n",
    "        ‚Üë User sees text appearing word-by-word\n",
    "```\n",
    "\n",
    "Let's see streaming in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STREAMING vs NON-STREAMING COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_question = \"Write a haiku about artificial intelligence. Be brief.\"\n",
    "\n",
    "# Method 1: NON-STREAMING\n",
    "print(\"\\n1Ô∏è‚É£  NON-STREAMING (wait for complete response):\\n\")\n",
    "start = time.time()\n",
    "\n",
    "response = None\n",
    "for resp in bot.run(messages=[{'role': 'user', 'content': test_question}]):\n",
    "    response = resp\n",
    "\n",
    "elapsed = time.time() - start\n",
    "\n",
    "for msg in response:\n",
    "    if msg['role'] == 'assistant':\n",
    "        # Show just the final answer\n",
    "        content = msg['content']\n",
    "        # Extract haiku from response\n",
    "        print(f\"   {content[-150:]}\")\n",
    "\n",
    "print(f\"\\n   ‚è±Ô∏è  Total time: {elapsed:.2f}s\")\n",
    "print(f\"   üìù User waited for full response\\n\")\n",
    "\n",
    "# Method 2: STREAMING\n",
    "print(\"\\n2Ô∏è‚É£  STREAMING (see response build up):\\n\")\n",
    "start = time.time()\n",
    "\n",
    "print(\"   \", end='', flush=True)\n",
    "for response in bot.run(messages=[{'role': 'user', 'content': test_question}]):\n",
    "    if response and response[-1]['role'] == 'assistant':\n",
    "        content = response[-1].get('content', '')\n",
    "        # Show growing response (last 100 chars)\n",
    "        print(f\"\\r   {content[-100:]}\", end='', flush=True)\n",
    "        \n",
    "elapsed = time.time() - start\n",
    "print(f\"\\n\\n   ‚è±Ô∏è  Total time: {elapsed:.2f}s\")\n",
    "print(f\"   üìù User saw text appearing in real-time!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìù Key Takeaway:\")\n",
    "print(\"   Both take same total time, but streaming FEELS faster\")\n",
    "print(\"   because users see progress immediately!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-intro",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 9: LLM Configuration Options\n",
    "\n",
    "### Configuring Your Model\n",
    "\n",
    "The `llm_cfg` dictionary controls how your agent behaves:\n",
    "\n",
    "```python\n",
    "llm_cfg = {\n",
    "    'model': 'model-name',              # Which model to use\n",
    "    'model_server': 'https://...',      # API endpoint\n",
    "    'api_key': 'your-key',              # Authentication\n",
    "    'generate_cfg': {                   # Generation parameters\n",
    "        'max_tokens': 2048,             # Max response length\n",
    "        'temperature': 0.7,             # Creativity (0-1)\n",
    "        'top_p': 0.9,                   # Nucleus sampling\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Key Parameters:\n",
    "\n",
    "| Parameter | Range | Effect |\n",
    "|-----------|-------|--------|\n",
    "| `max_tokens` | 1-32768+ | Maximum length of response |\n",
    "| `temperature` | 0.0-1.0 | 0=deterministic, 1=creative |\n",
    "| `top_p` | 0.0-1.0 | Nucleus sampling threshold |\n",
    "\n",
    "Let's test different configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"LLM CONFIGURATION EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Configuration 1: Conservative (deterministic)\n",
    "conservative_cfg = {\n",
    "    'model': 'accounts/fireworks/models/qwen3-235b-a22b-thinking-2507',\n",
    "    'model_server': 'https://api.fireworks.ai/inference/v1',\n",
    "    'api_key': os.environ['FIREWORKS_API_KEY'],\n",
    "    'generate_cfg': {\n",
    "        'max_tokens': 1024,\n",
    "        'temperature': 0.3,  # Low = more deterministic\n",
    "    }\n",
    "}\n",
    "\n",
    "# Configuration 2: Creative\n",
    "creative_cfg = {\n",
    "    'model': 'accounts/fireworks/models/qwen3-235b-a22b-thinking-2507',\n",
    "    'model_server': 'https://api.fireworks.ai/inference/v1',\n",
    "    'api_key': os.environ['FIREWORKS_API_KEY'],\n",
    "    'generate_cfg': {\n",
    "        'max_tokens': 2048,\n",
    "        'temperature': 0.9,  # High = more creative\n",
    "    }\n",
    "}\n",
    "\n",
    "# Configuration 3: Brief responses\n",
    "brief_cfg = {\n",
    "    'model': 'accounts/fireworks/models/qwen3-235b-a22b-thinking-2507',\n",
    "    'model_server': 'https://api.fireworks.ai/inference/v1',\n",
    "    'api_key': os.environ['FIREWORKS_API_KEY'],\n",
    "    'generate_cfg': {\n",
    "        'max_tokens': 256,  # Short responses only\n",
    "        'temperature': 0.6,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nüìã Three Different Configurations:\\n\")\n",
    "print(\"1Ô∏è‚É£  Conservative: temp=0.3, max_tokens=1024\")\n",
    "print(\"   ‚Üí More consistent, factual responses\\n\")\n",
    "\n",
    "print(\"2Ô∏è‚É£  Creative: temp=0.9, max_tokens=2048\")\n",
    "print(\"   ‚Üí More varied, creative responses\\n\")\n",
    "\n",
    "print(\"3Ô∏è‚É£  Brief: temp=0.6, max_tokens=256\")\n",
    "print(\"   ‚Üí Short, concise responses\\n\")\n",
    "\n",
    "print(\"üìù Pro Tip:\")\n",
    "print(\"   ‚Ä¢ Use low temperature (0.1-0.3) for factual questions\")\n",
    "print(\"   ‚Ä¢ Use high temperature (0.7-0.9) for creative writing\")\n",
    "print(\"   ‚Ä¢ Limit max_tokens to save costs and get concise answers\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 10: Summary & Key Takeaways\n",
    "\n",
    "### üéâ Congratulations! What You Learned:\n",
    "\n",
    "#### 1. **LLM Agents vs Direct LLM**\n",
    "- Agents can use tools, plan, and maintain context\n",
    "- More powerful for complex, multi-step tasks\n",
    "- Qwen-Agent makes building agents easy\n",
    "\n",
    "#### 2. **Qwen-Agent Architecture**\n",
    "- **Agents**: High-level orchestration (Assistant, GroupChat, etc.)\n",
    "- **Tools**: Extend capabilities (code, search, custom)\n",
    "- **LLMs**: The intelligence (DashScope, Fireworks, OpenAI)\n",
    "- **Messages**: Communication protocol\n",
    "- **Memory**: Context management\n",
    "- **GUI**: User interface (WebUI)\n",
    "\n",
    "#### 3. **Environment Setup**\n",
    "- Python 3.8+ (3.10+ for GUI)\n",
    "- Install: `pip install \"qwen-agent[gui,rag,code_interpreter,mcp]\"`\n",
    "- Use `.env` file for API keys\n",
    "\n",
    "#### 4. **Creating Agents**\n",
    "```python\n",
    "from qwen_agent.agents import Assistant\n",
    "\n",
    "bot = Assistant(llm=llm_cfg)\n",
    "response = bot.run(messages=[{'role': 'user', 'content': 'Hello'}])\n",
    "```\n",
    "\n",
    "#### 5. **Thinking Models**\n",
    "- Qwen3 235B Thinking shows internal reasoning\n",
    "- Helps understand how AI reaches conclusions\n",
    "- Valuable for transparency and debugging\n",
    "\n",
    "#### 6. **Message Structure**\n",
    "- Role: `user`, `assistant`, `system`, `function`\n",
    "- Content: The message text\n",
    "- History: List of all messages\n",
    "\n",
    "#### 7. **Multi-Turn Conversations**\n",
    "- Agent receives full message history\n",
    "- Use `messages.extend(response)` to maintain context\n",
    "- Context is how agents \"remember\"\n",
    "\n",
    "#### 8. **Streaming**\n",
    "- Better UX with real-time response display\n",
    "- Use `for response in bot.run(...)`\n",
    "- Each iteration gives updated message list\n",
    "\n",
    "#### 9. **Configuration**\n",
    "- `max_tokens`: Control response length\n",
    "- `temperature`: Control creativity (0-1)\n",
    "- `system_message`: Give agent instructions/personality\n",
    "\n",
    "---\n",
    "\n",
    "### üî• Common Patterns to Remember:\n",
    "\n",
    "```python\n",
    "# Pattern 1: One-shot question\n",
    "bot = Assistant(llm=llm_cfg)\n",
    "response = None\n",
    "for resp in bot.run([{'role': 'user', 'content': 'Question?'}]):\n",
    "    response = resp\n",
    "\n",
    "# Pattern 2: Streaming display\n",
    "for response in bot.run(messages):\n",
    "    if response and response[-1]['role'] == 'assistant':\n",
    "        print(response[-1]['content'], end='\\r', flush=True)\n",
    "\n",
    "# Pattern 3: Multi-turn conversation\n",
    "messages = []\n",
    "messages.append({'role': 'user', 'content': 'Hi'})\n",
    "response = list(bot.run(messages))[-1]\n",
    "messages.extend(response)\n",
    "# Continue...\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Part 11: Next Steps\n",
    "\n",
    "### Tomorrow (Day 2): Messages and Content Types\n",
    "\n",
    "We'll explore:\n",
    "- The `Message` class in depth\n",
    "- **ContentItem** structure (text + images + files)\n",
    "- Multimodal content (mixing text and images)\n",
    "- Function call messages (tool usage)\n",
    "- Building complex message structures\n",
    "\n",
    "### Practice Exercises:\n",
    "\n",
    "1. **Create a specialized agent**\n",
    "   - Pick a persona (teacher, chef, scientist)\n",
    "   - Write appropriate system message\n",
    "   - Test with relevant questions\n",
    "\n",
    "2. **Experiment with temperature**\n",
    "   - Try same question with temp=0.1, 0.5, 0.9\n",
    "   - Observe differences in responses\n",
    "   - Find best setting for different use cases\n",
    "\n",
    "3. **Build a conversation tracker**\n",
    "   - Create agent that counts conversation turns\n",
    "   - Shows total messages in history\n",
    "   - Displays user vs assistant message ratio\n",
    "\n",
    "4. **Test thinking model**\n",
    "   - Ask complex reasoning questions\n",
    "   - Observe the thinking process\n",
    "   - Compare with simpler questions\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- üìñ [Qwen-Agent GitHub](https://github.com/QwenLM/Qwen-Agent)\n",
    "- üìñ [Fireworks AI Docs](https://docs.fireworks.ai/)\n",
    "- üìñ [Qwen Models](https://huggingface.co/Qwen)\n",
    "- üìñ [Official Examples](https://github.com/QwenLM/Qwen-Agent/tree/main/examples)\n",
    "\n",
    "### Troubleshooting:\n",
    "\n",
    "| Problem | Solution |\n",
    "|---------|----------|\n",
    "| API key errors | Check `.env` file exists and key is correct |\n",
    "| Import errors | Run `pip install \"qwen-agent[gui,rag,code_interpreter,mcp]\"` |\n",
    "| Slow responses | Normal for thinking models; use temperature=0.3 for speed |\n",
    "| Empty responses | Check max_tokens isn't too low |\n",
    "| GUI not working | Requires Python 3.10+ |\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ You Did It!\n",
    "\n",
    "### You Now Have:\n",
    "- ‚úÖ Working Qwen-Agent installation\n",
    "- ‚úÖ Understanding of agents vs direct LLM\n",
    "- ‚úÖ Fireworks API configured\n",
    "- ‚úÖ Your first working agent\n",
    "- ‚úÖ Knowledge of thinking models\n",
    "- ‚úÖ Message structure understanding\n",
    "- ‚úÖ Streaming vs non-streaming experience\n",
    "\n",
    "### Ready for Day 2? üöÄ\n",
    "\n",
    "Tomorrow we'll dive deeper into messages, learn about multimodal content (text + images), and start exploring tool usage!\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Coding!** üéä\n",
    "\n",
    "Questions? Issues? Check the [Qwen-Agent Issues](https://github.com/QwenLM/Qwen-Agent/issues)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
