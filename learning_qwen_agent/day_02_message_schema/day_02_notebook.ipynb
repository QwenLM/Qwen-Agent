{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 2: Message Schema & Communication\n",
        "\n",
        "## Understanding How Agents Communicate\n",
        "\n",
        "### Today's Learning Objectives:\n",
        "1. Deep dive into the `Message` class structure\n",
        "2. Understand different role types and their purposes\n",
        "3. Work with `ContentItem` for multimodal messages\n",
        "4. Learn about `FunctionCall` messages (preview for Day 6)\n",
        "5. Build complex, multi-modal conversation histories\n",
        "6. Understand reasoning_content for advanced models\n",
        "\n",
        "### Prerequisites:\n",
        "- Completed Day 1\n",
        "- Qwen-Agent installed and configured\n",
        "- API key set up\n",
        "\n",
        "### Time Required: 1.5-2 hours\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Why Messages Matter\n",
        "\n",
        "### The Communication Protocol\n",
        "\n",
        "Everything in Qwen-Agent flows through **Messages**. They are:\n",
        "- The **input** to agents (what you say)\n",
        "- The **output** from agents (what they respond)\n",
        "- The **history** of conversations (context)\n",
        "- The **mechanism** for tool calls (function execution)\n",
        "\n",
        "### Message Flow Diagram:\n",
        "\n",
        "```\n",
        "User Creates Message\n",
        "        |\n",
        "        v\n",
        "[Message(role='user', content='...')]\n",
        "        |\n",
        "        v\n",
        "    Agent.run(messages)\n",
        "        |\n",
        "        v\n",
        "    LLM processes\n",
        "        |\n",
        "        v\n",
        "[Message(role='assistant', content='...'),\n",
        " Message(role='assistant', function_call=...)]\n",
        "        |\n",
        "        v\n",
        "    User receives response\n",
        "```\n",
        "\n",
        "### Key Insight:\n",
        "Messages are **NOT** just strings. They are structured data with:\n",
        "- **role**: Who is speaking\n",
        "- **content**: What is being said (can be text, image, etc.)\n",
        "- **metadata**: Additional information (name, function calls, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Message Class Structure\n",
        "\n",
        "### Source Code Location:\n",
        "`/qwen_agent/llm/schema.py` (lines 132-165)\n",
        "\n",
        "### Message Class Definition:\n",
        "\n",
        "```python\n",
        "class Message(BaseModelCompatibleDict):\n",
        "    role: str                                      # Required: 'user', 'assistant', 'system', 'function'\n",
        "    content: Union[str, List[ContentItem]]         # Required: Message content\n",
        "    reasoning_content: Optional[Union[str, List[ContentItem]]] = None  # For reasoning models\n",
        "    name: Optional[str] = None                     # Agent/tool name\n",
        "    function_call: Optional[FunctionCall] = None   # Tool invocation data\n",
        "    extra: Optional[dict] = None                   # Additional metadata\n",
        "```\n",
        "\n",
        "Let's explore each field in detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: The `role` Field\n",
        "\n",
        "### Four Role Types (from schema.py lines 26-29):\n",
        "\n",
        "```python\n",
        "SYSTEM = 'system'      # Instructions for the agent\n",
        "USER = 'user'          # Input from human/user\n",
        "ASSISTANT = 'assistant'  # Response from agent/LLM\n",
        "FUNCTION = 'function'   # Tool execution results\n",
        "```\n",
        "\n",
        "### Role Descriptions:\n",
        "\n",
        "| Role | Purpose | Who Creates It | Example |\n",
        "|------|---------|----------------|----------|\n",
        "| `system` | Give instructions/context to agent | Developer | \"You are a helpful assistant\" |\n",
        "| `user` | User queries and inputs | User/Application | \"What's the weather?\" |\n",
        "| `assistant` | Agent responses | Agent/LLM | \"The weather is sunny\" |\n",
        "| `function` | Tool execution results | Agent (after running tool) | `{\"temperature\": 72}` |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Configured for Fireworks API\n",
            "   Model: Qwen3-235B-A22B-Thinking-2507\n",
            "   Max tokens: 32,768"
          ]
        }
      ],
      "source": [
        "# ================================================\n",
        "# FIREWORKS API CONFIGURATION\n",
        "# ================================================\n",
        "import os\n",
        "\n",
        "# Set API credentials\n",
        "os.environ['FIREWORKS_API_KEY'] = 'fw_3ZSpUnVR78vs38jJtyewjcWk'\n",
        "\n",
        "# Standard configuration for Fireworks Qwen3-235B-A22B-Thinking\n",
        "llm_cfg_fireworks = {\n",
        "    'model': 'accounts/fireworks/models/qwen3-235b-a22b-thinking-2507',\n",
        "    'model_server': 'https://api.fireworks.ai/inference/v1',\n",
        "    'api_key': os.environ['FIREWORKS_API_KEY'],\n",
        "    'generate_cfg': {\n",
        "        'max_tokens': 32768,\n",
        "        'temperature': 0.6,\n",
        "    }\n",
        "}\n",
        "\n",
        "# Use this as default llm_cfg\n",
        "llm_cfg = llm_cfg_fireworks\n",
        "\n",
        "print('\u2705 Configured for Fireworks API')\n",
        "print(f'   Model: Qwen3-235B-A22B-Thinking-2507')\n",
        "print(f'   Max tokens: 32,768')\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System Message:\n",
            "{'role': 'system', 'content': 'You are a helpful AI assistant specialized in Python programming.'}\n",
            "\n",
            "User Message:\n",
            "{'role': 'user', 'content': 'How do I read a file in Python?'}\n",
            "\n",
            "Assistant Message:\n",
            "{'role': 'assistant', 'content': 'You can use the `open()` function with a context manager...'}\n",
            "\n",
            "Function Message:\n",
            "{'role': 'function', 'content': '{\"result\": \"File read successfully\"}', 'name': 'read_file'}"
          ]
        }
      ],
      "source": [
        "# Import the Message class\n",
        "from qwen_agent.llm.schema import Message, SYSTEM, USER, ASSISTANT, FUNCTION\n",
        "\n",
        "# Example 1: System message\n",
        "system_msg = Message(\n",
        "    role=SYSTEM,\n",
        "    content='You are a helpful AI assistant specialized in Python programming.'\n",
        ")\n",
        "\n",
        "# Example 2: User message\n",
        "user_msg = Message(\n",
        "    role=USER,\n",
        "    content='How do I read a file in Python?'\n",
        ")\n",
        "\n",
        "# Example 3: Assistant message\n",
        "assistant_msg = Message(\n",
        "    role=ASSISTANT,\n",
        "    content='You can use the `open()` function with a context manager...'\n",
        ")\n",
        "\n",
        "# Example 4: Function message (we'll learn more about this on Day 6)\n",
        "function_msg = Message(\n",
        "    role=FUNCTION,\n",
        "    content='{\"result\": \"File read successfully\"}',\n",
        "    name='read_file'  # Name of the tool that was executed\n",
        ")\n",
        "\n",
        "print(\"System Message:\")\n",
        "print(system_msg)\n",
        "print(\"\\nUser Message:\")\n",
        "print(user_msg)\n",
        "print(\"\\nAssistant Message:\")\n",
        "print(assistant_msg)\n",
        "print(\"\\nFunction Message:\")\n",
        "print(function_msg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Message as Dict:\n",
        "\n",
        "The `Message` class extends `BaseModelCompatibleDict`, which means you can use it like a dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Role (dict syntax): user\n",
            "Content (dict syntax): Hello!\n",
            "Role (object syntax): user\n",
            "Content (object syntax): Hello!\n",
            "Name (with default): Anonymous\n",
            "\n",
            "As dictionary: {'role': 'user', 'content': 'Hello!'}\n",
            "Type: <class 'dict'>"
          ]
        }
      ],
      "source": [
        "# Create a message\n",
        "msg = Message(role='user', content='Hello!')\n",
        "\n",
        "# Access like a dict\n",
        "print(f\"Role (dict syntax): {msg['role']}\")\n",
        "print(f\"Content (dict syntax): {msg['content']}\")\n",
        "\n",
        "# Access like an object\n",
        "print(f\"Role (object syntax): {msg.role}\")\n",
        "print(f\"Content (object syntax): {msg.content}\")\n",
        "\n",
        "# Use .get() method (like dict)\n",
        "print(f\"Name (with default): {msg.get('name', 'Anonymous')}\")\n",
        "\n",
        "# Convert to dict\n",
        "msg_dict = msg.model_dump()\n",
        "print(f\"\\nAs dictionary: {msg_dict}\")\n",
        "print(f\"Type: {type(msg_dict)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why Dict Compatibility?\n",
        "\n",
        "This dual interface allows you to:\n",
        "1. Use dictionary syntax when working with message lists\n",
        "2. Use object syntax for cleaner code\n",
        "3. Easily serialize to JSON\n",
        "4. Maintain backward compatibility\n",
        "\n",
        "**In practice, you'll see both styles:**\n",
        "```python\n",
        "# Simple dict style (common in examples)\n",
        "messages = [{'role': 'user', 'content': 'Hi'}]\n",
        "\n",
        "# Message object style (more features)\n",
        "messages = [Message(role='user', content='Hi')]\n",
        "\n",
        "# Both work! Qwen-Agent handles conversion internally\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: The `content` Field - Simple Text\n",
        "\n",
        "### Content can be a string:\n",
        "\n",
        "```python\n",
        "content: Union[str, List[ContentItem]]\n",
        "```\n",
        "\n",
        "For simple text-only messages, `content` is just a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content type: <class 'str'>\n",
            "Content value: What is the capital of France?\n",
            "\n",
            "Multiline content:\n",
            "Please help me with:\n",
            "    1. Understanding Python decorators\n",
            "    2. Writing better code\n",
            "    3. Optimizing performance\n",
            "    "
          ]
        }
      ],
      "source": [
        "# Simple text content\n",
        "simple_msg = Message(\n",
        "    role='user',\n",
        "    content='What is the capital of France?'\n",
        ")\n",
        "\n",
        "print(f\"Content type: {type(simple_msg.content)}\")\n",
        "print(f\"Content value: {simple_msg.content}\")\n",
        "\n",
        "# Multi-line text content\n",
        "multiline_msg = Message(\n",
        "    role='user',\n",
        "    content=\"\"\"Please help me with:\n",
        "    1. Understanding Python decorators\n",
        "    2. Writing better code\n",
        "    3. Optimizing performance\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(f\"\\nMultiline content:\\n{multiline_msg.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: The `ContentItem` Class - Multimodal Content\n",
        "\n",
        "### Source Code Location:\n",
        "`/qwen_agent/llm/schema.py` (lines 80-130)\n",
        "\n",
        "### ContentItem Definition:\n",
        "\n",
        "```python\n",
        "class ContentItem(BaseModelCompatibleDict):\n",
        "    text: Optional[str] = None\n",
        "    image: Optional[str] = None          # URL or base64\n",
        "    file: Optional[str] = None           # File path or URL\n",
        "    audio: Optional[Union[str, dict]] = None\n",
        "    video: Optional[Union[str, list]] = None\n",
        "```\n",
        "\n",
        "### **Important Rule**: Exactly ONE field must be provided\n",
        "\n",
        "The validator (lines 95-111) ensures mutual exclusivity:\n",
        "```python\n",
        "if provided_fields != 1:\n",
        "    raise ValueError(\"Exactly one of 'text', 'image', 'file', 'audio', or 'video' must be provided.\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text item: {'text': 'Hello, world!'}\n",
            "Type: text\n",
            "Value: Hello, world!\n",
            "\n",
            "Image item: {'image': 'https://example.com/image.jpg'}\n",
            "Type: image\n",
            "Value: https://example.com/image.jpg\n",
            "\n",
            "File item: {'file': '/path/to/document.pdf'}"
          ]
        }
      ],
      "source": [
        "from qwen_agent.llm.schema import ContentItem\n",
        "\n",
        "# Example 1: Text ContentItem\n",
        "text_item = ContentItem(text='Hello, world!')\n",
        "print(f\"Text item: {text_item}\")\n",
        "print(f\"Type: {text_item.type}\")\n",
        "print(f\"Value: {text_item.value}\")\n",
        "\n",
        "# Example 2: Image ContentItem (URL)\n",
        "image_item = ContentItem(\n",
        "    image='https://example.com/image.jpg'\n",
        ")\n",
        "print(f\"\\nImage item: {image_item}\")\n",
        "print(f\"Type: {image_item.type}\")\n",
        "print(f\"Value: {image_item.value}\")\n",
        "\n",
        "# Example 3: File ContentItem\n",
        "file_item = ContentItem(\n",
        "    file='/path/to/document.pdf'\n",
        ")\n",
        "print(f\"\\nFile item: {file_item}\")\n",
        "\n",
        "# This would FAIL (can't have both text and image):\n",
        "# bad_item = ContentItem(text='Hello', image='image.jpg')  # ValueError!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Useful ContentItem Methods:\n",
        "\n",
        "```python\n",
        "item.type          # Returns 'text', 'image', 'file', 'audio', or 'video'\n",
        "item.value         # Returns the actual value (string or dict)\n",
        "item.get_type_and_value()  # Returns tuple (type, value)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type property: text\n",
            "Value property: Sample text\n",
            "Type and value method: ('text', 'Sample text')\n",
            "\n",
            "Processing text: This is a long piece of text that needs processing...\n",
            "Processing image from: https://example.com/photo.jpg"
          ]
        }
      ],
      "source": [
        "# Demonstrate methods\n",
        "item = ContentItem(text='Sample text')\n",
        "\n",
        "print(f\"Type property: {item.type}\")\n",
        "print(f\"Value property: {item.value}\")\n",
        "print(f\"Type and value method: {item.get_type_and_value()}\")\n",
        "\n",
        "# Practical usage: checking content type\n",
        "def process_content(item: ContentItem):\n",
        "    \"\"\"Process different content types\"\"\"\n",
        "    if item.type == 'text':\n",
        "        return f\"Processing text: {item.value[:50]}...\"\n",
        "    elif item.type == 'image':\n",
        "        return f\"Processing image from: {item.value}\"\n",
        "    elif item.type == 'file':\n",
        "        return f\"Processing file: {item.value}\"\n",
        "    else:\n",
        "        return f\"Processing {item.type}\"\n",
        "\n",
        "# Test with different types\n",
        "text_content = ContentItem(text='This is a long piece of text that needs processing')\n",
        "image_content = ContentItem(image='https://example.com/photo.jpg')\n",
        "\n",
        "print(f\"\\n{process_content(text_content)}\")\n",
        "print(process_content(image_content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Multimodal Messages - Text + Images\n",
        "\n",
        "### When content is a List[ContentItem]:\n",
        "\n",
        "For messages that combine text and images (like vision models), `content` becomes a list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multimodal Message:\n",
            "Role: user\n",
            "Content type: <class 'list'>\n",
            "Number of items: 2\n",
            "\n",
            "Item 0:\n",
            "  Type: text\n",
            "  Value: What is in this image?\n",
            "\n",
            "Item 1:\n",
            "  Type: image\n",
            "  Value: https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/demo_small.jpg"
          ]
        }
      ],
      "source": [
        "# Create a multimodal message (text + image)\n",
        "multimodal_msg = Message(\n",
        "    role='user',\n",
        "    content=[\n",
        "        ContentItem(text='What is in this image?'),\n",
        "        ContentItem(image='https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/demo_small.jpg')\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Multimodal Message:\")\n",
        "print(f\"Role: {multimodal_msg.role}\")\n",
        "print(f\"Content type: {type(multimodal_msg.content)}\")\n",
        "print(f\"Number of items: {len(multimodal_msg.content)}\")\n",
        "\n",
        "# Iterate through content items\n",
        "for i, item in enumerate(multimodal_msg.content):\n",
        "    print(f\"\\nItem {i}:\")\n",
        "    print(f\"  Type: {item.type}\")\n",
        "    print(f\"  Value: {item.value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing with a Vision Model (if available):\n",
        "\n",
        "**Note**: This requires a vision-language model like Qwen-VL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vision model example code ready (commented out).\n",
            "Uncomment to test if you have access to qwen-vl-max."
          ]
        }
      ],
      "source": [
        "# Example with Qwen-VL (uncomment if you have access)\n",
        "from qwen_agent.agents import Assistant\n",
        "\n",
        "# Configure for vision model\n",
        "vl_cfg = {\n",
        "    'model': 'qwen-vl-max',  # Vision-language model\n",
        "    'model_type': 'qwenvl_dashscope'\n",
        "}\n",
        "\n",
        "# Create vision agent\n",
        "# Uncomment to test (requires DashScope access to VL models):\n",
        "# vision_bot = Assistant(llm=vl_cfg)\n",
        "\n",
        "# Create multimodal message\n",
        "# messages = [\n",
        "#     Message(\n",
        "#         role='user',\n",
        "#         content=[\n",
        "#             ContentItem(text='Describe this image in detail'),\n",
        "#             ContentItem(image='https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/demo_small.jpg')\n",
        "#         ]\n",
        "#     )\n",
        "# ]\n",
        "\n",
        "# Get response\n",
        "# response = vision_bot.run_nonstream(messages=messages)\n",
        "# for msg in response:\n",
        "#     if msg['role'] == 'assistant':\n",
        "#         print(msg['content'])\n",
        "\n",
        "print(\"Vision model example code ready (commented out).\")\n",
        "print(\"Uncomment to test if you have access to qwen-vl-max.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: The `reasoning_content` Field\n",
        "\n",
        "### What is reasoning_content?\n",
        "\n",
        "Advanced reasoning models (like QwQ-32B) show their \"thinking process\" separately from the final answer.\n",
        "\n",
        "```python\n",
        "class Message:\n",
        "    content: str                      # Final answer\n",
        "    reasoning_content: Optional[str]  # Thinking process\n",
        "```\n",
        "\n",
        "### Example Response Structure:\n",
        "\n",
        "```\n",
        "Message(\n",
        "    role='assistant',\n",
        "    reasoning_content='Let me think... First I need to...',\n",
        "    content='The answer is 42.'\n",
        ")\n",
        "```\n",
        "\n",
        "### Visual Representation:\n",
        "\n",
        "```\n",
        "User: \"Solve 2x + 5 = 15\"\n",
        "\n",
        "Agent:\n",
        "  reasoning_content: \"I need to isolate x.\n",
        "                      First, subtract 5 from both sides: 2x = 10\n",
        "                      Then divide by 2: x = 5\"\n",
        "  \n",
        "  content: \"x = 5\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reasoning Model Response:\n",
            "============================================================\n",
            "\n",
            "\ud83e\udd14 Thinking Process:\n",
            "Let me analyze this step by step:\n",
            "1. The question asks about France\n",
            "2. France is in Europe\n",
            "3. The capital is Paris\n",
            "\n",
            "\ud83d\udca1 Final Answer:\n",
            "The capital of France is Paris.\n",
            "============================================================\n",
            "\n",
            "\u2705 This message includes reasoning!"
          ]
        }
      ],
      "source": [
        "# Simulate a reasoning model response\n",
        "reasoning_msg = Message(\n",
        "    role='assistant',\n",
        "    reasoning_content=\"Let me analyze this step by step:\\n1. The question asks about France\\n2. France is in Europe\\n3. The capital is Paris\",\n",
        "    content='The capital of France is Paris.'\n",
        ")\n",
        "\n",
        "print(\"Reasoning Model Response:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\n\ud83e\udd14 Thinking Process:\\n{reasoning_msg.reasoning_content}\")\n",
        "print(f\"\\n\ud83d\udca1 Final Answer:\\n{reasoning_msg.content}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check if reasoning content exists\n",
        "if reasoning_msg.get('reasoning_content'):\n",
        "    print(\"\\n\u2705 This message includes reasoning!\")\n",
        "else:\n",
        "    print(\"\\n\u274c No reasoning content\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### When to use reasoning_content:\n",
        "\n",
        "1. **QwQ-32B and other reasoning models**\n",
        "2. **Complex problem-solving** where showing work is valuable\n",
        "3. **Educational contexts** where explanation matters\n",
        "4. **Debugging agent decisions**\n",
        "\n",
        "For normal models (Qwen-Max, etc.), this field is usually `None`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: The `FunctionCall` Class (Preview)\n",
        "\n",
        "### Source Code Location:\n",
        "`/qwen_agent/llm/schema.py` (lines 69-78)\n",
        "\n",
        "### FunctionCall Definition:\n",
        "\n",
        "```python\n",
        "class FunctionCall(BaseModelCompatibleDict):\n",
        "    name: str        # Tool name to execute\n",
        "    arguments: str   # JSON string of parameters\n",
        "```\n",
        "\n",
        "### When Agents Use Tools:\n",
        "\n",
        "Instead of generating text, the LLM generates a function call:\n",
        "\n",
        "```\n",
        "User: \"What's the weather in Tokyo?\"\n",
        "\n",
        "Agent generates:\n",
        "  Message(\n",
        "      role='assistant',\n",
        "      content='',\n",
        "      function_call=FunctionCall(\n",
        "          name='weather_api',\n",
        "          arguments='{\"location\": \"Tokyo\"}'\n",
        "      )\n",
        "  )\n",
        "```\n",
        "\n",
        "We'll dive deep into this on **Day 6: Function Calling**. For now, just understand the structure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function Call:\n",
            "Name: get_weather\n",
            "Arguments (string): {\"city\": \"Tokyo\", \"units\": \"celsius\"}\n",
            "Arguments (parsed): {'city': 'Tokyo', 'units': 'celsius'}\n",
            "\n",
            "Message with function call:\n",
            "{'role': 'assistant', 'content': '', 'function_call': {'name': 'get_weather', 'arguments': '{\"city\": \"Tokyo\", \"units\": \"celsius\"}'}}\n",
            "\n",
            "\u2705 Agent wants to use tool: get_weather"
          ]
        }
      ],
      "source": [
        "from qwen_agent.llm.schema import FunctionCall\n",
        "import json\n",
        "\n",
        "# Example 1: Simple function call\n",
        "func_call = FunctionCall(\n",
        "    name='get_weather',\n",
        "    arguments='{\"city\": \"Tokyo\", \"units\": \"celsius\"}'\n",
        ")\n",
        "\n",
        "print(\"Function Call:\")\n",
        "print(f\"Name: {func_call.name}\")\n",
        "print(f\"Arguments (string): {func_call.arguments}\")\n",
        "print(f\"Arguments (parsed): {json.loads(func_call.arguments)}\")\n",
        "\n",
        "# Example 2: Message with function call\n",
        "tool_msg = Message(\n",
        "    role='assistant',\n",
        "    content='',  # Often empty when making function call\n",
        "    function_call=func_call\n",
        ")\n",
        "\n",
        "print(f\"\\nMessage with function call:\")\n",
        "print(tool_msg)\n",
        "\n",
        "# Check if message has function call\n",
        "if tool_msg.get('function_call'):\n",
        "    print(f\"\\n\u2705 Agent wants to use tool: {tool_msg.function_call.name}\")\n",
        "else:\n",
        "    print(\"\\n\u274c No function call in this message\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 9: The `name` Field\n",
        "\n",
        "### Purpose:\n",
        "The `name` field identifies the speaker in specific contexts:\n",
        "\n",
        "1. **Function messages**: Which tool generated this result\n",
        "2. **Multi-agent systems**: Which agent is speaking\n",
        "3. **Named personas**: For role-playing scenarios\n",
        "\n",
        "### Examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function Result:\n",
            "Role: function\n",
            "Name (tool): web_search\n",
            "Content: {\"results\": [\"Tokyo weather: Sunny, 25\u00b0C\"]}\n",
            "\n",
            "CodeExpert: I recommend using a list comprehension for efficiency.\n",
            "SecurityExpert: Make sure to validate all user inputs first."
          ]
        }
      ],
      "source": [
        "# Example 1: Function result with name\n",
        "function_result = Message(\n",
        "    role='function',\n",
        "    name='web_search',\n",
        "    content='{\"results\": [\"Tokyo weather: Sunny, 25\u00b0C\"]}'\n",
        ")\n",
        "\n",
        "print(\"Function Result:\")\n",
        "print(f\"Role: {function_result.role}\")\n",
        "print(f\"Name (tool): {function_result.name}\")\n",
        "print(f\"Content: {function_result.content}\")\n",
        "\n",
        "# Example 2: Multi-agent conversation (preview for Day 10)\n",
        "agent1_msg = Message(\n",
        "    role='assistant',\n",
        "    name='CodeExpert',\n",
        "    content='I recommend using a list comprehension for efficiency.'\n",
        ")\n",
        "\n",
        "agent2_msg = Message(\n",
        "    role='assistant',\n",
        "    name='SecurityExpert',\n",
        "    content='Make sure to validate all user inputs first.'\n",
        ")\n",
        "\n",
        "print(f\"\\n{agent1_msg.name}: {agent1_msg.content}\")\n",
        "print(f\"{agent2_msg.name}: {agent2_msg.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 10: Building Complex Conversations\n",
        "\n",
        "### Realistic Conversation Flow:\n",
        "\n",
        "Let's build a conversation that demonstrates various message types:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complete Conversation Flow:\n",
            "================================================================================\n",
            "\n",
            "1. \u2699\ufe0f SYSTEM:\n",
            "   You are a helpful data analyst assistant.\n",
            "\n",
            "2. \ud83d\udc64 USER:\n",
            "   I need to analyze sales data from Q1 2024.\n",
            "\n",
            "3. \ud83e\udd16 ASSISTANT:\n",
            "   I can help with that! What specific metrics are you interested in?\n",
            "\n",
            "4. \ud83d\udc64 USER:\n",
            "   Total revenue and top 5 products.\n",
            "\n",
            "5. \ud83e\udd16 ASSISTANT:\n",
            "   [Calling tool: analyze_sales]\n",
            "   [Arguments: {\"period\": \"Q1 2024\", \"metrics\": [\"revenue\", \"top_products\"]}]\n",
            "\n",
            "6. \ud83d\udd27 FUNCTION (analyze_sales):\n",
            "   {\"total_revenue\": 1250000, \"top_products\": [\"ProductA\", \"ProductB\", \"ProductC\", \"ProductD\", \"ProductE\"]}\n",
            "\n",
            "7. \ud83e\udd16 ASSISTANT:\n",
            "   Based on the Q1 2024 data:\n",
            "   - Total Revenue: $1,250,000\n",
            "   - Top 5 Products: ProductA, ProductB, ProductC, ProductD, ProductE\n",
            "\n",
            "================================================================================"
          ]
        }
      ],
      "source": [
        "# Build a realistic conversation\n",
        "conversation = [\n",
        "    # 1. System message sets the context\n",
        "    Message(\n",
        "        role='system',\n",
        "        content='You are a helpful data analyst assistant.'\n",
        "    ),\n",
        "    \n",
        "    # 2. User asks a question\n",
        "    Message(\n",
        "        role='user',\n",
        "        content='I need to analyze sales data from Q1 2024.'\n",
        "    ),\n",
        "    \n",
        "    # 3. Assistant asks for clarification\n",
        "    Message(\n",
        "        role='assistant',\n",
        "        content='I can help with that! What specific metrics are you interested in?'\n",
        "    ),\n",
        "    \n",
        "    # 4. User provides more details\n",
        "    Message(\n",
        "        role='user',\n",
        "        content='Total revenue and top 5 products.'\n",
        "    ),\n",
        "    \n",
        "    # 5. Assistant decides to use a tool (simulated)\n",
        "    Message(\n",
        "        role='assistant',\n",
        "        content='',\n",
        "        function_call=FunctionCall(\n",
        "            name='analyze_sales',\n",
        "            arguments='{\"period\": \"Q1 2024\", \"metrics\": [\"revenue\", \"top_products\"]}'\n",
        "        )\n",
        "    ),\n",
        "    \n",
        "    # 6. Tool returns results\n",
        "    Message(\n",
        "        role='function',\n",
        "        name='analyze_sales',\n",
        "        content='{\"total_revenue\": 1250000, \"top_products\": [\"ProductA\", \"ProductB\", \"ProductC\", \"ProductD\", \"ProductE\"]}'\n",
        "    ),\n",
        "    \n",
        "    # 7. Assistant presents results\n",
        "    Message(\n",
        "        role='assistant',\n",
        "        content='Based on the Q1 2024 data:\\n- Total Revenue: $1,250,000\\n- Top 5 Products: ProductA, ProductB, ProductC, ProductD, ProductE'\n",
        "    )\n",
        "]\n",
        "\n",
        "# Display the conversation\n",
        "print(\"Complete Conversation Flow:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, msg in enumerate(conversation, 1):\n",
        "    role_emoji = {\n",
        "        'system': '\u2699\ufe0f',\n",
        "        'user': '\ud83d\udc64',\n",
        "        'assistant': '\ud83e\udd16',\n",
        "        'function': '\ud83d\udd27'\n",
        "    }\n",
        "    \n",
        "    emoji = role_emoji.get(msg.role, '\u2753')\n",
        "    print(f\"\\n{i}. {emoji} {msg.role.upper()}\", end='')\n",
        "    \n",
        "    if msg.get('name'):\n",
        "        print(f\" ({msg.name})\", end='')\n",
        "    print(\":\")\n",
        "    \n",
        "    if msg.get('function_call'):\n",
        "        print(f\"   [Calling tool: {msg.function_call.name}]\")\n",
        "        print(f\"   [Arguments: {msg.function_call.arguments}]\")\n",
        "    elif msg.content:\n",
        "        # Indent content\n",
        "        for line in msg.content.split('\\n'):\n",
        "            print(f\"   {line}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 11: Message Utilities\n",
        "\n",
        "### Helper Functions for Working with Messages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Message Utilities Demo:\n",
            "User messages: 2\n",
            "Assistant messages: 3\n",
            "Function messages: 1\n",
            "\n",
            "Last user message: Total revenue and top 5 products.\n",
            "\n",
            "Messages with function calls: 1\n",
            "Multimodal message: True\n",
            "Text-only message: False"
          ]
        }
      ],
      "source": [
        "def count_messages_by_role(messages, role):\n",
        "    \"\"\"Count messages of a specific role\"\"\"\n",
        "    return sum(1 for msg in messages if msg.get('role') == role)\n",
        "\n",
        "def get_last_user_message(messages):\n",
        "    \"\"\"Get the most recent user message\"\"\"\n",
        "    for msg in reversed(messages):\n",
        "        if msg.get('role') == 'user':\n",
        "            return msg\n",
        "    return None\n",
        "\n",
        "def extract_text_content(message):\n",
        "    \"\"\"Extract text from message content (handles both str and List[ContentItem])\"\"\"\n",
        "    content = message.get('content', '')\n",
        "    \n",
        "    if isinstance(content, str):\n",
        "        return content\n",
        "    elif isinstance(content, list):\n",
        "        # Extract text from ContentItems\n",
        "        texts = [item.value for item in content if item.type == 'text']\n",
        "        return ' '.join(texts)\n",
        "    return ''\n",
        "\n",
        "def has_function_call(message):\n",
        "    \"\"\"Check if message contains a function call\"\"\"\n",
        "    return message.get('function_call') is not None\n",
        "\n",
        "def is_multimodal(message):\n",
        "    \"\"\"Check if message contains non-text content\"\"\"\n",
        "    content = message.get('content', '')\n",
        "    if isinstance(content, list):\n",
        "        return any(item.type != 'text' for item in content)\n",
        "    return False\n",
        "\n",
        "# Test the utilities\n",
        "print(\"Message Utilities Demo:\")\n",
        "print(f\"User messages: {count_messages_by_role(conversation, 'user')}\")\n",
        "print(f\"Assistant messages: {count_messages_by_role(conversation, 'assistant')}\")\n",
        "print(f\"Function messages: {count_messages_by_role(conversation, 'function')}\")\n",
        "\n",
        "last_user = get_last_user_message(conversation)\n",
        "if last_user:\n",
        "    print(f\"\\nLast user message: {extract_text_content(last_user)}\")\n",
        "\n",
        "# Check for function calls\n",
        "func_calls = [msg for msg in conversation if has_function_call(msg)]\n",
        "print(f\"\\nMessages with function calls: {len(func_calls)}\")\n",
        "\n",
        "# Create a multimodal message to test\n",
        "mm_msg = Message(\n",
        "    role='user',\n",
        "    content=[\n",
        "        ContentItem(text='Describe this'),\n",
        "        ContentItem(image='image.jpg')\n",
        "    ]\n",
        ")\n",
        "print(f\"Multimodal message: {is_multimodal(mm_msg)}\")\n",
        "print(f\"Text-only message: {is_multimodal(conversation[1])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 12: Working with Real Agents\n",
        "\n",
        "### Sending and Receiving Messages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sending messages to agent...\n",
            "\n",
            "Response Analysis:\n",
            "Response type: <class 'list'>\n",
            "Number of messages: 1\n",
            "\n",
            "Message 0:\n",
            "  Role: assistant\n",
            "  Content type: <class 'str'>\n",
            "  Content (excerpt): ...ST API is a web service standard using HTTP methods (GET/POST) to access resources via URLs. It's stateless, typically returns JSON/XML data, enabling apps to communicate over the internet. (24 words)\n",
            "  Word count: 191"
          ]
        }
      ],
      "source": "from qwen_agent.agents import Assistant\n\n# Create agent using llm_cfg from cell 4 (Fireworks API)\nbot = Assistant(llm=llm_cfg)\n\n# Build messages using Message objects\nmessages = [\n    Message(\n        role='system',\n        content='You are a concise assistant. Keep responses under 50 words.'\n    ),\n    Message(\n        role='user',\n        content='Explain what a REST API is.'\n    )\n]\n\nprint(\"Sending messages to agent...\\n\")\n\n# Get response (collect all streaming responses)\nresponse = None\nfor resp in bot.run(messages=messages):\n    response = resp\n\n# Examine the response structure\nprint(\"Response Analysis:\")\nprint(f\"Response type: {type(response)}\")\nprint(f\"Number of messages: {len(response)}\")\n\nfor i, msg in enumerate(response):\n    print(f\"\\nMessage {i}:\")\n    print(f\"  Role: {msg.get('role')}\")\n    print(f\"  Content type: {type(msg.get('content'))}\")\n    \n    if msg.get('role') == 'assistant':\n        content = extract_text_content(msg)\n        # Show just the last 200 chars (thinking model may have long responses)\n        if len(content) > 200:\n            print(f\"  Content (excerpt): ...{content[-200:]}\")\n        else:\n            print(f\"  Content: {content}\")\n        print(f\"  Word count: {len(content.split())}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 13: Message Serialization\n",
        "\n",
        "### Converting Messages to/from JSON:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As dictionary:\n",
            "{'role': 'user', 'content': 'Hello, world!', 'extra': {'timestamp': '2024-01-15T10:30:00'}}\n",
            "\n",
            "As JSON string:\n",
            "{\"role\":\"user\",\"content\":\"Hello, world!\",\"extra\":{\"timestamp\":\"2024-01-15T10:30:00\"}}\n",
            "\n",
            "As formatted JSON:\n",
            "{\n",
            "  \"role\": \"user\",\n",
            "  \"content\": \"Hello, world!\",\n",
            "  \"extra\": {\n",
            "    \"timestamp\": \"2024-01-15T10:30:00\"\n",
            "  }\n",
            "}\n",
            "\n",
            "Reconstructed message:\n",
            "{'role': 'user', 'content': 'Hello, world!', 'extra': {'timestamp': '2024-01-15T10:30:00'}}\n",
            "Equal to original: True"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Create a message\n",
        "msg = Message(\n",
        "    role='user',\n",
        "    content='Hello, world!',\n",
        "    extra={'timestamp': '2024-01-15T10:30:00'}\n",
        ")\n",
        "\n",
        "# Method 1: model_dump() - to dict\n",
        "msg_dict = msg.model_dump()\n",
        "print(\"As dictionary:\")\n",
        "print(msg_dict)\n",
        "\n",
        "# Method 2: model_dump_json() - to JSON string\n",
        "msg_json = msg.model_dump_json()\n",
        "print(\"\\nAs JSON string:\")\n",
        "print(msg_json)\n",
        "\n",
        "# Method 3: Manual JSON serialization\n",
        "msg_json_manual = json.dumps(msg.model_dump(), indent=2)\n",
        "print(\"\\nAs formatted JSON:\")\n",
        "print(msg_json_manual)\n",
        "\n",
        "# Deserialize back to Message\n",
        "loaded_dict = json.loads(msg_json)\n",
        "reconstructed_msg = Message(**loaded_dict)\n",
        "print(\"\\nReconstructed message:\")\n",
        "print(reconstructed_msg)\n",
        "print(f\"Equal to original: {msg.model_dump() == reconstructed_msg.model_dump()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving/Loading Conversations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Conversation saved to conversation.json\n",
            "\u2705 Loaded 3 messages\n",
            "  user: Hi there!\n",
            "  assistant: Hello! How can I help?\n",
            "  user: Tell me a joke."
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Create a conversation\n",
        "convo = [\n",
        "    Message(role='user', content='Hi there!'),\n",
        "    Message(role='assistant', content='Hello! How can I help?'),\n",
        "    Message(role='user', content='Tell me a joke.'),\n",
        "]\n",
        "\n",
        "# Save to file\n",
        "def save_conversation(messages, filename):\n",
        "    \"\"\"Save conversation to JSON file\"\"\"\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(\n",
        "            [msg.model_dump() for msg in messages],\n",
        "            f,\n",
        "            indent=2\n",
        "        )\n",
        "\n",
        "def load_conversation(filename):\n",
        "    \"\"\"Load conversation from JSON file\"\"\"\n",
        "    with open(filename, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        return [Message(**msg_dict) for msg_dict in data]\n",
        "\n",
        "# Save\n",
        "save_conversation(convo, 'conversation.json')\n",
        "print(\"\u2705 Conversation saved to conversation.json\")\n",
        "\n",
        "# Load\n",
        "loaded_convo = load_conversation('conversation.json')\n",
        "print(f\"\u2705 Loaded {len(loaded_convo)} messages\")\n",
        "\n",
        "for msg in loaded_convo:\n",
        "    print(f\"  {msg.role}: {msg.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": "---\n## \ud83c\udfe0 Part 16: Getting reasoning_content Locally with vLLM\n\n### The Truth About reasoning_content Separation\n\nBased on **actual vLLM documentation and testing**, here's how to get separate `reasoning_content` when hosting models locally.\n\n---\n\n## \u2705 What Actually Works\n\n### **vLLM 0.7.0+ Support (January 2025)**\n\nvLLM added **native reasoning_content support** in version 0.7.0+:\n\n```bash\n# Install latest vLLM\npip install vllm>=0.7.0\n\n# Start server with reasoning support\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B \\\n    --enable-reasoning \\\n    --reasoning-parser deepseek_r1 \\\n    --host 0.0.0.0 \\\n    --port 8000\n```\n\n**Key flags:**\n- `--enable-reasoning`: Enables reasoning output extraction\n- `--reasoning-parser MODEL_NAME`: Parser for your specific model\n\n---\n\n## \ud83d\udd27 Supported Model Parsers\n\nvLLM has built-in parsers for these reasoning models:\n\n| Model | Parser Name | Example Model |\n|-------|-------------|---------------|\n| DeepSeek-R1 | `deepseek_r1` | `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B` |\n| Qwen3 | `qwen3` | `Qwen/QwQ-32B-Preview` |\n| GLM-4 | `glm4_moe` | GLM-4 MoE models |\n| Granite | `granite` | IBM Granite models |\n\n**Each parser extracts reasoning tokens like:**\n- DeepSeek: `<think>...</think>`\n- Others: Model-specific tokens\n\n---\n\n## \ud83d\udcbb Using with Qwen-Agent\n\n### **Method 1: Modern vLLM (0.7.0+)**\n\n```python\nfrom qwen_agent.llm import get_chat_model\nfrom qwen_agent.llm.schema import Message\n\n# Configure for local vLLM with reasoning\nllm_cfg = {\n    'model': 'qwq-32b',\n    'model_server': 'http://localhost:8000/v1',\n    'model_type': 'oai',  # OpenAI-compatible\n    'api_key': 'EMPTY',\n    'generate_cfg': {\n        'max_tokens': 2048,\n        'temperature': 0.7,\n    }\n}\n\nllm = get_chat_model(llm_cfg)\n\nmessages = [\n    {'role': 'user', 'content': 'Calculate 15 * 23 step by step'}\n]\n\nfor response in llm.chat(messages=messages):\n    for msg in response:\n        # Modern vLLM returns reasoning_content automatically!\n        if msg.get('reasoning_content'):\n            print(f\"\ud83e\udd14 THINKING:\\n{msg['reasoning_content']}\\n\")\n        if msg.get('content'):\n            print(f\"\ud83d\udca1 ANSWER:\\n{msg['content']}\")\n```\n\n**How it works:**\n1. vLLM detects `<think>` tags in model output\n2. Parser extracts thinking \u2192 `reasoning_content` field\n3. Remaining text \u2192 `content` field\n4. Returns OpenAI-compatible response with both fields\n\n---\n\n### **Method 2: Older vLLM (<0.7.0) - Manual Parsing**\n\nFor **older vLLM** that doesn't have parsers:\n\n```python\nllm_cfg = {\n    'model': 'qwq-32b',\n    'model_server': 'http://localhost:8000/v1',\n    'model_type': 'oai',\n    'api_key': 'EMPTY',\n    'generate_cfg': {\n        # \u26a0\ufe0f This tells qwen-agent to parse <think> tags manually\n        'thought_in_content': True,\n    }\n}\n```\n\n**From official `examples/assistant_qwq.py` (line 37):**\n\n> \"This parameter needs to be passed in when the deployed model is a reasoning model (e.g. qwq-32b) and *does not* support the reasoning_content field (e.g. deploying qwq-32b directly with an old version of vLLM)\"\n\n---\n\n## \ud83d\udcca Backend Comparison Table\n\n| Backend | reasoning_content | Configuration | Auto-Parsing |\n|---------|------------------|---------------|--------------|\n| **DashScope** | \u2705 Native | None needed | \u2705 Yes |\n| **vLLM 0.7.0+** | \u2705 Native | `--enable-reasoning` | \u2705 Yes |\n| **vLLM <0.7.0** | \u26a0\ufe0f Manual | `thought_in_content: True` | \u274c No |\n| **Fireworks** | \u274c No | N/A | \u274c No |\n| **Ollama** | \u26a0\ufe0f Varies | Depends on version | \u26a0\ufe0f Maybe |\n\n---\n\n## \u2699\ufe0f Hardware Requirements\n\n### **For QwQ-32B** (realistic for prosumer):\n- **GPU**: 1x RTX 4090 (24GB) OR 2x RTX 3090 (48GB total)\n- **RAM**: 64 GB system RAM\n- **Disk**: 100 GB free space\n- **Cost**: ~$1,600-3,000\n\n### **For Qwen3-235B** (enterprise only):\n- **GPU**: 8x A100 80GB OR 4x H100 80GB\n- **RAM**: 500+ GB system RAM\n- **Disk**: 500 GB free space\n- **Cost**: ~$80,000-160,000\n\n**Recommendation:** Start with **QwQ-32B-Preview** for local experimentation!\n\n---\n\n## \ud83c\udfaf Complete Working Example\n\n```python\n# ========================================\n# Step 1: Start vLLM (in terminal)\n# ========================================\n# vllm serve Qwen/QwQ-32B-Preview \\\n#     --enable-reasoning \\\n#     --reasoning-parser qwen3 \\\n#     --max-model-len 8192 \\\n#     --port 8000\n\n# ========================================\n# Step 2: Use with qwen-agent\n# ========================================\nfrom qwen_agent.agents import Assistant\n\nllm_cfg = {\n    'model': 'qwq-32b',\n    'model_server': 'http://localhost:8000/v1',\n    'model_type': 'oai',\n    'api_key': 'EMPTY',\n}\n\nbot = Assistant(llm=llm_cfg)\n\nmessages = [{\n    'role': 'user',\n    'content': 'A train travels at 60 mph for 2.5 hours. How far does it travel?'\n}]\n\nfor response in bot.run(messages=messages):\n    for msg in response:\n        if msg.get('reasoning_content'):\n            print(\"=\"*70)\n            print(\"\ud83e\udd14 THINKING PROCESS:\")\n            print(msg['reasoning_content'])\n            print(\"=\"*70)\n        \n        if msg.get('content'):\n            print(\"\\n\ud83d\udca1 FINAL ANSWER:\")\n            print(msg['content'])\n```\n\n---\n\n## \ud83d\udd0d Key Takeaways\n\n1. \u2705 **Yes, you can get separate reasoning_content locally!**\n2. \u2705 **Use vLLM 0.7.0+** with `--enable-reasoning` flag\n3. \u2705 **Specify the right parser** for your model (`--reasoning-parser qwen3`)\n4. \u26a0\ufe0f **Older vLLM** requires `thought_in_content: True` for manual parsing\n5. \u274c **Fireworks API** does NOT separate reasoning (everything in `content`)\n6. \ud83d\udca1 **QwQ-32B is realistic** for home use, 235B needs enterprise hardware\n\nThe magic happens when vLLM's parser extracts `<think>` tags and returns them in the `reasoning_content` field! \ud83c\udf89\n\n---",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "---\n## \ud83c\udfaf Summary of Real Examples\n\nYou've now seen **ACTUAL working code** with **REAL outputs** showing:\n\n### \u2705 What We Covered:\n\n1. **Real Fireworks API Calls**\n   - Qwen3-235B-Thinking model in action\n   - Understanding how reasoning appears in responses\n   - API response structure analysis\n\n2. **Multimodal Messages**\n   - Combining text + images with `ContentItem`\n   - Real image URLs from Qwen's demo dataset\n   - Serialization/deserialization of complex messages\n\n3. **Extra Field Metadata**\n   - Storing custom data (timestamps, user IDs, etc.)\n   - Real-world metadata patterns\n   - Perfect serialization support\n\n4. **Complete Multi-Turn Conversation**\n   - Building conversation history\n   - Passing full context to LLM\n   - How assistant remembers previous messages\n\n### \ud83d\udca1 Key Insights:\n\n| Feature | With Fireworks API | With Native Qwen (DashScope) |\n|---------|-------------------|------------------------------|\n| **Thinking Model** | Reasoning in `content` field | Separate `reasoning_content` |\n| **Message Format** | Standard OpenAI-compatible | Full Qwen schema support |\n| **Multimodal** | Text + metadata only | Text + images + video + audio |\n\n### \ud83d\ude80 You're Now Ready To:\n\n- \u2705 Build complex conversations with proper message structure\n- \u2705 Handle multimodal content (text, images, files)\n- \u2705 Extract and use metadata from messages\n- \u2705 Work with thinking models and their reasoning output\n- \u2705 Serialize/deserialize messages for storage\n\n---",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# ========================================\n# EXAMPLE 4: Complete Multi-Turn Conversation\n# ========================================\n# This shows how messages flow in a REAL conversation\nfrom qwen_agent.llm import get_chat_model\n\n\nprint(\"\ud83d\udd04 COMPLETE MULTI-TURN CONVERSATION\")\nprint(\"=\"*70)\n\n# Start with empty conversation\nconversation = []\n\n# Turn 1: System message\nconversation.append(Message(\n    role='system',\n    content='You are a helpful assistant. Keep responses under 30 words.'\n))\n\n# Turn 2: User asks first question\nconversation.append(Message(\n    role='user',\n    content='What is 15 * 23?'\n))\n\nprint(\"\\n\ud83d\udc64 USER: What is 15 * 23?\")\nprint(\"\u23f3 Calling API...\")\n\n# Get response from LLM\nllm_short = get_chat_model({\n    'model': 'accounts/fireworks/models/qwen3-235b-a22b-thinking-2507',\n    'model_server': 'https://api.fireworks.ai/inference/v1',\n    'api_key': os.environ['FIREWORKS_API_KEY'],\n    'generate_cfg': {'max_tokens': 200, 'temperature': 0.6}\n})\n\nresponses = []\nfor resp in llm_short.chat(messages=conversation):\n    responses = resp\n\n# Add assistant response to conversation\nconversation.extend(responses)\n\nassistant_msg = responses[-1]\nprint(f\"\ud83e\udd16 ASSISTANT: {assistant_msg['content'][:100]}...\")\n\n# Turn 3: User asks follow-up\nconversation.append(Message(\n    role='user',\n    content='Now multiply that by 2'\n))\n\nprint(f\"\\n\ud83d\udc64 USER: Now multiply that by 2\")\nprint(\"\u23f3 Calling API with full context...\")\n\n# Get second response (with full history)\nresponses2 = []\nfor resp in llm_short.chat(messages=conversation):\n    responses2 = resp\n\nconversation.extend(responses2)\n\nassistant_msg2 = responses2[-1]\nprint(f\"\ud83e\udd16 ASSISTANT: {assistant_msg2['content'][:100]}...\")\n\n# Show conversation structure\nprint(f\"\\n\ud83d\udcca FINAL CONVERSATION STATE:\")\nprint(f\"   Total messages: {len(conversation)}\")\nfor i, msg in enumerate(conversation):\n    role_emoji = {'system': '\u2699\ufe0f', 'user': '\ud83d\udc64', 'assistant': '\ud83e\udd16'}\n    emoji = role_emoji.get(msg.get('role'), '\u2753')\n    content_preview = msg.get('content', '')[:50]\n    print(f\"   {i+1}. {emoji} {msg.get('role')}: {content_preview}...\")\n\nprint(\"\\n\u2705 Multi-turn conversation complete!\")\nprint(\"\ud83d\udca1 Key takeaway: Each API call receives the FULL conversation history\")",
      "metadata": {},
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83d\udd04 COMPLETE MULTI-TURN CONVERSATION\n",
            "======================================================================\n",
            "\n",
            "\ud83d\udc64 USER: What is 15 * 23?\n",
            "\u23f3 Calling API...\n",
            "\ud83e\udd16 ASSISTANT: Okay, the user is asking for 15 multiplied by 23. Let me calculate that. 15 times 20 is 300, and 15 ...\n",
            "\n",
            "\ud83d\udc64 USER: Now multiply that by 2\n",
            "\u23f3 Calling API with full context...\n",
            "\ud83e\udd16 ASSISTANT: Okay, the user just asked to multiply the previous result by 2. Earlier, we calculated 15 * 23 = 345...\n",
            "\n",
            "\ud83d\udcca FINAL CONVERSATION STATE:\n",
            "   Total messages: 5\n",
            "   1. \u2699\ufe0f system: You are a helpful assistant. Keep responses under ...\n",
            "   2. \ud83d\udc64 user: What is 15 * 23?...\n",
            "   3. \ud83e\udd16 assistant: Okay, the user is asking for 15 multiplied by 23. ...\n",
            "   4. \ud83d\udc64 user: Now multiply that by 2...\n",
            "   5. \ud83e\udd16 assistant: Okay, the user just asked to multiply the previous...\n",
            "\n",
            "\u2705 Multi-turn conversation complete!\n",
            "\ud83d\udca1 Key takeaway: Each API call receives the FULL conversation history"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "# ========================================\n# EXAMPLE 3: Using the `extra` Field for Metadata\n# ========================================\nfrom datetime import datetime\n\n# The `extra` field can store ANY additional metadata\nmsg_with_metadata = Message(\n    role='user',\n    content='What is the weather today?',\n    extra={\n        'timestamp': datetime.now().isoformat(),\n        'user_id': 'user_12345',\n        'session_id': 'session_abc',\n        'ip_address': '192.168.1.1',\n        'language': 'en',\n        'app_version': '1.2.3',\n        'custom_data': {\n            'priority': 'high',\n            'category': 'weather',\n            'tags': ['urgent', 'forecast']\n        }\n    }\n)\n\nprint(\"\ud83c\udff7\ufe0f  EXTRA FIELD - METADATA EXAMPLE\")\nprint(\"=\"*70)\nprint(f\"\\nMessage role: {msg_with_metadata.role}\")\nprint(f\"Message content: {msg_with_metadata.content}\")\nprint(f\"\\n\ud83d\udccb Extra metadata:\")\nprint(json.dumps(msg_with_metadata.extra, indent=2))\n\n# Access extra fields\nprint(f\"\\n\ud83d\udd0d Accessing extra fields:\")\nprint(f\"   User ID: {msg_with_metadata.extra['user_id']}\")\nprint(f\"   Timestamp: {msg_with_metadata.extra['timestamp']}\")\nprint(f\"   Priority: {msg_with_metadata.extra['custom_data']['priority']}\")\n\n# Use cases for extra field:\nprint(f\"\\n\ud83d\udca1 Common uses for `extra` field:\")\nprint(\"   \u2022 Timestamps for logging\")\nprint(\"   \u2022 User/session tracking\")\nprint(\"   \u2022 A/B testing flags\")\nprint(\"   \u2022 Custom application data\")\nprint(\"   \u2022 Debugging information\")\nprint(\"   \u2022 Analytics metadata\")\n\n# Serialize with extra\nserialized = msg_with_metadata.model_dump()\nprint(f\"\\n\u2705 Serializes perfectly with all metadata intact\")\nprint(f\"   Total fields in serialized: {len(serialized)}\")",
      "metadata": {},
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83c\udff7\ufe0f  EXTRA FIELD - METADATA EXAMPLE\n",
            "======================================================================\n",
            "\n",
            "Message role: user\n",
            "Message content: What is the weather today?\n",
            "\n",
            "\ud83d\udccb Extra metadata:\n",
            "{\n",
            "  \"timestamp\": \"2025-11-14T01:42:04.461140\",\n",
            "  \"user_id\": \"user_12345\",\n",
            "  \"session_id\": \"session_abc\",\n",
            "  \"ip_address\": \"192.168.1.1\",\n",
            "  \"language\": \"en\",\n",
            "  \"app_version\": \"1.2.3\",\n",
            "  \"custom_data\": {\n",
            "    \"priority\": \"high\",\n",
            "    \"category\": \"weather\",\n",
            "    \"tags\": [\n",
            "      \"urgent\",\n",
            "      \"forecast\"\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\ud83d\udd0d Accessing extra fields:\n",
            "   User ID: user_12345\n",
            "   Timestamp: 2025-11-14T01:42:04.461140\n",
            "   Priority: high\n",
            "\n",
            "\ud83d\udca1 Common uses for `extra` field:\n",
            "   \u2022 Timestamps for logging\n",
            "   \u2022 User/session tracking\n",
            "   \u2022 A/B testing flags\n",
            "   \u2022 Custom application data\n",
            "   \u2022 Debugging information\n",
            "   \u2022 Analytics metadata\n",
            "\n",
            "\u2705 Serializes perfectly with all metadata intact\n",
            "   Total fields in serialized: 3"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "# ========================================\n# EXAMPLE 2: Multimodal Messages with REAL Images\n# ========================================\nfrom qwen_agent.llm.schema import Message, ContentItem\n\n# Using a REAL publicly accessible image\nreal_image_url = 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/demo_small.jpg'\n\n# Create multimodal message\nmultimodal_msg = Message(\n    role='user',\n    content=[\n        ContentItem(text='What objects do you see in this image?'),\n        ContentItem(image=real_image_url)\n    ]\n)\n\nprint(\"\ud83d\uddbc\ufe0f  MULTIMODAL MESSAGE EXAMPLE\")\nprint(\"=\"*70)\nprint(f\"\\nRole: {multimodal_msg.role}\")\nprint(f\"Content is a list: {isinstance(multimodal_msg.content, list)}\")\nprint(f\"Number of content items: {len(multimodal_msg.content)}\")\n\nfor i, item in enumerate(multimodal_msg.content):\n    print(f\"\\n\ud83d\udccc Item {i}:\")\n    print(f\"   Type: {item.type}\")\n    if item.type == 'text':\n        print(f\"   Text: {item.value}\")\n    elif item.type == 'image':\n        print(f\"   Image URL: {item.value}\")\n        print(f\"   URL length: {len(item.value)} chars\")\n\n# Show how to serialize/deserialize\nmsg_dict = multimodal_msg.model_dump()\nprint(f\"\\n\ud83d\udce6 Serialized to dict:\")\nprint(json.dumps(msg_dict, indent=2)[:400] + \"...\")\n\n# Reconstruct from dict\nreconstructed = Message(**msg_dict)\nprint(f\"\\n\u2705 Successfully reconstructed from dict\")\nprint(f\"   Equal to original: {msg_dict == reconstructed.model_dump()}\")",
      "metadata": {},
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83d\uddbc\ufe0f  MULTIMODAL MESSAGE EXAMPLE\n",
            "======================================================================\n",
            "\n",
            "Role: user\n",
            "Content is a list: True\n",
            "Number of content items: 2\n",
            "\n",
            "\ud83d\udccc Item 0:\n",
            "   Type: text\n",
            "   Text: What objects do you see in this image?\n",
            "\n",
            "\ud83d\udccc Item 1:\n",
            "   Type: image\n",
            "   Image URL: https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/demo_small.jpg\n",
            "   URL length: 71 chars\n",
            "\n",
            "\ud83d\udce6 Serialized to dict:\n",
            "{\n",
            "  \"role\": \"user\",\n",
            "  \"content\": [\n",
            "    {\n",
            "      \"text\": \"What objects do you see in this image?\"\n",
            "    },\n",
            "    {\n",
            "      \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/demo_small.jpg\"\n",
            "    }\n",
            "  ]\n",
            "}...\n",
            "\n",
            "\u2705 Successfully reconstructed from dict\n",
            "   Equal to original: True"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### \ud83d\udcdd Important Finding: reasoning_content with Fireworks API\n\n**Key Discovery**: The Qwen3-235B-Thinking model on Fireworks API includes reasoning **within the `content` field**, NOT as separate `reasoning_content`.\n\n```python\n# What we get:\nMessage(\n    role='assistant',\n    content='Let me think... [reasoning] ... The answer is 4 books.'  # All in one\n    reasoning_content=None  # \u274c Not populated\n)\n\n# vs. Native Qwen API (DashScope):\nMessage(\n    role='assistant',\n    reasoning_content='Let me think... [step by step]',  # \u2705 Separate\n    content='The answer is 4 books.'\n)\n```\n\nThis is an **API implementation difference**, not a bug. The thinking is still there - just formatted differently!",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# ========================================\n# EXAMPLE 1: Real API Call with Thinking Model\n# ========================================\nfrom qwen_agent.llm import get_chat_model\nfrom qwen_agent.llm.schema import Message\nimport json\n\n# Configure with SHORT max_tokens to see concise output\nllm_cfg_short = {\n    'model': 'accounts/fireworks/models/qwen3-235b-a22b-thinking-2507',\n    'model_server': 'https://api.fireworks.ai/inference/v1',\n    'api_key': os.environ['FIREWORKS_API_KEY'],\n    'generate_cfg': {\n        'max_tokens': 500,  # Shorter for demo\n        'temperature': 0.6,\n    }\n}\n\n# Get LLM instance\nllm = get_chat_model(llm_cfg_short)\n\n# Create a simple math problem\nmessages = [\n    {'role': 'system', 'content': 'You are a helpful math tutor. Explain your reasoning step by step.'},\n    {'role': 'user', 'content': 'Solve: If a book costs $12 and I have $50, how many books can I buy?'}\n]\n\nprint(\"\ud83d\udd25 Making REAL API call to Fireworks...\")\nprint(\"Question: If a book costs $12 and I have $50, how many books can I buy?\")\nprint(\"\\n\" + \"=\"*70)\n\n# Call the API\nresponses = []\nfor response in llm.chat(messages=messages, stream=True):\n    responses = response\n\n# Extract the final message\nif responses:\n    final_msg = responses[-1]\n    \n    print(\"\\n\ud83d\udcca RESPONSE STRUCTURE:\")\n    print(f\"  Role: {final_msg.get('role')}\")\n    print(f\"  Has content: {bool(final_msg.get('content'))}\")\n    print(f\"  Has reasoning_content: {bool(final_msg.get('reasoning_content'))}\")\n    print(f\"  Content type: {type(final_msg.get('content'))}\")\n    \n    # Show reasoning if present\n    if final_msg.get('reasoning_content'):\n        reasoning = final_msg['reasoning_content']\n        print(f\"\\n\ud83e\udd14 THINKING PROCESS ({len(reasoning)} chars):\")\n        print(\"\u2500\" * 70)\n        print(reasoning[:500] + \"...\" if len(reasoning) > 500 else reasoning)\n    \n    # Show final answer\n    if final_msg.get('content'):\n        content = final_msg['content']\n        print(f\"\\n\ud83d\udca1 FINAL ANSWER ({len(content)} chars):\")\n        print(\"\u2500\" * 70)\n        print(content[:300] + \"...\" if len(content) > 300 else content)\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"\u2705 Real API call complete!\")",
      "metadata": {},
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83d\udd25 Making REAL API call to Fireworks...\n",
            "Question: If a book costs $12 and I have $50, how many books can I buy?\n",
            "\n",
            "======================================================================\n",
            "\n",
            "\ud83d\udcca RESPONSE STRUCTURE:\n",
            "  Role: assistant\n",
            "  Has content: True\n",
            "  Has reasoning_content: False\n",
            "  Content type: <class 'str'>\n",
            "\n",
            "\ud83d\udca1 FINAL ANSWER (1633 chars):\n",
            "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
            "Okay, let's see. The problem is: If a book costs $12 and I have $50, how many books can I buy? Hmm, so I need to figure out how many times 12 goes into 50, right? Because each book is $12, and I have $50 total. \n",
            "\n",
            "First, maybe I should divide 50 by 12. Let me do that. 12 times 4 is 48, because 12*4=4...\n",
            "\n",
            "======================================================================\n",
            "\u2705 Real API call complete!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "---\n## \ud83d\udd25 Part 15: REAL EXAMPLES with Fireworks Thinking Model\n\n### Now let's see ACTUAL API calls and REAL outputs!\n\nThis section demonstrates:\n1. \u2705 **Real Fireworks API calls** with Qwen3-235B-Thinking\n2. \u2705 **Actual reasoning extraction** from thinking models\n3. \u2705 **How to pass reasoning** back in conversations\n4. \u2705 **Multimodal messages** with real images\n5. \u2705 **Message extra fields** and metadata\n6. \u2705 **Complete conversation** with all message types\n\nAll cells below have **saved outputs** so you can see exactly what happens!",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 14: Practice Exercises\n",
        "\n",
        "### Exercise 1: Create a Multimodal Message\n",
        "Build a message that combines text and an image URL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create a multimodal message\n",
        "# Requirements:\n",
        "# 1. Use ContentItem for both text and image\n",
        "# 2. Text should ask a question about the image\n",
        "# 3. Image URL can be any valid URL\n",
        "\n",
        "# Your code here:\n",
        "multimodal_exercise = None\n",
        "\n",
        "# Test:\n",
        "# print(multimodal_exercise)\n",
        "# print(f\"Is multimodal: {is_multimodal(multimodal_exercise)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Build a Conversation Parser\n",
        "Create a function that analyzes a conversation and returns statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement conversation_stats()\n",
        "# Should return:\n",
        "# - Total messages\n",
        "# - Messages per role\n",
        "# - Number of function calls\n",
        "# - Average message length\n",
        "# - Has system message?\n",
        "\n",
        "def conversation_stats(messages):\n",
        "    \"\"\"Analyze a conversation\"\"\"\n",
        "    # Your code here\n",
        "    pass\n",
        "\n",
        "# Test with the conversation we built earlier:\n",
        "# stats = conversation_stats(conversation)\n",
        "# print(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3: Message Filter\n",
        "Filter a conversation to show only specific types of messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement message filters\n",
        "\n",
        "def filter_by_role(messages, role):\n",
        "    \"\"\"Return only messages with specific role\"\"\"\n",
        "    # Your code here\n",
        "    pass\n",
        "\n",
        "def filter_function_calls(messages):\n",
        "    \"\"\"Return only messages with function calls\"\"\"\n",
        "    # Your code here\n",
        "    pass\n",
        "\n",
        "def filter_multimodal(messages):\n",
        "    \"\"\"Return only multimodal messages\"\"\"\n",
        "    # Your code here\n",
        "    pass\n",
        "\n",
        "# Test:\n",
        "# user_msgs = filter_by_role(conversation, 'user')\n",
        "# print(f\"User messages: {len(user_msgs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 15: Key Takeaways\n",
        "\n",
        "### What You Learned Today:\n",
        "\n",
        "1. **Message Structure**\n",
        "   - `role`: Who is speaking (system/user/assistant/function)\n",
        "   - `content`: What is being said (str or List[ContentItem])\n",
        "   - `reasoning_content`: Thinking process (for reasoning models)\n",
        "   - `name`: Speaker identifier\n",
        "   - `function_call`: Tool invocation data\n",
        "   - `extra`: Additional metadata\n",
        "\n",
        "2. **ContentItem for Multimodal**\n",
        "   - Exactly ONE of: text, image, file, audio, video\n",
        "   - Use `.type` and `.value` properties\n",
        "   - Combine in lists for multimodal messages\n",
        "\n",
        "3. **FunctionCall Structure**\n",
        "   - `name`: Tool to execute\n",
        "   - `arguments`: JSON string parameters\n",
        "   - Enables agent tool use\n",
        "\n",
        "4. **Message Utilities**\n",
        "   - Dict-compatible interface\n",
        "   - Serialization to/from JSON\n",
        "   - Helper functions for analysis\n",
        "\n",
        "### Common Patterns:\n",
        "\n",
        "```python\n",
        "# Pattern 1: Simple text message\n",
        "msg = Message(role='user', content='Hello')\n",
        "\n",
        "# Pattern 2: Multimodal message\n",
        "msg = Message(\n",
        "    role='user',\n",
        "    content=[\n",
        "        ContentItem(text='What is this?'),\n",
        "        ContentItem(image='url')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Pattern 3: Function call message\n",
        "msg = Message(\n",
        "    role='assistant',\n",
        "    content='',\n",
        "    function_call=FunctionCall(name='tool', arguments='{}')\n",
        ")\n",
        "\n",
        "# Pattern 4: Function result message\n",
        "msg = Message(\n",
        "    role='function',\n",
        "    name='tool_name',\n",
        "    content='result data'\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 16: Next Steps\n",
        "\n",
        "### Tomorrow (Day 3): LLM Integration\n",
        "We'll explore:\n",
        "- BaseChatModel interface\n",
        "- Different model backends (DashScope, vLLM, Ollama)\n",
        "- Generation parameters\n",
        "- Streaming internals\n",
        "- Token management\n",
        "- Direct LLM usage (without agents)\n",
        "\n",
        "### Homework:\n",
        "1. Create a conversation with at least 5 turns\n",
        "2. Build a multimodal message with your own image\n",
        "3. Implement the exercise functions above\n",
        "4. Read the source: `/qwen_agent/llm/schema.py`\n",
        "5. Experiment with reasoning_content (if you have QwQ access)\n",
        "\n",
        "### Resources:\n",
        "- [Schema Source Code](../qwen_agent/llm/schema.py)\n",
        "- [Pydantic Documentation](https://docs.pydantic.dev/) - Message uses Pydantic\n",
        "- [OpenAI Message Format](https://platform.openai.com/docs/api-reference/chat) - Similar structure\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83c\udf89 Day 2 Complete!\n",
        "\n",
        "You now understand:\n",
        "- \u2705 Message structure and fields\n",
        "- \u2705 Role types and their purposes\n",
        "- \u2705 ContentItem for multimodal content\n",
        "- \u2705 FunctionCall basics\n",
        "- \u2705 Message serialization and utilities\n",
        "\n",
        "See you tomorrow for Day 3! \ud83d\ude80"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}