{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3: LLM Integration\n",
    "\n",
    "## Understanding How Qwen-Agent Talks to Language Models\n",
    "\n",
    "### Today's Learning Objectives:\n",
    "1. Understand the `BaseChatModel` interface\n",
    "2. Learn about different model backends (DashScope, OpenAI-compatible, local)\n",
    "3. Configure LLMs with various parameters\n",
    "4. Call LLMs directly (without agents)\n",
    "5. Master streaming responses\n",
    "6. Understand token counting and context windows\n",
    "\n",
    "### Prerequisites:\n",
    "- Completed Day 1 & 2\n",
    "- Understanding of Messages\n",
    "- API key configured\n",
    "\n",
    "### Time Required: 1.5-2 hours\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The LLM Abstraction Layer\n",
    "\n",
    "### Why Abstract LLMs?\n",
    "\n",
    "Qwen-Agent supports many different LLM providers:\n",
    "- **DashScope** (Alibaba Cloud's Qwen models)\n",
    "- **OpenAI** (GPT-4, GPT-3.5)\n",
    "- **vLLM** (Self-hosted high-performance)\n",
    "- **Ollama** (Local CPU/GPU)\n",
    "- **Together.AI, Azure, and more**\n",
    "\n",
    "Without abstraction, you'd need different code for each provider. With `BaseChatModel`, you write once and switch providers by changing configuration!\n",
    "\n",
    "### Architecture:\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502        Your Application             \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "               \u2502\n",
    "               v\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502       BaseChatModel Interface       \u2502  \u2190 Uniform API\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "               \u2502\n",
    "     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "     v         v         v         v\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502DashScope\u2502 \u2502OpenAI\u2502 \u2502 vLLM \u2502 \u2502 Ollama \u2502  \u2190 Different backends\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "### Key Insight:\n",
    "The agent doesn't care which LLM it talks to - it just sends messages and receives responses through the `BaseChatModel` interface!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: BaseChatModel Interface\n",
    "\n",
    "### Source Code Location:\n",
    "`/qwen_agent/llm/base.py` (lines 61-100)\n",
    "\n",
    "### Key Properties:\n",
    "\n",
    "```python\n",
    "class BaseChatModel(ABC):\n",
    "    @property\n",
    "    def support_multimodal_input(self) -> bool:\n",
    "        # Can this model accept images/audio/video?\n",
    "        return False\n",
    "    \n",
    "    @property\n",
    "    def support_multimodal_output(self) -> bool:\n",
    "        # Can this model generate images/audio/video?\n",
    "        return False\n",
    "    \n",
    "    @property\n",
    "    def support_audio_input(self) -> bool:\n",
    "        return False\n",
    "```\n",
    "\n",
    "### Key Methods:\n",
    "- `chat(messages, stream=True/False)` - Main inference method\n",
    "- `_chat_stream()` - Streaming implementation (abstract)\n",
    "- `_chat_no_stream()` - Non-streaming implementation (abstract)\n",
    "\n",
    "Each LLM backend implements these methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Getting an LLM Instance\n",
    "\n",
    "### The `get_chat_model()` Factory Function:\n",
    "\n",
    "```python\n",
    "from qwen_agent.llm import get_chat_model\n",
    "\n",
    "llm = get_chat_model(config_dict)\n",
    "```\n",
    "\n",
    "This function:\n",
    "1. Reads your configuration\n",
    "2. Determines which backend to use\n",
    "3. Returns the appropriate `BaseChatModel` subclass\n",
    "4. All ready to use!\n",
    "\n",
    "Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# FIREWORKS API CONFIGURATION\n",
    "# ================================================\n",
    "import os\n",
    "\n",
    "# Set API credentials\n",
    "os.environ['FIREWORKS_API_KEY'] = 'fw_3ZTLPrnEtuscTUPYy3sYx3ag'\n",
    "\n",
    "# Standard configuration for Fireworks Qwen3-235B-A22B-Thinking\n",
    "llm_cfg_fireworks = {\n",
    "    'model': 'accounts/fireworks/models/qwen3-235b-a22b-thinking-2507',\n",
    "    'model_server': 'https://api.fireworks.ai/inference/v1',\n",
    "    'api_key': os.environ['FIREWORKS_API_KEY'],\n",
    "    'generate_cfg': {\n",
    "        'max_tokens': 32768,\n",
    "        'temperature': 0.6,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Use this as default llm_cfg\n",
    "llm_cfg = llm_cfg_fireworks\n",
    "\n",
    "print('\u2705 Configured for Fireworks API')\n",
    "print(f'   Model: Qwen3-235B-A22B-Thinking-2507')\n",
    "print(f'   Max tokens: 32,768')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwen_agent.llm import get_chat_model\n",
    "import os\n",
    "\n",
    "# Basic configuration for DashScope\n",
    "llm_config = {\n",
    "    'model': 'qwen-max-latest',\n",
    "    # API key from environment variable\n",
    "}\n",
    "\n",
    "# Get the LLM instance\n",
    "llm = get_chat_model(llm_config)\n",
    "\n",
    "print(f\"LLM Type: {type(llm)}\")\n",
    "print(f\"Model: {llm.model}\")\n",
    "print(f\"Supports multimodal input: {llm.support_multimodal_input}\")\n",
    "print(f\"Supports multimodal output: {llm.support_multimodal_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Direct LLM Calling (Without Agent)\n",
    "\n",
    "### The `.chat()` Method:\n",
    "\n",
    "You can call LLMs directly without creating an agent:\n",
    "\n",
    "```python\n",
    "response = llm.chat(\n",
    "    messages=[...],\n",
    "    stream=False,          # True for streaming\n",
    "    functions=[...],       # Optional: for function calling\n",
    "    extra_generate_cfg={}  # Optional: additional parameters\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Simple non-streaming call\n",
    "messages = [\n",
    "    {'role': 'user', 'content': 'What is 15 + 27?'}\n",
    "]\n",
    "\n",
    "print(\"Calling LLM...\\n\")\n",
    "response = llm.chat(messages=messages, stream=False)\n",
    "\n",
    "print(\"Response:\")\n",
    "for msg in response:\n",
    "    print(f\"Role: {msg['role']}\")\n",
    "    print(f\"Content: {msg['content']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Response:\n",
    "\n",
    "The `.chat()` method returns a **list of messages**:\n",
    "- Non-streaming: Complete list immediately\n",
    "- Streaming: Progressive updates (iterator)\n",
    "\n",
    "Usually contains:\n",
    "- One or more assistant messages\n",
    "- Possibly function call messages\n",
    "- Same Message format we learned on Day 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Multi-turn conversation\n",
    "messages = [\n",
    "    {'role': 'user', 'content': 'My favorite color is blue.'},\n",
    "]\n",
    "\n",
    "# First turn\n",
    "response1 = llm.chat(messages=messages, stream=False)\n",
    "messages.extend(response1)\n",
    "\n",
    "# Second turn - LLM should remember\n",
    "messages.append({'role': 'user', 'content': 'What is my favorite color?'})\n",
    "response2 = llm.chat(messages=messages, stream=False)\n",
    "\n",
    "print(\"Full conversation:\")\n",
    "for msg in messages:\n",
    "    print(f\"{msg['role']:10} | {msg['content'][:60]}...\")\n",
    "for msg in response2:\n",
    "    print(f\"{msg['role']:10} | {msg['content'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Streaming vs. Non-Streaming Deep Dive\n",
    "\n",
    "### Non-Streaming (`stream=False`):\n",
    "\n",
    "```python\n",
    "response = llm.chat(messages, stream=False)\n",
    "# response is List[Message] - complete immediately\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- Simple to use\n",
    "- Get complete response at once\n",
    "- Easier error handling\n",
    "\n",
    "**Cons:**\n",
    "- User waits for entire response\n",
    "- Feels slower\n",
    "- No early feedback\n",
    "\n",
    "### Streaming (`stream=True`):\n",
    "\n",
    "```python\n",
    "for chunk in llm.chat(messages, stream=True):\n",
    "    # chunk is List[Message] - progressively complete\n",
    "    # Process incrementally\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- Lower perceived latency\n",
    "- Better UX (like ChatGPT)\n",
    "- Can start processing early\n",
    "\n",
    "**Cons:**\n",
    "- More complex to handle\n",
    "- Need to manage state\n",
    "- Harder error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "messages = [{'role': 'user', 'content': 'Count from 1 to 5 with explanations for each number.'}]\n",
    "\n",
    "print(\"Streaming Response:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "full_content = \"\"\n",
    "start_time = time.time()\n",
    "first_token_time = None\n",
    "\n",
    "for chunk in llm.chat(messages=messages, stream=True):\n",
    "    if chunk and chunk[-1]['role'] == 'assistant':\n",
    "        content = chunk[-1].get('content', '')\n",
    "        \n",
    "        # Track time to first token\n",
    "        if not first_token_time and content:\n",
    "            first_token_time = time.time() - start_time\n",
    "        \n",
    "        # Print incrementally\n",
    "        if content != full_content:\n",
    "            new_content = content[len(full_content):]\n",
    "            print(new_content, end='', flush=True)\n",
    "            full_content = content\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"\\nTime to first token: {first_token_time:.2f}s\")\n",
    "print(f\"Total time: {total_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Pattern:\n",
    "\n",
    "```python\n",
    "# Common pattern to get final result from streaming\n",
    "*_, final_response = llm.chat(messages, stream=True)\n",
    "\n",
    "# Equivalent to:\n",
    "for chunk in llm.chat(messages, stream=True):\n",
    "    final_response = chunk  # Keep updating\n",
    "# final_response now has complete result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: LLM Configuration Options\n",
    "\n",
    "### Basic Configuration:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'model': str,           # Model name (required)\n",
    "    'model_type': str,      # Backend type (optional, auto-detected)\n",
    "    'api_key': str,         # API key (optional, uses env var)\n",
    "    'model_server': str,    # API base URL (for custom endpoints)\n",
    "    'generate_cfg': dict,   # Generation parameters\n",
    "}\n",
    "```\n",
    "\n",
    "### Generation Parameters (`generate_cfg`):\n",
    "\n",
    "```python\n",
    "'generate_cfg': {\n",
    "    'top_p': float,              # Nucleus sampling (0.0-1.0)\n",
    "    'temperature': float,        # Randomness (model dependent)\n",
    "    'max_tokens': int,           # Max output length\n",
    "    'max_input_tokens': int,     # Max input length (for truncation)\n",
    "    'fncall_prompt_type': str,   # 'qwen' or 'nous' for function calling\n",
    "    'enable_thinking': bool,     # For reasoning models (QwQ)\n",
    "    'use_raw_api': bool,         # Use native API tool calling\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Creative vs. Deterministic\n",
    "\n",
    "# Configuration 1: Creative (high top_p)\n",
    "creative_llm = get_chat_model({\n",
    "    'model': 'qwen-max-latest',\n",
    "    'generate_cfg': {\n",
    "        'top_p': 0.95,  # More diverse outputs\n",
    "    }\n",
    "})\n",
    "\n",
    "# Configuration 2: Focused (low top_p)\n",
    "focused_llm = get_chat_model({\n",
    "    'model': 'qwen-max-latest',\n",
    "    'generate_cfg': {\n",
    "        'top_p': 0.3,  # More deterministic\n",
    "    }\n",
    "})\n",
    "\n",
    "prompt = [{'role': 'user', 'content': 'Give me a creative name for a pet robot.'}]\n",
    "\n",
    "print(\"Creative LLM (top_p=0.95):\")\n",
    "response = creative_llm.chat(messages=prompt, stream=False)\n",
    "print(response[-1]['content'])\n",
    "\n",
    "print(\"\\nFocused LLM (top_p=0.3):\")\n",
    "response = focused_llm.chat(messages=prompt, stream=False)\n",
    "print(response[-1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-P (Nucleus Sampling) Explained:\n",
    "\n",
    "```\n",
    "Top-P = 0.1 (Deterministic)\n",
    "Only consider most likely tokens\n",
    "Output: Predictable, focused\n",
    "Use for: Math, code, factual Q&A\n",
    "\n",
    "Top-P = 0.5 (Balanced)\n",
    "Moderate diversity\n",
    "Output: Natural, reasonable variety\n",
    "Use for: General conversation\n",
    "\n",
    "Top-P = 0.95 (Creative)\n",
    "Consider many possible tokens\n",
    "Output: Diverse, creative\n",
    "Use for: Storytelling, brainstorming\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Different Model Backends\n",
    "\n",
    "### Option 1: DashScope (Recommended for Qwen Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DashScope configuration\n",
    "dashscope_config = {\n",
    "    'model': 'qwen-max-latest',\n",
    "    'model_type': 'qwen_dashscope',  # Explicitly specify\n",
    "    # 'api_key': 'sk-xxx',  # Or use DASHSCOPE_API_KEY env var\n",
    "}\n",
    "\n",
    "dashscope_llm = get_chat_model(dashscope_config)\n",
    "print(f\"Using DashScope: {dashscope_llm.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: OpenAI-Compatible APIs\n",
    "\n",
    "Works with:\n",
    "- **OpenAI** (official API)\n",
    "- **vLLM** (self-hosted)\n",
    "- **Ollama** (local)\n",
    "- **Together.AI, Groq, etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: vLLM self-hosted\n",
    "# (Requires vLLM server running on localhost:8000)\n",
    "vllm_config = {\n",
    "    'model': 'Qwen2.5-7B-Instruct',\n",
    "    'model_server': 'http://localhost:8000/v1',  # OpenAI-compatible endpoint\n",
    "    'api_key': 'EMPTY',  # vLLM doesn't need real key\n",
    "}\n",
    "\n",
    "# Uncomment to test if you have vLLM running:\n",
    "# vllm_llm = get_chat_model(vllm_config)\n",
    "# print(f\"Using vLLM: {vllm_llm.model}\")\n",
    "\n",
    "print(\"vLLM config ready (commented out - needs server running)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Ollama (local)\n",
    "# (Requires Ollama installed and running)\n",
    "ollama_config = {\n",
    "    'model': 'qwen2.5:7b',\n",
    "    'model_server': 'http://localhost:11434/v1',\n",
    "    'api_key': 'EMPTY',\n",
    "}\n",
    "\n",
    "# Uncomment to test if you have Ollama:\n",
    "# ollama_llm = get_chat_model(ollama_config)\n",
    "# print(f\"Using Ollama: {ollama_llm.model}\")\n",
    "\n",
    "print(\"Ollama config ready (commented out - needs Ollama running)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Backends:\n",
    "\n",
    "| Backend | Pros | Cons | Best For |\n",
    "|---------|------|------|----------|\n",
    "| **DashScope** | Official Qwen, no setup, latest models | Requires internet, costs money | Production, quick start |\n",
    "| **vLLM** | Fast inference, self-hosted, free | Requires GPU, setup complexity | High throughput production |\n",
    "| **Ollama** | Easy local setup, CPU support | Slower, limited models | Development, offline work |\n",
    "| **OpenAI** | GPT-4 access, reliable | Expensive, not Qwen models | Comparison, benchmarking |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Advanced Configuration\n",
    "\n",
    "### Max Input Tokens (Context Window Management):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration with input limit\n",
    "limited_llm = get_chat_model({\n",
    "    'model': 'qwen-max-latest',\n",
    "    'generate_cfg': {\n",
    "        'max_input_tokens': 2000,  # Truncate if input too long\n",
    "    }\n",
    "})\n",
    "\n",
    "# Long conversation that might exceed limit\n",
    "long_messages = [\n",
    "    {'role': 'user', 'content': 'Tell me about Python.' * 100},  # Very long\n",
    "]\n",
    "\n",
    "# LLM will automatically truncate to fit 2000 tokens\n",
    "response = limited_llm.chat(messages=long_messages, stream=False)\n",
    "print(\"Response received (input was truncated if needed)\")\n",
    "print(response[-1]['content'][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Output Tokens (Response Length Limit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short responses\n",
    "concise_llm = get_chat_model({\n",
    "    'model': 'qwen-max-latest',\n",
    "    'generate_cfg': {\n",
    "        'max_tokens': 50,  # Short responses only\n",
    "    }\n",
    "})\n",
    "\n",
    "messages = [{'role': 'user', 'content': 'Explain quantum physics in detail.'}]\n",
    "response = concise_llm.chat(messages=messages, stream=False)\n",
    "\n",
    "print(\"Concise response (max 50 tokens):\")\n",
    "print(response[-1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Error Handling\n",
    "\n",
    "### Common Errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwen_agent.llm.base import ModelServiceError\n",
    "\n",
    "def safe_chat(llm, messages):\n",
    "    \"\"\"Chat with error handling\"\"\"\n",
    "    try:\n",
    "        response = llm.chat(messages=messages, stream=False)\n",
    "        return response\n",
    "    except ModelServiceError as e:\n",
    "        print(f\"Model service error: {e.message}\")\n",
    "        print(f\"Error code: {e.code}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {type(e).__name__}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with valid request\n",
    "result = safe_chat(llm, [{'role': 'user', 'content': 'Hi'}])\n",
    "if result:\n",
    "    print(\"Success!\")\n",
    "    print(result[-1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling API Rate Limits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def chat_with_retry(llm, messages, max_retries=3):\n",
    "    \"\"\"Chat with exponential backoff retry\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return llm.chat(messages=messages, stream=False)\n",
    "        except ModelServiceError as e:\n",
    "            if 'rate limit' in str(e).lower() and attempt < max_retries - 1:\n",
    "                wait_time = 2 ** attempt  # Exponential backoff\n",
    "                print(f\"Rate limited. Waiting {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                raise\n",
    "    return None\n",
    "\n",
    "# Usage\n",
    "# result = chat_with_retry(llm, messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Token Counting\n",
    "\n",
    "### Why Count Tokens?\n",
    "\n",
    "- **Cost management** - APIs charge per token\n",
    "- **Context limits** - Models have max token limits\n",
    "- **Performance** - Longer = slower\n",
    "\n",
    "### Using the Tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwen_agent.utils.tokenization_qwen import tokenizer\n",
    "\n",
    "# Example text\n",
    "text = \"Hello, world! This is a test of token counting in Qwen-Agent.\"\n",
    "\n",
    "# Encode to tokens\n",
    "tokens = tokenizer.encode(text)\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Token count: {len(tokens)}\")\n",
    "print(f\"Tokens: {tokens[:10]}...\")  # First 10 tokens\n",
    "\n",
    "# Decode back\n",
    "decoded = tokenizer.decode(tokens)\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count tokens in a conversation\n",
    "def count_message_tokens(messages):\n",
    "    \"\"\"Estimate token count for messages\"\"\"\n",
    "    total = 0\n",
    "    for msg in messages:\n",
    "        # Role\n",
    "        total += len(tokenizer.encode(msg['role']))\n",
    "        # Content\n",
    "        content = msg.get('content', '')\n",
    "        if isinstance(content, str):\n",
    "            total += len(tokenizer.encode(content))\n",
    "        # Overhead (separators, etc.)\n",
    "        total += 4  # Approximate\n",
    "    return total\n",
    "\n",
    "# Test\n",
    "messages = [\n",
    "    {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "    {'role': 'user', 'content': 'What is machine learning?'},\n",
    "    {'role': 'assistant', 'content': 'Machine learning is a subset of AI...'},\n",
    "]\n",
    "\n",
    "token_count = count_message_tokens(messages)\n",
    "print(f\"Total tokens in conversation: {token_count}\")\n",
    "print(f\"Estimated cost (if $0.001 per 1K tokens): ${token_count * 0.001 / 1000:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Model Comparison\n",
    "\n",
    "Let's compare different Qwen models on the same task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Models to compare\n",
    "models = [\n",
    "    ('qwen-turbo-latest', 'Fast and cheap'),\n",
    "    ('qwen-plus-latest', 'Balanced'),\n",
    "    ('qwen-max-latest', 'Most capable'),\n",
    "]\n",
    "\n",
    "# Test prompt\n",
    "prompt = [{'role': 'user', 'content': 'In exactly 20 words, explain what a transformer model is.'}]\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model_name, description in models:\n",
    "    print(f\"\\n{model_name} ({description})\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    llm = get_chat_model({'model': model_name})\n",
    "    \n",
    "    start = time.time()\n",
    "    response = llm.chat(messages=prompt, stream=False)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    content = response[-1]['content']\n",
    "    word_count = len(content.split())\n",
    "    \n",
    "    print(f\"Response: {content}\")\n",
    "    print(f\"Words: {word_count} | Time: {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Practice Exercises\n",
    "\n",
    "### Exercise 1: Configuration Experimentation\n",
    "Test different top_p values and observe the creativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create LLMs with top_p values of 0.1, 0.5, and 0.9\n",
    "# Test each with the same creative prompt\n",
    "# Compare the outputs\n",
    "\n",
    "# Your code here:\n",
    "prompt = \"Write a creative story opening in one sentence.\"\n",
    "\n",
    "# Test with different top_p:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Token Budget Manager\n",
    "Create a function that truncates messages to fit a token budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement truncate_to_budget(messages, max_tokens)\n",
    "# Should:\n",
    "# 1. Count tokens in messages\n",
    "# 2. Remove oldest messages if over budget\n",
    "# 3. Always keep system message (if present)\n",
    "# 4. Return truncated message list\n",
    "\n",
    "def truncate_to_budget(messages, max_tokens):\n",
    "    # Your implementation\n",
    "    pass\n",
    "\n",
    "# Test:\n",
    "# long_convo = [...]  # Create long conversation\n",
    "# truncated = truncate_to_budget(long_convo, 500)\n",
    "# print(f\"Truncated from {len(long_convo)} to {len(truncated)} messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Streaming Progress Bar\n",
    "Display a progress bar while streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a streaming chat with progress indicator\n",
    "# Show:\n",
    "# - Time elapsed\n",
    "# - Tokens received\n",
    "# - Tokens per second\n",
    "\n",
    "# Your code here:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 13: Key Takeaways\n",
    "\n",
    "### What You Learned Today:\n",
    "\n",
    "1. **LLM Abstraction**\n",
    "   - `BaseChatModel` provides uniform interface\n",
    "   - Switch backends by changing config\n",
    "   - Same code works with any provider\n",
    "\n",
    "2. **Getting LLM Instances**\n",
    "   - Use `get_chat_model(config)`\n",
    "   - Factory pattern\n",
    "   - Auto-detects backend\n",
    "\n",
    "3. **Direct LLM Calling**\n",
    "   - `.chat(messages, stream=True/False)`\n",
    "   - Returns list of messages\n",
    "   - No agent needed\n",
    "\n",
    "4. **Configuration Options**\n",
    "   - `top_p` for creativity\n",
    "   - `max_tokens` for length\n",
    "   - `max_input_tokens` for context\n",
    "   - Model-specific options\n",
    "\n",
    "5. **Backends**\n",
    "   - DashScope (official Qwen)\n",
    "   - vLLM (self-hosted)\n",
    "   - Ollama (local)\n",
    "   - OpenAI-compatible\n",
    "\n",
    "6. **Token Management**\n",
    "   - Count with tokenizer\n",
    "   - Monitor costs\n",
    "   - Handle limits\n",
    "\n",
    "### Common Patterns:\n",
    "\n",
    "```python\n",
    "# Pattern 1: Simple call\n",
    "llm = get_chat_model({'model': 'qwen-max-latest'})\n",
    "response = llm.chat(messages=[...], stream=False)\n",
    "\n",
    "# Pattern 2: Streaming\n",
    "for chunk in llm.chat(messages=[...], stream=True):\n",
    "    process(chunk)\n",
    "\n",
    "# Pattern 3: Get final from stream\n",
    "*_, final = llm.chat(messages=[...], stream=True)\n",
    "\n",
    "# Pattern 4: With generation config\n",
    "llm = get_chat_model({\n",
    "    'model': 'qwen-max-latest',\n",
    "    'generate_cfg': {'top_p': 0.8}\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 14: Next Steps\n",
    "\n",
    "### Tomorrow (Day 4): Built-in Tools\n",
    "We'll explore:\n",
    "- BaseTool interface\n",
    "- code_interpreter (Python execution)\n",
    "- doc_parser (PDF/DOCX parsing)\n",
    "- web_search (Internet search)\n",
    "- And more!\n",
    "\n",
    "### Homework:\n",
    "1. Test different models (turbo vs. max)\n",
    "2. Experiment with top_p values\n",
    "3. Set up vLLM or Ollama (optional)\n",
    "4. Count tokens in a long conversation\n",
    "5. Read: `/qwen_agent/llm/base.py`\n",
    "\n",
    "### Resources:\n",
    "- [DashScope Models](https://help.aliyun.com/zh/dashscope/developer-reference/model-introduction)\n",
    "- [vLLM Deployment](https://docs.vllm.ai/)\n",
    "- [Ollama](https://ollama.ai/)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udf89 Day 3 Complete!\n",
    "\n",
    "You now understand:\n",
    "- \u2705 LLM abstraction and `BaseChatModel`\n",
    "- \u2705 Direct LLM calling\n",
    "- \u2705 Configuration options\n",
    "- \u2705 Different backends\n",
    "- \u2705 Streaming responses\n",
    "- \u2705 Token management\n",
    "\n",
    "Tomorrow we'll start giving our LLMs superpowers with **Tools**! \ud83d\udee0\ufe0f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}