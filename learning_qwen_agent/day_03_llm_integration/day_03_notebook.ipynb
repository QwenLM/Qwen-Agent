{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 3: LLM Integration\n",
        "\n",
        "## Understanding How Qwen-Agent Talks to Language Models\n",
        "\n",
        "### Today's Learning Objectives:\n",
        "1. Understand the `BaseChatModel` interface\n",
        "2. Learn about different model backends (DashScope, OpenAI-compatible, local)\n",
        "3. Configure LLMs with various parameters\n",
        "4. Call LLMs directly (without agents)\n",
        "5. Master streaming responses\n",
        "6. Understand token counting and context windows\n",
        "\n",
        "### Prerequisites:\n",
        "- Completed Day 1 & 2\n",
        "- Understanding of Messages\n",
        "- API key configured\n",
        "\n",
        "### Time Required: 1.5-2 hours\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: The LLM Abstraction Layer\n",
        "\n",
        "### Why Abstract LLMs?\n",
        "\n",
        "Qwen-Agent supports many different LLM providers:\n",
        "- **DashScope** (Alibaba Cloud's Qwen models)\n",
        "- **OpenAI** (GPT-4, GPT-3.5)\n",
        "- **vLLM** (Self-hosted high-performance)\n",
        "- **Ollama** (Local CPU/GPU)\n",
        "- **Together.AI, Azure, and more**\n",
        "\n",
        "Without abstraction, you'd need different code for each provider. With `BaseChatModel`, you write once and switch providers by changing configuration!\n",
        "\n",
        "### Architecture:\n",
        "\n",
        "```\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502        Your Application             \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "               \u2502\n",
        "               v\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502       BaseChatModel Interface       \u2502  \u2190 Uniform API\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "               \u2502\n",
        "     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "     v         v         v         v\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502DashScope\u2502 \u2502OpenAI\u2502 \u2502 vLLM \u2502 \u2502 Ollama \u2502  \u2190 Different backends\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "```\n",
        "\n",
        "### Key Insight:\n",
        "The agent doesn't care which LLM it talks to - it just sends messages and receives responses through the `BaseChatModel` interface!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: BaseChatModel Interface\n",
        "\n",
        "### Source Code Location:\n",
        "`/qwen_agent/llm/base.py` (lines 61-100)\n",
        "\n",
        "### Key Properties:\n",
        "\n",
        "```python\n",
        "class BaseChatModel(ABC):\n",
        "    @property\n",
        "    def support_multimodal_input(self) -> bool:\n",
        "        # Can this model accept images/audio/video?\n",
        "        return False\n",
        "    \n",
        "    @property\n",
        "    def support_multimodal_output(self) -> bool:\n",
        "        # Can this model generate images/audio/video?\n",
        "        return False\n",
        "    \n",
        "    @property\n",
        "    def support_audio_input(self) -> bool:\n",
        "        return False\n",
        "```\n",
        "\n",
        "### Key Methods:\n",
        "- `chat(messages, stream=True/False)` - Main inference method\n",
        "- `_chat_stream()` - Streaming implementation (abstract)\n",
        "- `_chat_no_stream()` - Non-streaming implementation (abstract)\n",
        "\n",
        "Each LLM backend implements these methods!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Getting an LLM Instance\n",
        "\n",
        "### The `get_chat_model()` Factory Function:\n",
        "\n",
        "```python\n",
        "from qwen_agent.llm import get_chat_model\n",
        "\n",
        "llm = get_chat_model(config_dict)\n",
        "```\n",
        "\n",
        "This function:\n",
        "1. Reads your configuration\n",
        "2. Determines which backend to use\n",
        "3. Returns the appropriate `BaseChatModel` subclass\n",
        "4. All ready to use!\n",
        "\n",
        "Let's see it in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Configured for Fireworks API\n",
            "   Model: Qwen3-235B-A22B-Thinking-2507\n",
            "   Max tokens: 32,768"
          ]
        }
      ],
      "source": "# ================================================\n# FIREWORKS API CONFIGURATION\n# ================================================\nimport os\n\n# Set API credentials\nos.environ['FIREWORKS_API_KEY'] = 'fw_3ZSpUnVR78vs38jJtyewjcWk'\n\n# Standard configuration for Fireworks Qwen3-235B-A22B-Thinking\nllm_cfg_fireworks = {\n    'model': 'accounts/fireworks/models/qwen3-235b-a22b-thinking-2507',\n    'model_server': 'https://api.fireworks.ai/inference/v1',\n    'api_key': os.environ['FIREWORKS_API_KEY'],\n    'generate_cfg': {\n        'max_tokens': 32768,\n        'temperature': 0.6,\n    }\n}\n\n# Use this as default llm_cfg\nllm_cfg = llm_cfg_fireworks\n\nprint('\u2705 Configured for Fireworks API')\nprint(f'   Model: Qwen3-235B-A22B-Thinking-2507')\nprint(f'   Max tokens: 32,768')"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 LLM Type: <class 'qwen_agent.llm.oai.TextChatAtOAI'>\n",
            "\u2705 Model: accounts/fireworks/models/qwen3-235b-a22b-thinking-2507\n",
            "\u2705 Supports multimodal input: False\n",
            "\u2705 Supports multimodal output: False"
          ]
        }
      ],
      "source": "from qwen_agent.llm import get_chat_model\nimport os\n\n# Use Fireworks configuration (from cell 4)\nllm_config = llm_cfg\n\n# Get the LLM instance\nllm = get_chat_model(llm_config)\n\nprint(f\"\u2705 LLM Type: {type(llm)}\")\nprint(f\"\u2705 Model: {llm.model}\")\nprint(f\"\u2705 Supports multimodal input: {llm.support_multimodal_input}\")\nprint(f\"\u2705 Supports multimodal output: {llm.support_multimodal_output}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Direct LLM Calling (Without Agent)\n",
        "\n",
        "### The `.chat()` Method:\n",
        "\n",
        "You can call LLMs directly without creating an agent:\n",
        "\n",
        "```python\n",
        "response = llm.chat(\n",
        "    messages=[...],\n",
        "    stream=False,          # True for streaming\n",
        "    functions=[...],       # Optional: for function calling\n",
        "    extra_generate_cfg={}  # Optional: additional parameters\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling LLM...\n",
            "\n",
            "Response:\n",
            "Role: assistant\n",
            "Content: Okay, let's see. I need to add 15 and 27. Hmm, how do I do this? Maybe break it down into tens and ones. 15 is 10 + 5, and 27 is 20 + 7. So if I add the tens first: 10 + 20 is 30. Then the ones: 5 + 7 is 12. Now add those two results together: 30 + 12. That should be 42. Let me check another way. Maybe start with 15 and add 20 first, which is 35, then add 7 more. 35 + 7 is 42. Yeah, that works. Or use a number line: 15 + 27. Start at 15, add 20 to get to 35, then add 7 to reach 42. All methods give the same answer, so I think it's 42.\n",
            "</think>\n",
            "\n",
            "To solve $15 + 27$, we can break it down step by step:\n",
            "\n",
            "1. **Break into tens and ones**:\n",
            "   - $15 = 10 + 5$\n",
            "   - $27 = 20 + 7$\n",
            "\n",
            "2. **Add the tens**:\n",
            "   - $10 + 20 = 30$\n",
            "\n",
            "3. **Add the ones**:\n",
            "   - $5 + 7 = 12$\n",
            "\n",
            "4. **Combine the results**:\n",
            "   - $30 + 12 = 42$\n",
            "\n",
            "**Alternative method**:\n",
            "- Start with $15$, add $20$ to get $35$, then add $7$:  \n",
            "  $15 + 20 = 35$, $35 + 7 = 42$.\n",
            "\n",
            "**Final Answer**:  \n",
            "$$\n",
            "\\boxed{42}\n",
            "$$"
          ]
        }
      ],
      "source": [
        "# Example 1: Simple non-streaming call\n",
        "messages = [\n",
        "    {'role': 'user', 'content': 'What is 15 + 27?'}\n",
        "]\n",
        "\n",
        "print(\"Calling LLM...\\n\")\n",
        "response = llm.chat(messages=messages, stream=False)\n",
        "\n",
        "print(\"Response:\")\n",
        "for msg in response:\n",
        "    print(f\"Role: {msg['role']}\")\n",
        "    print(f\"Content: {msg['content']}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the Response:\n",
        "\n",
        "The `.chat()` method returns a **list of messages**:\n",
        "- Non-streaming: Complete list immediately\n",
        "- Streaming: Progressive updates (iterator)\n",
        "\n",
        "Usually contains:\n",
        "- One or more assistant messages\n",
        "- Possibly function call messages\n",
        "- Same Message format we learned on Day 2!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full conversation:\n",
            "user       | My favorite color is blue....\n",
            "assistant  | Okay, the user just stated \"My favorite color is blue.\" Hmm,...\n",
            "user       | What is my favorite color?...\n",
            "assistant  | Okay, the user just asked \"What is my favorite color?\" after..."
          ]
        }
      ],
      "source": [
        "# Example 2: Multi-turn conversation\n",
        "messages = [\n",
        "    {'role': 'user', 'content': 'My favorite color is blue.'},\n",
        "]\n",
        "\n",
        "# First turn\n",
        "response1 = llm.chat(messages=messages, stream=False)\n",
        "messages.extend(response1)\n",
        "\n",
        "# Second turn - LLM should remember\n",
        "messages.append({'role': 'user', 'content': 'What is my favorite color?'})\n",
        "response2 = llm.chat(messages=messages, stream=False)\n",
        "\n",
        "print(\"Full conversation:\")\n",
        "for msg in messages:\n",
        "    print(f\"{msg['role']:10} | {msg['content'][:60]}...\")\n",
        "for msg in response2:\n",
        "    print(f\"{msg['role']:10} | {msg['content'][:60]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Streaming vs. Non-Streaming Deep Dive\n",
        "\n",
        "### Non-Streaming (`stream=False`):\n",
        "\n",
        "```python\n",
        "response = llm.chat(messages, stream=False)\n",
        "# response is List[Message] - complete immediately\n",
        "```\n",
        "\n",
        "**Pros:**\n",
        "- Simple to use\n",
        "- Get complete response at once\n",
        "- Easier error handling\n",
        "\n",
        "**Cons:**\n",
        "- User waits for entire response\n",
        "- Feels slower\n",
        "- No early feedback\n",
        "\n",
        "### Streaming (`stream=True`):\n",
        "\n",
        "```python\n",
        "for chunk in llm.chat(messages, stream=True):\n",
        "    # chunk is List[Message] - progressively complete\n",
        "    # Process incrementally\n",
        "```\n",
        "\n",
        "**Pros:**\n",
        "- Lower perceived latency\n",
        "- Better UX (like ChatGPT)\n",
        "- Can start processing early\n",
        "\n",
        "**Cons:**\n",
        "- More complex to handle\n",
        "- Need to manage state\n",
        "- Harder error handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streaming Response:\n",
            "============================================================\n",
            "We are counting from 1 to 5, and for each number we provide a brief explanation.\n",
            " Let's do it step by step.\n",
            "</think>\n",
            "\n",
            "### Counting from 1 to 5 with Explanations:\n",
            "\n",
            "1. **One**  \n",
            "   *The starting point of counting.*  \n",
            "   Represents a single, indivisible unit. In mathematics, it is the multiplicative identity (any number multiplied by 1 remains unchanged). Philosophically, it symbolizes unity, singularity, or the origin of all numbers.  \n",
            "\n",
            "2. **Two**  \n",
            "   *The first even number and the only even prime.*  \n",
            "   Introduces the concept of duality (e.g., pairs, opposites like light/dark). In binary systems (the foundation of computing), it\u2019s the base of the numeral system (0s and 1s). Also signifies balance, choice, or division.  \n",
            "\n",
            "3. **Three**  \n",
            "   *The first odd prime and a symbol of harmony.*  \n",
            "   Often represents completeness (e.g., past/present/future, birth/life/death). In geometry, it defines the simplest polygon (triangle), which is structurally stable. Many cultures consider it a \"lucky\" or sacred number (e.g., the Christian Trinity).  \n",
            "\n",
            "4. **Four**  \n",
            "   *The first composite number and a symbol of stability.*  \n",
            "   Associated with order and foundation (e.g., four cardinal directions, four seasons, four legs of a table). In numerology, it signifies practicality and hard work. Mathematically, it\u2019s the smallest squared prime (2\u00b2) and the only number equal to its number of letters in English (\"f-o-u-r\").  \n",
            "\n",
            "5. **Five**  \n",
            "   *The center of the first five numbers and a symbol of humanity.*  \n",
            "   Represents the human senses (sight, hearing, touch, taste, smell) and the pentagon (a shape found in nature, like starfish). In many cultures, it\u2019s linked to grace or balance (e.g., the five fingers, the five-pointed star). Mathematically, it\u2019s the third prime number and the only prime that ends in 5.  \n",
            "\n",
            "This sequence illustrates how numbers evolve from unity (1) through complexity (5), each carrying unique mathematical, cultural, and philosophical significance. \ud83c\udf1f\n",
            "============================================================\n",
            "\n",
            "Time to first token: 0.75s\n",
            "Total time: 6.84s"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "messages = [{'role': 'user', 'content': 'Count from 1 to 5 with explanations for each number.'}]\n",
        "\n",
        "print(\"Streaming Response:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "full_content = \"\"\n",
        "start_time = time.time()\n",
        "first_token_time = None\n",
        "\n",
        "for chunk in llm.chat(messages=messages, stream=True):\n",
        "    if chunk and chunk[-1]['role'] == 'assistant':\n",
        "        content = chunk[-1].get('content', '')\n",
        "        \n",
        "        # Track time to first token\n",
        "        if not first_token_time and content:\n",
        "            first_token_time = time.time() - start_time\n",
        "        \n",
        "        # Print incrementally\n",
        "        if content != full_content:\n",
        "            new_content = content[len(full_content):]\n",
        "            print(new_content, end='', flush=True)\n",
        "            full_content = content\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"\\nTime to first token: {first_token_time:.2f}s\")\n",
        "print(f\"Total time: {total_time:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Streaming Pattern:\n",
        "\n",
        "```python\n",
        "# Common pattern to get final result from streaming\n",
        "*_, final_response = llm.chat(messages, stream=True)\n",
        "\n",
        "# Equivalent to:\n",
        "for chunk in llm.chat(messages, stream=True):\n",
        "    final_response = chunk  # Keep updating\n",
        "# final_response now has complete result\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: LLM Configuration Options\n",
        "\n",
        "### Basic Configuration:\n",
        "\n",
        "```python\n",
        "{\n",
        "    'model': str,           # Model name (required)\n",
        "    'model_type': str,      # Backend type (optional, auto-detected)\n",
        "    'api_key': str,         # API key (optional, uses env var)\n",
        "    'model_server': str,    # API base URL (for custom endpoints)\n",
        "    'generate_cfg': dict,   # Generation parameters\n",
        "}\n",
        "```\n",
        "\n",
        "### Generation Parameters (`generate_cfg`):\n",
        "\n",
        "```python\n",
        "'generate_cfg': {\n",
        "    'top_p': float,              # Nucleus sampling (0.0-1.0)\n",
        "    'temperature': float,        # Randomness (model dependent)\n",
        "    'max_tokens': int,           # Max output length\n",
        "    'max_input_tokens': int,     # Max input length (for truncation)\n",
        "    'fncall_prompt_type': str,   # 'qwen' or 'nous' for function calling\n",
        "    'enable_thinking': bool,     # For reasoning models (QwQ)\n",
        "    'use_raw_api': bool,         # Use native API tool calling\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creative LLM (top_p=0.9):\n",
            "Sure! How about **\"Nimbleton Quirk\"**?\n",
            "\n",
            "It sounds playful and intelligent\u2014like a curious little robot with a dash of personality. \"Nimbleton\" suggests agility and clever engineering, while \"Quirk\" hints\n",
            "\n",
            "Focused LLM (top_p=0.3):\n",
            "Sure! How about **\"Zippy\"**?\n",
            "\n",
            "It\u2019s playful, energetic, and hints at quick movements and smart responsiveness\u2014perfect for a nimble, affectionate pet robot. If you'd like something more futuristic or whimsical, here are"
          ]
        }
      ],
      "source": "# Example: Creative vs. Deterministic\n# Using Fireworks API with different top_p values\n\n# Configuration 1: Creative (high top_p)\ncreative_llm = get_chat_model({\n    'model': 'accounts/fireworks/models/qwen3-235b-a22b-instruct-2507',\n    'model_server': 'https://api.fireworks.ai/inference/v1',\n    'api_key': os.environ['FIREWORKS_API_KEY'],\n    'generate_cfg': {\n        'top_p': 0.9,  # More diverse outputs\n        'max_tokens': 50\n    }\n})\n\n# Configuration 2: Focused (low top_p)\nfocused_llm = get_chat_model({\n    'model': 'accounts/fireworks/models/qwen3-235b-a22b-instruct-2507',\n    'model_server': 'https://api.fireworks.ai/inference/v1',\n    'api_key': os.environ['FIREWORKS_API_KEY'],\n    'generate_cfg': {\n        'top_p': 0.3,  # More deterministic\n        'max_tokens': 50\n    }\n})\n\nprompt = [{'role': 'user', 'content': 'Give me a creative name for a pet robot.'}]\n\nprint(\"Creative LLM (top_p=0.9):\")\nresponse = creative_llm.chat(messages=prompt, stream=False)\nprint(response[-1]['content'])\n\nprint(\"\\nFocused LLM (top_p=0.3):\")\nresponse = focused_llm.chat(messages=prompt, stream=False)\nprint(response[-1]['content'])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Top-P (Nucleus Sampling) Explained:\n",
        "\n",
        "```\n",
        "Top-P = 0.1 (Deterministic)\n",
        "Only consider most likely tokens\n",
        "Output: Predictable, focused\n",
        "Use for: Math, code, factual Q&A\n",
        "\n",
        "Top-P = 0.5 (Balanced)\n",
        "Moderate diversity\n",
        "Output: Natural, reasonable variety\n",
        "Use for: General conversation\n",
        "\n",
        "Top-P = 0.95 (Creative)\n",
        "Consider many possible tokens\n",
        "Output: Diverse, creative\n",
        "Use for: Storytelling, brainstorming\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Different Model Backends\n",
        "\n",
        "### Option 1: DashScope (Recommended for Qwen Models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DashScope configuration example (requires DashScope API key):\n",
            "  Model: qwen-max-latest\n",
            "  Type: qwen_dashscope\n",
            "\n",
            "\u2705 We're using Fireworks API instead (configured in cell 4)"
          ]
        }
      ],
      "source": "# Fireworks configuration (already configured in cell 4)\n# This example shows how you would configure DashScope if you had access\n\ndashscope_config = {\n    'model': 'qwen-max-latest',\n    'model_type': 'qwen_dashscope',  # Explicitly specify\n    # 'api_key': 'sk-xxx',  # Or use DASHSCOPE_API_KEY env var\n}\n\nprint(\"DashScope configuration example (requires DashScope API key):\")\nprint(f\"  Model: {dashscope_config['model']}\")\nprint(f\"  Type: {dashscope_config['model_type']}\")\nprint(\"\\n\u2705 We're using Fireworks API instead (configured in cell 4)\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 2: OpenAI-Compatible APIs\n",
        "\n",
        "Works with:\n",
        "- **OpenAI** (official API)\n",
        "- **vLLM** (self-hosted)\n",
        "- **Ollama** (local)\n",
        "- **Together.AI, Groq, etc.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vLLM config ready (commented out - needs server running)"
          ]
        }
      ],
      "source": [
        "# Example: vLLM self-hosted\n",
        "# (Requires vLLM server running on localhost:8000)\n",
        "vllm_config = {\n",
        "    'model': 'Qwen2.5-7B-Instruct',\n",
        "    'model_server': 'http://localhost:8000/v1',  # OpenAI-compatible endpoint\n",
        "    'api_key': 'EMPTY',  # vLLM doesn't need real key\n",
        "}\n",
        "\n",
        "# Uncomment to test if you have vLLM running:\n",
        "# vllm_llm = get_chat_model(vllm_config)\n",
        "# print(f\"Using vLLM: {vllm_llm.model}\")\n",
        "\n",
        "print(\"vLLM config ready (commented out - needs server running)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ollama config ready (commented out - needs Ollama running)"
          ]
        }
      ],
      "source": [
        "# Example: Ollama (local)\n",
        "# (Requires Ollama installed and running)\n",
        "ollama_config = {\n",
        "    'model': 'qwen2.5:7b',\n",
        "    'model_server': 'http://localhost:11434/v1',\n",
        "    'api_key': 'EMPTY',\n",
        "}\n",
        "\n",
        "# Uncomment to test if you have Ollama:\n",
        "# ollama_llm = get_chat_model(ollama_config)\n",
        "# print(f\"Using Ollama: {ollama_llm.model}\")\n",
        "\n",
        "print(\"Ollama config ready (commented out - needs Ollama running)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing Backends:\n",
        "\n",
        "| Backend | Pros | Cons | Best For |\n",
        "|---------|------|------|----------|\n",
        "| **DashScope** | Official Qwen, no setup, latest models | Requires internet, costs money | Production, quick start |\n",
        "| **vLLM** | Fast inference, self-hosted, free | Requires GPU, setup complexity | High throughput production |\n",
        "| **Ollama** | Easy local setup, CPU support | Slower, limited models | Development, offline work |\n",
        "| **OpenAI** | GPT-4 access, reliable | Expensive, not Qwen models | Comparison, benchmarking |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: Advanced Configuration\n",
        "\n",
        "### Max Input Tokens (Context Window Management):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing with potentially long input...\n",
            "\u2705 Response received (input was truncated if needed)\n",
            "Response: Sure! Here's a comprehensive overview of **Python programming**:\n",
            "\n",
            "---\n",
            "\n",
            "### \ud83d\udc0d What is Python?\n",
            "\n",
            "**Python** is a high-level, interpreted, and general-purpose programming language known for its simplicity..."
          ]
        }
      ],
      "source": "# Configuration with input limit (using Fireworks)\nlimited_llm = get_chat_model({\n    'model': 'accounts/fireworks/models/qwen3-235b-a22b-instruct-2507',\n    'model_server': 'https://api.fireworks.ai/inference/v1',\n    'api_key': os.environ['FIREWORKS_API_KEY'],\n    'generate_cfg': {\n        'max_input_tokens': 2000,  # Truncate if input too long\n        'max_tokens': 100\n    }\n})\n\n# Long conversation that might exceed limit\nlong_messages = [\n    {'role': 'user', 'content': 'Tell me about Python programming. ' * 50},  # Repeated text\n]\n\nprint(\"Testing with potentially long input...\")\n# LLM will automatically truncate to fit 2000 tokens\nresponse = limited_llm.chat(messages=long_messages, stream=False)\nprint(\"\u2705 Response received (input was truncated if needed)\")\nprint(f\"Response: {response[-1]['content'][:200]}...\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Max Output Tokens (Response Length Limit):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Concise response (max 50 tokens):\n",
            "Quantum physics (also known as quantum mechanics or quantum theory) is a fundamental branch of physics that describes the behavior of matter and energy at the smallest scales\u2014atomic and subatomic levels. It fundamentally differs from classical physics (Newtonian mechanics and electrom\n",
            "\n",
            "\u2705 Response limited to ~50 tokens despite asking for 'detail'"
          ]
        }
      ],
      "source": "# Short responses (using Fireworks)\nconcise_llm = get_chat_model({\n    'model': 'accounts/fireworks/models/qwen3-235b-a22b-instruct-2507',\n    'model_server': 'https://api.fireworks.ai/inference/v1',\n    'api_key': os.environ['FIREWORKS_API_KEY'],\n    'generate_cfg': {\n        'max_tokens': 50,  # Short responses only\n    }\n})\n\nmessages = [{'role': 'user', 'content': 'Explain quantum physics in detail.'}]\nresponse = concise_llm.chat(messages=messages, stream=False)\n\nprint(\"Concise response (max 50 tokens):\")\nprint(response[-1]['content'])\nprint(f\"\\n\u2705 Response limited to ~50 tokens despite asking for 'detail'\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 9: Error Handling\n",
        "\n",
        "### Common Errors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n",
            "Okay, the user said \"Hi\". That's a simple greeting. I should respond in a friendly and welcoming way. Let me make sure to keep it open-ended so they feel comfortable sharing what they need. Maybe add an emoji to keep it warm. Let me check if there's anything else they might need right now. Since it's just a greeting, probably not. Just a polite reply. Alright, \"Hello! \ud83d\ude0a How can I assist you today?\" sounds good.\n",
            "</think>\n",
            "\n",
            "Hello! \ud83d\ude0a How can I assist you today?"
          ]
        }
      ],
      "source": [
        "from qwen_agent.llm.base import ModelServiceError\n",
        "\n",
        "def safe_chat(llm, messages):\n",
        "    \"\"\"Chat with error handling\"\"\"\n",
        "    try:\n",
        "        response = llm.chat(messages=messages, stream=False)\n",
        "        return response\n",
        "    except ModelServiceError as e:\n",
        "        print(f\"Model service error: {e.message}\")\n",
        "        print(f\"Error code: {e.code}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {type(e).__name__}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Test with valid request\n",
        "result = safe_chat(llm, [{'role': 'user', 'content': 'Hi'}])\n",
        "if result:\n",
        "    print(\"Success!\")\n",
        "    print(result[-1]['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Handling API Rate Limits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(No output)\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def chat_with_retry(llm, messages, max_retries=3):\n",
        "    \"\"\"Chat with exponential backoff retry\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            return llm.chat(messages=messages, stream=False)\n",
        "        except ModelServiceError as e:\n",
        "            if 'rate limit' in str(e).lower() and attempt < max_retries - 1:\n",
        "                wait_time = 2 ** attempt  # Exponential backoff\n",
        "                print(f\"Rate limited. Waiting {wait_time}s...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                raise\n",
        "    return None\n",
        "\n",
        "# Usage\n",
        "# result = chat_with_retry(llm, messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 10: Token Counting\n",
        "\n",
        "### Why Count Tokens?\n",
        "\n",
        "- **Cost management** - APIs charge per token\n",
        "- **Context limits** - Models have max token limits\n",
        "- **Performance** - Longer = slower\n",
        "\n",
        "### Using the Tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Hello, world! This is a test of token counting in Qwen-Agent.\n",
            "Token count: 16\n",
            "Tokens (first 10): [9707, 11, 1879, 0, 1096, 374, 264, 1273, 315, 3950]\n",
            "\n",
            "\u2705 Successfully encoded text to 16 tokens\n",
            "\u2705 Each token is an integer ID in the vocabulary"
          ]
        }
      ],
      "source": "from qwen_agent.utils.tokenization_qwen import tokenizer\n\n# Example text\ntext = \"Hello, world! This is a test of token counting in Qwen-Agent.\"\n\n# Encode to tokens\ntokens = tokenizer.encode(text)\nprint(f\"Text: {text}\")\nprint(f\"Token count: {len(tokens)}\")\nprint(f\"Tokens (first 10): {tokens[:10]}\")\n\n# Note: The tokenizer doesn't have a decode() method in this version\n# Tokens are integer IDs that represent pieces of text\nprint(f\"\\n\u2705 Successfully encoded text to {len(tokens)} tokens\")\nprint(f\"\u2705 Each token is an integer ID in the vocabulary\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens in conversation: 34\n",
            "Estimated cost (if $0.001 per 1K tokens): $0.000034"
          ]
        }
      ],
      "source": [
        "# Count tokens in a conversation\n",
        "def count_message_tokens(messages):\n",
        "    \"\"\"Estimate token count for messages\"\"\"\n",
        "    total = 0\n",
        "    for msg in messages:\n",
        "        # Role\n",
        "        total += len(tokenizer.encode(msg['role']))\n",
        "        # Content\n",
        "        content = msg.get('content', '')\n",
        "        if isinstance(content, str):\n",
        "            total += len(tokenizer.encode(content))\n",
        "        # Overhead (separators, etc.)\n",
        "        total += 4  # Approximate\n",
        "    return total\n",
        "\n",
        "# Test\n",
        "messages = [\n",
        "    {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
        "    {'role': 'user', 'content': 'What is machine learning?'},\n",
        "    {'role': 'assistant', 'content': 'Machine learning is a subset of AI...'},\n",
        "]\n",
        "\n",
        "token_count = count_message_tokens(messages)\n",
        "print(f\"Total tokens in conversation: {token_count}\")\n",
        "print(f\"Estimated cost (if $0.001 per 1K tokens): ${token_count * 0.001 / 1000:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 11: Model Comparison\n",
        "\n",
        "Let's compare different Qwen models on the same task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Configuration Comparison:\n",
            "======================================================================\n",
            "\n",
            "Instruct (Standard)\n",
            "----------------------------------------------------------------------\n",
            "Response (excerpt): A transformer model is a neural network using self-attention to process sequential data, enabling efficient, parallelized training for tasks like lang...\n",
            "Words: 25 | Time: 2.18s\n",
            "\n",
            "Instruct (Precise)\n",
            "----------------------------------------------------------------------\n",
            "Response (excerpt): A transformer model is a neural network using self-attention to process sequential data, enabling efficient parallelization and state-of-the-art perfo...\n",
            "Words: 23 | Time: 2.22s\n",
            "\n",
            "Thinking Model\n",
            "----------------------------------------------------------------------\n",
            "Response (excerpt): Hmm, the user wants me to explain what a transformer model is in exactly 20 words. That's quite precise! They're probably looking for a concise, techn...\n",
            "Words: 118 | Time: 2.75s\n",
            "\n",
            "======================================================================\n",
            "Note: Thinking model may show reasoning process, affecting length"
          ]
        }
      ],
      "source": "import time\n\n# Since we're using Fireworks, let's compare different configurations\n# of the same model instead of different models\n\n# Configurations to compare\nconfigs = [\n    ('Instruct (Standard)', {\n        'model': 'accounts/fireworks/models/qwen3-235b-a22b-instruct-2507',\n        'generate_cfg': {'max_tokens': 100, 'temperature': 0.7}\n    }),\n    ('Instruct (Precise)', {\n        'model': 'accounts/fireworks/models/qwen3-235b-a22b-instruct-2507',\n        'generate_cfg': {'max_tokens': 100, 'temperature': 0.3}\n    }),\n    ('Thinking Model', {\n        'model': 'accounts/fireworks/models/qwen3-235b-a22b-thinking-2507',\n        'generate_cfg': {'max_tokens': 150, 'temperature': 0.6}\n    }),\n]\n\n# Test prompt\nprompt = [{'role': 'user', 'content': 'In exactly 20 words, explain what a transformer model is.'}]\n\nprint(\"Model Configuration Comparison:\")\nprint(\"=\"*70)\n\nfor config_name, config in configs:\n    print(f\"\\n{config_name}\")\n    print(\"-\"*70)\n    \n    full_config = {\n        **config,\n        'model_server': 'https://api.fireworks.ai/inference/v1',\n        'api_key': os.environ['FIREWORKS_API_KEY'],\n    }\n    \n    llm = get_chat_model(full_config)\n    \n    start = time.time()\n    response = llm.chat(messages=prompt, stream=False)\n    elapsed = time.time() - start\n    \n    content = response[-1]['content']\n    word_count = len(content.split())\n    \n    # Show excerpt\n    if len(content) > 150:\n        print(f\"Response (excerpt): {content[:150]}...\")\n    else:\n        print(f\"Response: {content}\")\n    print(f\"Words: {word_count} | Time: {elapsed:.2f}s\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"Note: Thinking model may show reasoning process, affecting length\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 12: Practice Exercises\n",
        "\n",
        "### Exercise 1: Configuration Experimentation\n",
        "Test different top_p values and observe the creativity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create LLMs with top_p values of 0.1, 0.5, and 0.9\n",
        "# Test each with the same creative prompt\n",
        "# Compare the outputs\n",
        "\n",
        "# Your code here:\n",
        "prompt = \"Write a creative story opening in one sentence.\"\n",
        "\n",
        "# Test with different top_p:\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Token Budget Manager\n",
        "Create a function that truncates messages to fit a token budget."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement truncate_to_budget(messages, max_tokens)\n",
        "# Should:\n",
        "# 1. Count tokens in messages\n",
        "# 2. Remove oldest messages if over budget\n",
        "# 3. Always keep system message (if present)\n",
        "# 4. Return truncated message list\n",
        "\n",
        "def truncate_to_budget(messages, max_tokens):\n",
        "    # Your implementation\n",
        "    pass\n",
        "\n",
        "# Test:\n",
        "# long_convo = [...]  # Create long conversation\n",
        "# truncated = truncate_to_budget(long_convo, 500)\n",
        "# print(f\"Truncated from {len(long_convo)} to {len(truncated)} messages\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3: Streaming Progress Bar\n",
        "Display a progress bar while streaming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create a streaming chat with progress indicator\n",
        "# Show:\n",
        "# - Time elapsed\n",
        "# - Tokens received\n",
        "# - Tokens per second\n",
        "\n",
        "# Your code here:\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 13: Key Takeaways\n",
        "\n",
        "### What You Learned Today:\n",
        "\n",
        "1. **LLM Abstraction**\n",
        "   - `BaseChatModel` provides uniform interface\n",
        "   - Switch backends by changing config\n",
        "   - Same code works with any provider\n",
        "\n",
        "2. **Getting LLM Instances**\n",
        "   - Use `get_chat_model(config)`\n",
        "   - Factory pattern\n",
        "   - Auto-detects backend\n",
        "\n",
        "3. **Direct LLM Calling**\n",
        "   - `.chat(messages, stream=True/False)`\n",
        "   - Returns list of messages\n",
        "   - No agent needed\n",
        "\n",
        "4. **Configuration Options**\n",
        "   - `top_p` for creativity\n",
        "   - `max_tokens` for length\n",
        "   - `max_input_tokens` for context\n",
        "   - Model-specific options\n",
        "\n",
        "5. **Backends**\n",
        "   - DashScope (official Qwen)\n",
        "   - vLLM (self-hosted)\n",
        "   - Ollama (local)\n",
        "   - OpenAI-compatible\n",
        "\n",
        "6. **Token Management**\n",
        "   - Count with tokenizer\n",
        "   - Monitor costs\n",
        "   - Handle limits\n",
        "\n",
        "### Common Patterns:\n",
        "\n",
        "```python\n",
        "# Pattern 1: Simple call\n",
        "llm = get_chat_model({'model': 'qwen-max-latest'})\n",
        "response = llm.chat(messages=[...], stream=False)\n",
        "\n",
        "# Pattern 2: Streaming\n",
        "for chunk in llm.chat(messages=[...], stream=True):\n",
        "    process(chunk)\n",
        "\n",
        "# Pattern 3: Get final from stream\n",
        "*_, final = llm.chat(messages=[...], stream=True)\n",
        "\n",
        "# Pattern 4: With generation config\n",
        "llm = get_chat_model({\n",
        "    'model': 'qwen-max-latest',\n",
        "    'generate_cfg': {'top_p': 0.8}\n",
        "})\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 14: Next Steps\n",
        "\n",
        "### Tomorrow (Day 4): Built-in Tools\n",
        "We'll explore:\n",
        "- BaseTool interface\n",
        "- code_interpreter (Python execution)\n",
        "- doc_parser (PDF/DOCX parsing)\n",
        "- web_search (Internet search)\n",
        "- And more!\n",
        "\n",
        "### Homework:\n",
        "1. Test different models (turbo vs. max)\n",
        "2. Experiment with top_p values\n",
        "3. Set up vLLM or Ollama (optional)\n",
        "4. Count tokens in a long conversation\n",
        "5. Read: `/qwen_agent/llm/base.py`\n",
        "\n",
        "### Resources:\n",
        "- [DashScope Models](https://help.aliyun.com/zh/dashscope/developer-reference/model-introduction)\n",
        "- [vLLM Deployment](https://docs.vllm.ai/)\n",
        "- [Ollama](https://ollama.ai/)\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83c\udf89 Day 3 Complete!\n",
        "\n",
        "You now understand:\n",
        "- \u2705 LLM abstraction and `BaseChatModel`\n",
        "- \u2705 Direct LLM calling\n",
        "- \u2705 Configuration options\n",
        "- \u2705 Different backends\n",
        "- \u2705 Streaming responses\n",
        "- \u2705 Token management\n",
        "\n",
        "Tomorrow we'll start giving our LLMs superpowers with **Tools**! \ud83d\udee0\ufe0f"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}