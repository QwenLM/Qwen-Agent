{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 6: Function Calling - Teaching LLMs to Use Tools\n",
        "\n",
        "## What You'll Learn Today\n",
        "\n",
        "Today we dive deep into **function calling** - the mechanism that allows LLMs to use tools!\n",
        "\n",
        "**Why Function Calling Matters:**\n",
        "- Without it: LLM can only talk about using a weather API\n",
        "- With it: LLM can actually CALL the weather API and get real data\n",
        "\n",
        "### Today's Learning Path:\n",
        "1. **Understand function calling** - How LLMs request tool use\n",
        "2. **Function schemas** - Defining functions for the LLM\n",
        "3. **Direct LLM function calling** - Without agents\n",
        "4. **fncall_prompt_type** - 'qwen' vs 'nous' formats\n",
        "5. **function_choice parameter** - Control when functions are called\n",
        "6. **Parallel function calls** - Multiple tools at once\n",
        "7. **Error handling** - Dealing with malformed calls\n",
        "\n",
        "Let's unlock the full power of LLMs! \ud83d\udd27"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 1: Configure Our Environment\n",
        "\n",
        "Same Fireworks API setup."
      ]
    },
    {
      "cell_type": "markdown",
      "source": "---\n## \u26a0\ufe0f IMPORTANT: Known Compatibility Issue\n\n**Function calling with Fireworks API currently has compatibility issues with qwen-agent.**\n\nThe cells in this notebook that call `llm.chat()` with `functions` parameter will encounter a validation error:\n```\nValidationError: 1 validation error for FunctionCall\narguments - Input should be a valid string\n```\n\n**This affects cells:** 6, 8, 10, 12-14, 16, 18, 23, 25\n\n**What works:**\n- \u2705 Function definitions (cells 5, 20, 22)\n- \u2705 Error handling patterns\n- \u2705 Conceptual understanding\n- \u2705 Using tools via Assistant agent (Day 7, Day 8)\n\n**Recommended alternatives:**\n1. **Use Assistant agent** (Day 8) - It abstracts function calling and works with Fireworks\n2. **Use DashScope API** - Native Qwen platform (requires different API key)\n3. **Study the concepts** - The patterns shown are correct, just API incompatibility\n\n**This notebook is still valuable for:**\n- Understanding function calling concepts\n- Learning JSON Schema format\n- Error handling patterns\n- Seeing the complete function calling flow\n\nContinue to learn the concepts - you'll use them successfully in Days 7 & 8!",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Configured for Fireworks API\n",
            "   Model: Qwen3-235B-A22B-Thinking-2507"
          ]
        }
      ],
      "source": "# ================================================\n# FIREWORKS API CONFIGURATION  \n# ================================================\nimport os\nimport json\n\nos.environ['FIREWORKS_API_KEY'] = 'fw_3ZSpUnVR78vs38jJtyewjcWk'\n\nllm_cfg_fireworks = {\n    'model': 'accounts/fireworks/models/qwen3-235b-a22b-thinking-2507',\n    'model_server': 'https://api.fireworks.ai/inference/v1',\n    'api_key': os.environ['FIREWORKS_API_KEY'],\n    'generate_cfg': {\n        'max_tokens': 32768,\n        'temperature': 0.6,\n    }\n}\n\nllm_cfg = llm_cfg_fireworks\n\nprint('\u2705 Configured for Fireworks API')\nprint(f'   Model: Qwen3-235B-A22B-Thinking-2507')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 2: What is Function Calling?\n",
        "\n",
        "### The Function Calling Flow\n",
        "\n",
        "Function calling is a **structured way for LLMs to request tool execution**:\n",
        "\n",
        "```\n",
        "1. User: \"What's the weather in San Francisco?\"\n",
        "   \u2193\n",
        "2. You provide:\n",
        "   - Message history\n",
        "   - Available functions (with schemas)\n",
        "   \u2193\n",
        "3. LLM responds with:\n",
        "   {\n",
        "     \"function_call\": {\n",
        "       \"name\": \"get_weather\",\n",
        "       \"arguments\": \"{\\\"location\\\": \\\"San Francisco\\\"}\"\n",
        "     }\n",
        "   }\n",
        "   \u2193\n",
        "4. You execute the function\n",
        "   \u2193\n",
        "5. You add the result to messages\n",
        "   \u2193\n",
        "6. Call LLM again to get final answer\n",
        "```\n",
        "\n",
        "**Key insight:** The LLM doesn't execute functions - it just generates **structured requests** for YOU to execute!\n",
        "\n",
        "### Function Schema Format\n",
        "\n",
        "Functions are defined using **JSON Schema**:\n",
        "\n",
        "```python\n",
        "{\n",
        "    'name': 'function_name',           # Unique identifier\n",
        "    'description': 'What it does',     # How LLM knows when to use it\n",
        "    'parameters': {                    # JSON Schema for parameters\n",
        "        'type': 'object',\n",
        "        'properties': {\n",
        "            'param1': {\n",
        "                'type': 'string',\n",
        "                'description': 'What this parameter is for'\n",
        "            }\n",
        "        },\n",
        "        'required': ['param1']         # Which params are mandatory\n",
        "    }\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 3: Direct LLM Function Calling\n",
        "\n",
        "### Example: Weather Function\n",
        "\n",
        "Let's implement the classic weather example from the official Qwen-Agent examples!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "FUNCTION CALLING DEMONSTRATION\n",
            "======================================================================\n",
            "\n",
            "User: What's the weather like in San Francisco?\n",
            "\n",
            "Step 1: We provide the function schema to the LLM\n",
            "Function name: get_current_weather\n",
            "Description: Get the current weather in a given location\n",
            "\n",
            "Step 2: LLM WOULD respond with (example from official docs):\n",
            "{\n",
            "  \"role\": \"assistant\",\n",
            "  \"content\": \"\",\n",
            "  \"function_call\": {\n",
            "    \"name\": \"get_current_weather\",\n",
            "    \"arguments\": \"{\\\"location\\\": \\\"San Francisco, CA\\\", \\\"unit\\\": \\\"fahrenheit\\\"}\"\n",
            "  }\n",
            "}\n",
            "\n",
            "\ud83d\udca1 KEY INSIGHT: The LLM generates a structured request, not actual results!\n",
            "   - role: 'assistant' (the LLM is responding)\n",
            "   - function_call.name: Which function to call\n",
            "   - function_call.arguments: JSON string of parameters\n",
            "\n",
            "Step 3: We execute the function:\n",
            "Function result: {\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"fahrenheit\"}\n",
            "\n",
            "Step 4: Add result to conversation history\n",
            "Step 5: Call LLM again to get natural language answer\n",
            "\n",
            "Final answer: 'The weather in San Francisco is 72\u00b0F'\n",
            "\n",
            "======================================================================"
          ]
        }
      ],
      "source": [
        "# WORKING DEMONSTRATION: Function Calling Flow\n",
        "# Shows EXACTLY what happens (without Fireworks API issues)\n",
        "\n",
        "from qwen_agent.llm import get_chat_model\n",
        "import json\n",
        "\n",
        "# Step 1: Define the function schema (this part works!)\n",
        "functions = [{\n",
        "    'name': 'get_current_weather',\n",
        "    'description': 'Get the current weather in a given location',\n",
        "    'parameters': {\n",
        "        'type': 'object',\n",
        "        'properties': {\n",
        "            'location': {\n",
        "                'type': 'string',\n",
        "                'description': 'The city and state, e.g. San Francisco, CA',\n",
        "            },\n",
        "            'unit': {\n",
        "                'type': 'string',\n",
        "                'enum': ['celsius', 'fahrenheit']\n",
        "            },\n",
        "        },\n",
        "        'required': ['location'],\n",
        "    },\n",
        "}]\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"FUNCTION CALLING DEMONSTRATION\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nUser: What's the weather like in San Francisco?\\n\")\n",
        "print(\"Step 1: We provide the function schema to the LLM\")\n",
        "print(\"Function name:\", functions[0]['name'])\n",
        "print(\"Description:\", functions[0]['description'])\n",
        "\n",
        "print(\"\\nStep 2: LLM WOULD respond with (example from official docs):\")\n",
        "simulated_llm_response = [{\n",
        "    'role': 'assistant',\n",
        "    'content': '',\n",
        "    'function_call': {\n",
        "        'name': 'get_current_weather',\n",
        "        'arguments': '{\\\"location\\\": \\\"San Francisco, CA\\\", \\\"unit\\\": \\\"fahrenheit\\\"}'\n",
        "    }\n",
        "}]\n",
        "\n",
        "print(json.dumps(simulated_llm_response[0], indent=2))\n",
        "\n",
        "print(\"\\n\ud83d\udca1 KEY INSIGHT: The LLM generates a structured request, not actual results!\")\n",
        "print(\"   - role: 'assistant' (the LLM is responding)\")\n",
        "print(\"   - function_call.name: Which function to call\")\n",
        "print(\"   - function_call.arguments: JSON string of parameters\")\n",
        "\n",
        "# Define the actual function\n",
        "def get_current_weather(location, unit='fahrenheit'):\n",
        "    if 'san francisco' in location.lower():\n",
        "        return json.dumps({'location': 'San Francisco', 'temperature': '72', 'unit': 'fahrenheit'})\n",
        "    elif 'tokyo' in location.lower():\n",
        "        return json.dumps({'location': 'Tokyo', 'temperature': '10', 'unit': 'celsius'})\n",
        "    elif 'paris' in location.lower():\n",
        "        return json.dumps({'location': 'Paris', 'temperature': '22', 'unit': 'celsius'})\n",
        "    else:\n",
        "        return json.dumps({'location': location, 'temperature': 'unknown'})\n",
        "\n",
        "print(\"\\nStep 3: We execute the function:\")\n",
        "function_args = json.loads(simulated_llm_response[0]['function_call']['arguments'])\n",
        "result = get_current_weather(**function_args)\n",
        "print(f\"Function result: {result}\")\n",
        "\n",
        "print(\"\\nStep 4: Add result to conversation history\")\n",
        "print(\"Step 5: Call LLM again to get natural language answer\")\n",
        "print(\"\\nFinal answer: 'The weather in San Francisco is 72\u00b0F'\")\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ValidationError: 1 validation error for FunctionCall\narguments\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Create LLM client\n",
        "llm = get_chat_model(llm_cfg)\n",
        "\n",
        "# Step 4: Send user query with function definitions\n",
        "messages = [{'role': 'user', 'content': \"What's the weather like in San Francisco?\"}]\n",
        "\n",
        "print(\"User: What's the weather like in San Francisco?\\n\")\n",
        "print(\"Calling LLM with function definitions...\\n\")\n",
        "\n",
        "# Get LLM response\n",
        "responses = []\n",
        "for responses in llm.chat(\n",
        "    messages=messages,\n",
        "    functions=functions,\n",
        "    stream=True\n",
        "):\n",
        "    pass  # Just get the final response\n",
        "\n",
        "print(\"LLM Response:\")\n",
        "print(json.dumps(responses, indent=2, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# COMPLETE WORKING EXAMPLE: Full Function Calling Loop\n",
        "print(\"=\"*70)\n",
        "print(\"COMPLETE FUNCTION CALLING LOOP\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Simulated LLM response (what a working API would return)\n",
        "messages = [{'role': 'user', 'content': \"What's the weather like in San Francisco?\"}]\n",
        "\n",
        "llm_response = {\n",
        "    'role': 'assistant',\n",
        "    'content': '',\n",
        "    'function_call': {\n",
        "        'name': 'get_current_weather',\n",
        "        'arguments': '{\\\"location\\\": \\\"San Francisco, CA\\\", \\\"unit\\\": \\\"fahrenheit\\\"}'\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\n\ud83d\udccb Step 1: User asks question\")\n",
        "print(f\"User: {messages[0]['content']}\")\n",
        "\n",
        "print(\"\\n\ud83d\udccb Step 2: LLM decides to call function\")\n",
        "print(f\"Function: {llm_response['function_call']['name']}\")\n",
        "print(f\"Arguments: {llm_response['function_call']['arguments']}\")\n",
        "\n",
        "# Add LLM's function call to history\n",
        "messages.append(llm_response)\n",
        "\n",
        "print(\"\\n\ud83d\udccb Step 3: Execute the function\")\n",
        "function_name = llm_response['function_call']['name']\n",
        "function_args = json.loads(llm_response['function_call']['arguments'])\n",
        "\n",
        "# Execute\n",
        "function_response = get_current_weather(\n",
        "    location=function_args.get('location'),\n",
        "    unit=function_args.get('unit', 'fahrenheit')\n",
        ")\n",
        "print(f\"Result: {function_response}\")\n",
        "\n",
        "print(\"\\n\ud83d\udccb Step 4: Add function result to messages\")\n",
        "messages.append({\n",
        "    'role': 'function',\n",
        "    'name': function_name,\n",
        "    'content': function_response\n",
        "})\n",
        "\n",
        "print(\"\\n\ud83d\udccb Step 5: LLM would generate natural language response\")\n",
        "final_answer = \"The current weather in San Francisco is 72\u00b0F.\"\n",
        "print(f\"Final answer: {final_answer}\")\n",
        "\n",
        "print(\"\\n\ud83d\udcca Complete message history:\")\n",
        "for i, msg in enumerate(messages, 1):\n",
        "    role = msg.get('role', 'unknown')\n",
        "    print(f\"  {i}. {role}: \", end='')\n",
        "    if msg.get('function_call'):\n",
        "        print(f\"[FUNCTION CALL: {msg['function_call']['name']}]\")\n",
        "    elif msg.get('content'):\n",
        "        print(msg['content'][:50] + ('...' if len(msg['content']) > 50 else ''))\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\u2705 This is the EXACT pattern used in production!\")\n",
        "print(\"\u2705 Works with DashScope API, vLLM, and other compatible backends\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(No output)\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Execute the function\n",
        "messages.extend(responses)  # Add LLM's function call to history\n",
        "\n",
        "last_response = messages[-1]\n",
        "if last_response.get('function_call'):\n",
        "    print(\"\\n\ud83d\udd27 LLM requested function call!\\n\")\n",
        "    \n",
        "    # Parse the function call\n",
        "    function_name = last_response['function_call']['name']\n",
        "    function_args = json.loads(last_response['function_call']['arguments'])\n",
        "    \n",
        "    print(f\"Function: {function_name}\")\n",
        "    print(f\"Arguments: {function_args}\\n\")\n",
        "    \n",
        "    # Execute the function\n",
        "    available_functions = {\n",
        "        'get_current_weather': get_current_weather,\n",
        "    }\n",
        "    function_to_call = available_functions[function_name]\n",
        "    function_response = function_to_call(\n",
        "        location=function_args.get('location'),\n",
        "        unit=function_args.get('unit', 'fahrenheit'),\n",
        "    )\n",
        "    \n",
        "    print(f\"Function Response: {function_response}\\n\")\n",
        "    \n",
        "    # Step 6: Add function result to messages\n",
        "    messages.append({\n",
        "        'role': 'function',\n",
        "        'name': function_name,\n",
        "        'content': function_response,\n",
        "    })\n",
        "    \n",
        "    # Step 7: Call LLM again to get final answer\n",
        "    print(\"Calling LLM again with function result...\\n\")\n",
        "    final_responses = []\n",
        "    for final_responses in llm.chat(\n",
        "        messages=messages,\n",
        "        functions=functions,\n",
        "        stream=True\n",
        "    ):\n",
        "        pass\n",
        "    \n",
        "    print(\"Final Answer:\")\n",
        "    print(final_responses[-1].get('content', ''))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Understanding fncall_prompt_type\n",
        "print(\"=\"*70)\n",
        "print(\"FUNCTION CALL PROMPT TYPES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n\ud83d\udd27 What is fncall_prompt_type?\")\n",
        "print(\"   Different models expect different function calling formats:\")\n",
        "\n",
        "print(\"\\n1\ufe0f\u20e3 'qwen' format (Qwen's native):\")\n",
        "print(\"   - Used by: Qwen models via DashScope\")\n",
        "print(\"   - System prompt format: Special Qwen function calling template\")\n",
        "print(\"   - Response format: Qwen's structured output\")\n",
        "\n",
        "print(\"\\n2\ufe0f\u20e3 'nous' format (NousResearch):\")\n",
        "print(\"   - Used by: Most OpenAI-compatible APIs\")\n",
        "print(\"   - System prompt format: OpenAI-style function definitions\")\n",
        "print(\"   - Response format: OpenAI function calling structure\")\n",
        "\n",
        "print(\"\\n\ud83d\udcdd How to configure:\")\n",
        "print(\"   llm_cfg = {\")\n",
        "print(\"       'model': 'qwen-max',\")\n",
        "print(\"       'generate_cfg': {\")\n",
        "print(\"           'fncall_prompt_type': 'qwen'  # or 'nous'\")\n",
        "print(\"       }\")\n",
        "print(\"   }\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 For Fireworks API:\")\n",
        "print(\"   - Try 'nous' format (OpenAI-compatible)\")\n",
        "print(\"   - Check Fireworks documentation for latest compatibility\")\n",
        "print(\"   - Consider using DashScope for native Qwen function calling\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CONTROLLING FUNCTION CALLS: function_choice\n",
            "======================================================================\n",
            "\n",
            "\ud83c\udf9b\ufe0f  The function_choice parameter controls when functions are called:\n",
            "\n",
            "1\ufe0f\u20e3 function_choice='auto' (default)\n",
            "   - LLM decides whether to call a function\n",
            "   - Example: 'What is 2+2?' \u2192 Direct answer (no function)\n",
            "   - Example: 'What is the weather?' \u2192 Calls get_weather function\n",
            "\n",
            "2\ufe0f\u20e3 function_choice='none'\n",
            "   - LLM NEVER calls functions\n",
            "   - Forces direct text answer\n",
            "   - Example: 'What is the weather?' \u2192 'I don't have real-time data...'\n",
            "\n",
            "3\ufe0f\u20e3 function_choice='function_name'\n",
            "   - FORCES LLM to call specific function\n",
            "   - LLM must generate parameters for that function\n",
            "   - Example: 'Tell me about Paris' + choice='get_weather' \u2192\n",
            "              LLM calls get_weather(location='Paris')\n",
            "\n",
            "\ud83d\udcdd Usage example:\n",
            "   responses = llm.chat(\n",
            "       messages=messages,\n",
            "       functions=functions,\n",
            "       extra_generate_cfg={'function_choice': 'auto'}  # or 'none' or 'get_weather'\n",
            "   )\n",
            "\n",
            "\ud83d\udca1 Real-world use cases:\n",
            "   - 'auto': Normal chatbot operation\n",
            "   - 'none': When you want guaranteed text response\n",
            "   - 'function_name': When user action requires specific tool\n",
            "\n",
            "======================================================================"
          ]
        }
      ],
      "source": [
        "# Understanding function_choice Parameter\n",
        "print(\"=\"*70)\n",
        "print(\"CONTROLLING FUNCTION CALLS: function_choice\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n\ud83c\udf9b\ufe0f  The function_choice parameter controls when functions are called:\")\n",
        "\n",
        "print(\"\\n1\ufe0f\u20e3 function_choice='auto' (default)\")\n",
        "print(\"   - LLM decides whether to call a function\")\n",
        "print(\"   - Example: 'What is 2+2?' \u2192 Direct answer (no function)\")\n",
        "print(\"   - Example: 'What is the weather?' \u2192 Calls get_weather function\")\n",
        "\n",
        "print(\"\\n2\ufe0f\u20e3 function_choice='none'\")\n",
        "print(\"   - LLM NEVER calls functions\")\n",
        "print(\"   - Forces direct text answer\")\n",
        "print(\"   - Example: 'What is the weather?' \u2192 'I don't have real-time data...'\")\n",
        "\n",
        "print(\"\\n3\ufe0f\u20e3 function_choice='function_name'\")\n",
        "print(\"   - FORCES LLM to call specific function\")\n",
        "print(\"   - LLM must generate parameters for that function\")\n",
        "print(\"   - Example: 'Tell me about Paris' + choice='get_weather' \u2192\")\n",
        "print(\"              LLM calls get_weather(location='Paris')\")\n",
        "\n",
        "print(\"\\n\ud83d\udcdd Usage example:\")\n",
        "print(\"   responses = llm.chat(\")\n",
        "print(\"       messages=messages,\")\n",
        "print(\"       functions=functions,\")\n",
        "print(\"       extra_generate_cfg={'function_choice': 'auto'}  # or 'none' or 'get_weather'\")\n",
        "print(\"   )\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Real-world use cases:\")\n",
        "print(\"   - 'auto': Normal chatbot operation\")\n",
        "print(\"   - 'none': When you want guaranteed text response\")\n",
        "print(\"   - 'function_name': When user action requires specific tool\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 5: Controlling Function Calls with function_choice\n",
        "\n",
        "### The function_choice Parameter\n",
        "\n",
        "You can control when the LLM uses functions:\n",
        "\n",
        "| Value | Behavior | Use Case |\n",
        "|-------|----------|----------|\n",
        "| **'auto'** (default) | LLM decides | Normal operation |\n",
        "| **'none'** | Never call functions | Force direct answer |\n",
        "| **function_name** | Force this function | Required tool use |\n",
        "\n",
        "Let's see examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "EXAMPLE 1: function_choice='auto' (default)\n",
            "======================================================================\n",
            "\n",
            "User: What's the weather in Tokyo?\n",
            "\n",
            "\ud83d\udca1 With function_choice='auto', LLM decides:\n",
            "   - Query mentions 'weather' \u2192 LLM chooses to call function\n",
            "\n",
            "LLM Decision: Call function \u2713\n",
            "Function: get_current_weather\n",
            "Arguments: {\"location\": \"Tokyo\"}\n",
            "\n",
            "======================================================================"
          ]
        }
      ],
      "source": [
        "# Example 1: Auto (default) - LLM decides\n",
        "print(\"=\"*70)\n",
        "print(\"EXAMPLE 1: function_choice='auto' (default)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nUser: What's the weather in Tokyo?\")\n",
        "print(\"\\n\ud83d\udca1 With function_choice='auto', LLM decides:\")\n",
        "print(\"   - Query mentions 'weather' \u2192 LLM chooses to call function\")\n",
        "\n",
        "# Simulated LLM response\n",
        "llm_decision = {\n",
        "    'role': 'assistant',\n",
        "    'content': '',\n",
        "    'function_call': {\n",
        "        'name': 'get_current_weather',\n",
        "        'arguments': '{\\\"location\\\": \\\"Tokyo\\\"}'\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\nLLM Decision: Call function \u2713\")\n",
        "print(f\"Function: {llm_decision['function_call']['name']}\")\n",
        "print(f\"Arguments: {llm_decision['function_call']['arguments']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "EXAMPLE 2: function_choice='get_current_weather' (forced)\n",
            "======================================================================\n",
            "\n",
            "User: Tell me about Tokyo\n",
            "\n",
            "\ud83d\udca1 With function_choice='get_current_weather', LLM is FORCED:\n",
            "   - Even though query doesn't mention weather\n",
            "   - LLM must call get_current_weather with appropriate arguments\n",
            "\n",
            "LLM Response: Function was forced to be called!\n",
            "Function: get_current_weather\n",
            "Arguments: {\"location\": \"Tokyo\"}\n",
            "\n",
            "\ud83d\udca1 Use case: When you KNOW user needs specific tool\n",
            "   Example: 'Check order status' always requires 'get_order' function\n",
            "\n",
            "======================================================================"
          ]
        }
      ],
      "source": [
        "# Example 2: Force function call\n",
        "print(\"=\"*70)\n",
        "print(\"EXAMPLE 2: function_choice='get_current_weather' (forced)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nUser: Tell me about Tokyo\")\n",
        "print(\"\\n\ud83d\udca1 With function_choice='get_current_weather', LLM is FORCED:\")\n",
        "print(\"   - Even though query doesn't mention weather\")\n",
        "print(\"   - LLM must call get_current_weather with appropriate arguments\")\n",
        "\n",
        "# Simulated forced function call\n",
        "forced_call = {\n",
        "    'role': 'assistant',\n",
        "    'content': '',\n",
        "    'function_call': {\n",
        "        'name': 'get_current_weather',\n",
        "        'arguments': '{\\\"location\\\": \\\"Tokyo\\\"}'\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\nLLM Response: Function was forced to be called!\")\n",
        "print(f\"Function: {forced_call['function_call']['name']}\")\n",
        "print(f\"Arguments: {forced_call['function_call']['arguments']}\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Use case: When you KNOW user needs specific tool\")\n",
        "print(\"   Example: 'Check order status' always requires 'get_order' function\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "EXAMPLE 3: function_choice='none' (disabled)\n",
            "======================================================================\n",
            "\n",
            "User: What's the weather in Paris?\n",
            "\n",
            "\ud83d\udca1 With function_choice='none', LLM CANNOT call functions:\n",
            "   - Even though get_current_weather is available\n",
            "   - LLM must provide direct text answer\n",
            "\n",
            "LLM Response: Direct text answer (no function_call)\n",
            "Has function_call: False\n",
            "\n",
            "Direct answer:\n",
            "  I don't have access to real-time weather data. Please check a weather service like weather.com for current conditions in Paris.\n",
            "\n",
            "\ud83d\udca1 Use case: When you want guaranteed text response\n",
            "   Example: Final user-facing message after all tools executed\n",
            "\n",
            "======================================================================"
          ]
        }
      ],
      "source": [
        "# Example 3: Disable function calls\n",
        "print(\"=\"*70)\n",
        "print(\"EXAMPLE 3: function_choice='none' (disabled)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nUser: What's the weather in Paris?\")\n",
        "print(\"\\n\ud83d\udca1 With function_choice='none', LLM CANNOT call functions:\")\n",
        "print(\"   - Even though get_current_weather is available\")\n",
        "print(\"   - LLM must provide direct text answer\")\n",
        "\n",
        "# Simulated direct answer (no function call)\n",
        "direct_answer = {\n",
        "    'role': 'assistant',\n",
        "    'content': \"I don't have access to real-time weather data. Please check a weather service like weather.com for current conditions in Paris.\"\n",
        "}\n",
        "\n",
        "print(\"\\nLLM Response: Direct text answer (no function_call)\")\n",
        "print(f\"Has function_call: {direct_answer.get('function_call') is not None}\")\n",
        "print(f\"\\nDirect answer:\")\n",
        "print(f\"  {direct_answer['content']}\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Use case: When you want guaranteed text response\")\n",
        "print(\"   Example: Final user-facing message after all tools executed\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PARALLEL FUNCTION CALLS - Complete Working Example\n",
        "print(\"=\"*70)\n",
        "print(\"PARALLEL FUNCTION CALLING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nUser: What's the weather in San Francisco, Tokyo, and Paris?\")\n",
        "\n",
        "print(\"\\n\ud83d\udccb With parallel_function_calls=True, LLM can generate multiple calls:\")\n",
        "\n",
        "# Simulated LLM response with parallel calls\n",
        "parallel_response = [\n",
        "    {\n",
        "        'role': 'assistant',\n",
        "        'content': '',\n",
        "        'function_call': {\n",
        "            'name': 'get_current_weather',\n",
        "            'arguments': '{\\\"location\\\": \\\"San Francisco, CA\\\"}'\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'role': 'assistant',\n",
        "        'content': '',\n",
        "        'function_call': {\n",
        "            'name': 'get_current_weather',\n",
        "            'arguments': '{\\\"location\\\": \\\"Tokyo\\\"}'\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'role': 'assistant',\n",
        "        'content': '',\n",
        "        'function_call': {\n",
        "            'name': 'get_current_weather',\n",
        "            'arguments': '{\\\"location\\\": \\\"Paris\\\"}'\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"\\n\u2705 LLM generated {len(parallel_response)} function calls in parallel!\")\n",
        "\n",
        "for i, call in enumerate(parallel_response, 1):\n",
        "    args = json.loads(call['function_call']['arguments'])\n",
        "    print(f\"\\nCall {i}:\")\n",
        "    print(f\"  Function: {call['function_call']['name']}\")\n",
        "    print(f\"  Location: {args['location']}\")\n",
        "\n",
        "print(\"\\n\ud83d\udccb Execute all functions:\")\n",
        "results = []\n",
        "for call in parallel_response:\n",
        "    args = json.loads(call['function_call']['arguments'])\n",
        "    result = get_current_weather(**args)\n",
        "    results.append(result)\n",
        "    print(f\"  {args['location']}: {result}\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 KEY BENEFIT: All 3 weather checks happen in ONE LLM call!\")\n",
        "print(\"   - Without parallel: 3 separate LLM calls (slow)\")\n",
        "print(\"   - With parallel: 1 LLM call with 3 function requests (fast)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ValidationError: 1 validation error for FunctionCall\narguments\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\n"
          ]
        }
      ],
      "source": [
        "# Enable parallel function calls\n",
        "messages = [{\n",
        "    'role': 'user',\n",
        "    'content': \"What's the weather like in San Francisco, Tokyo, and Paris?\"\n",
        "}]\n",
        "\n",
        "print(\"User: What's the weather in San Francisco, Tokyo, and Paris?\\n\")\n",
        "print(\"Calling LLM with parallel_function_calls=True...\\n\")\n",
        "\n",
        "responses = []\n",
        "for responses in llm.chat(\n",
        "    messages=messages,\n",
        "    functions=functions,\n",
        "    stream=True,\n",
        "    extra_generate_cfg={'parallel_function_calls': True}\n",
        "):\n",
        "    pass\n",
        "\n",
        "# Check if we got multiple function calls\n",
        "fncall_msgs = [rsp for rsp in responses if rsp.get('function_call')]\n",
        "print(f\"Number of function calls: {len(fncall_msgs)}\\n\")\n",
        "\n",
        "for i, msg in enumerate(fncall_msgs, 1):\n",
        "    print(f\"Call {i}:\")\n",
        "    print(f\"  Function: {msg['function_call']['name']}\")\n",
        "    print(f\"  Arguments: {msg['function_call']['arguments']}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# COMPLETE PARALLEL EXECUTION FLOW\n",
        "messages = [{'role': 'user', 'content': 'What is the weather in San Francisco, Tokyo, and Paris?'}]\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"COMPLETE PARALLEL FUNCTION CALLING FLOW\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Step 1: Simulate parallel function call response\n",
        "fncall_msgs = [\n",
        "    {'role': 'assistant', 'content': '', 'function_call': {\n",
        "        'name': 'get_current_weather',\n",
        "        'arguments': '{\\\"location\\\": \\\"San Francisco\\\"}'\n",
        "    }},\n",
        "    {'role': 'assistant', 'content': '', 'function_call': {\n",
        "        'name': 'get_current_weather',\n",
        "        'arguments': '{\\\"location\\\": \\\"Tokyo\\\"}'\n",
        "    }},\n",
        "    {'role': 'assistant', 'content': '', 'function_call': {\n",
        "        'name': 'get_current_weather',\n",
        "        'arguments': '{\\\"location\\\": \\\"Paris\\\"}'\n",
        "    }}\n",
        "]\n",
        "\n",
        "messages.extend(fncall_msgs)\n",
        "\n",
        "print(f\"\\n\ud83d\udccb Step 1: LLM generated {len(fncall_msgs)} parallel function calls\")\n",
        "\n",
        "# Step 2: Execute all functions\n",
        "print(\"\\n\ud83d\udccb Step 2: Execute all functions in parallel\")\n",
        "for msg in fncall_msgs:\n",
        "    function_name = msg['function_call']['name']\n",
        "    function_args = json.loads(msg['function_call']['arguments'])\n",
        "\n",
        "    result = get_current_weather(**function_args)\n",
        "\n",
        "    print(f\"  {function_args['location']}: {result}\")\n",
        "\n",
        "    # Add each result to messages (IMPORTANT: same order as calls!)\n",
        "    messages.append({\n",
        "        'role': 'function',\n",
        "        'name': function_name,\n",
        "        'content': result\n",
        "    })\n",
        "\n",
        "print(\"\\n\ud83d\udccb Step 3: LLM would synthesize all results into natural answer\")\n",
        "final_answer = \"\"\"Here's the weather in all three cities:\n",
        "- San Francisco: 72\u00b0F\n",
        "- Tokyo: 10\u00b0C\n",
        "- Paris: 22\u00b0C\"\"\"\n",
        "\n",
        "print(f\"\\nFinal Answer:\")\n",
        "print(final_answer)\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Total messages in conversation: {len(messages)}\")\n",
        "print(\"   1 user + 3 function_calls + 3 function_results + 1 final_answer = 8 messages\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "NameError: name 'fncall_msgs' is not defined\n"
          ]
        }
      ],
      "source": [
        "messages.extend(responses)  # Add all function calls to history\n",
        "\n",
        "if fncall_msgs:\n",
        "    print(\"Executing all function calls...\\n\")\n",
        "    \n",
        "    available_functions = {\n",
        "        'get_current_weather': get_current_weather,\n",
        "    }\n",
        "    \n",
        "    # Execute each function call\n",
        "    for msg in fncall_msgs:\n",
        "        function_name = msg['function_call']['name']\n",
        "        function_args = json.loads(msg['function_call']['arguments'])\n",
        "        \n",
        "        function_to_call = available_functions[function_name]\n",
        "        function_response = function_to_call(\n",
        "            location=function_args.get('location'),\n",
        "            unit=function_args.get('unit', 'fahrenheit'),\n",
        "        )\n",
        "        \n",
        "        print(f\"Result for {function_args['location']}: {function_response}\")\n",
        "        \n",
        "        # Add function result to messages (in same order as calls!)\n",
        "        messages.append({\n",
        "            'role': 'function',\n",
        "            'name': function_name,\n",
        "            'content': function_response,\n",
        "        })\n",
        "    \n",
        "    print(\"\\nCalling LLM again with all results...\\n\")\n",
        "    \n",
        "    # Get final synthesized answer\n",
        "    final_responses = []\n",
        "    for final_responses in llm.chat(\n",
        "        messages=messages,\n",
        "        functions=functions,\n",
        "        stream=True,\n",
        "        extra_generate_cfg={'parallel_function_calls': True}\n",
        "    ):\n",
        "        pass\n",
        "    \n",
        "    print(\"Final Answer:\")\n",
        "    print(final_responses[-1].get('content', ''))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 7: Error Handling\n",
        "\n",
        "### Common Errors in Function Calling\n",
        "\n",
        "1. **Malformed JSON** - LLM generates invalid JSON arguments\n",
        "2. **Unknown function** - LLM tries to call a function you didn't define\n",
        "3. **Missing required parameters** - LLM omits required arguments\n",
        "4. **Type errors** - LLM provides wrong type (string instead of number)\n",
        "\n",
        "Let's handle these gracefully:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing error handling...\n",
            "\n",
            "Error handling result:\n",
            "{\"error\": \"Function 'unknown_function' not found\", \"available_functions\": [\"get_current_weather\"]}"
          ]
        }
      ],
      "source": [
        "def safe_execute_function(function_call_msg, available_functions):\n",
        "    \"\"\"\n",
        "    Safely execute a function call with error handling\n",
        "    \"\"\"\n",
        "    try:\n",
        "        function_name = function_call_msg['function_call']['name']\n",
        "        \n",
        "        # Check if function exists\n",
        "        if function_name not in available_functions:\n",
        "            return json.dumps({\n",
        "                'error': f\"Function '{function_name}' not found\",\n",
        "                'available_functions': list(available_functions.keys())\n",
        "            })\n",
        "        \n",
        "        # Parse arguments\n",
        "        try:\n",
        "            function_args = json.loads(function_call_msg['function_call']['arguments'])\n",
        "        except json.JSONDecodeError as e:\n",
        "            return json.dumps({\n",
        "                'error': f\"Invalid JSON in arguments: {str(e)}\",\n",
        "                'arguments': function_call_msg['function_call']['arguments']\n",
        "            })\n",
        "        \n",
        "        # Execute function\n",
        "        function_to_call = available_functions[function_name]\n",
        "        result = function_to_call(**function_args)\n",
        "        return result\n",
        "        \n",
        "    except TypeError as e:\n",
        "        return json.dumps({\n",
        "            'error': f\"TypeError: {str(e)}\",\n",
        "            'hint': 'Check if all required parameters are provided with correct types'\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return json.dumps({\n",
        "            'error': f\"Unexpected error: {str(e)}\",\n",
        "            'type': type(e).__name__\n",
        "        })\n",
        "\n",
        "# Test error handling\n",
        "print(\"Testing error handling...\\n\")\n",
        "\n",
        "# Simulate a malformed function call\n",
        "test_msg = {\n",
        "    'function_call': {\n",
        "        'name': 'unknown_function',\n",
        "        'arguments': '{\"invalid\": json}'\n",
        "    }\n",
        "}\n",
        "\n",
        "result = safe_execute_function(test_msg, {'get_current_weather': get_current_weather})\n",
        "print(f\"Error handling result:\\n{result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 8: Multiple Functions Example\n",
        "\n",
        "### Giving the LLM Multiple Tools\n",
        "\n",
        "Let's add more functions for the LLM to choose from!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Defined 3 functions for the LLM!"
          ]
        }
      ],
      "source": [
        "# Define multiple functions\n",
        "def get_current_time(timezone='UTC'):\n",
        "    \"\"\"Get the current time in a timezone\"\"\"\n",
        "    from datetime import datetime\n",
        "    return json.dumps({'timezone': timezone, 'time': datetime.now().isoformat()})\n",
        "\n",
        "def calculate(expression):\n",
        "    \"\"\"Calculate a mathematical expression\"\"\"\n",
        "    try:\n",
        "        result = eval(expression)  # In production, use a safe math parser!\n",
        "        return json.dumps({'expression': expression, 'result': result})\n",
        "    except Exception as e:\n",
        "        return json.dumps({'error': str(e)})\n",
        "\n",
        "# Define function schemas\n",
        "multi_functions = [\n",
        "    {\n",
        "        'name': 'get_current_weather',\n",
        "        'description': 'Get the current weather in a given location',\n",
        "        'parameters': {\n",
        "            'type': 'object',\n",
        "            'properties': {\n",
        "                'location': {'type': 'string', 'description': 'The city name'},\n",
        "                'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}\n",
        "            },\n",
        "            'required': ['location']\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'get_current_time',\n",
        "        'description': 'Get the current time in a specific timezone',\n",
        "        'parameters': {\n",
        "            'type': 'object',\n",
        "            'properties': {\n",
        "                'timezone': {'type': 'string', 'description': 'Timezone like UTC, EST, PST'}\n",
        "            },\n",
        "            'required': []\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'calculate',\n",
        "        'description': 'Calculate a mathematical expression',\n",
        "        'parameters': {\n",
        "            'type': 'object',\n",
        "            'properties': {\n",
        "                'expression': {'type': 'string', 'description': 'Math expression like \"2+2\" or \"sqrt(16)\"'}\n",
        "            },\n",
        "            'required': ['expression']\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "available_functions = {\n",
        "    'get_current_weather': get_current_weather,\n",
        "    'get_current_time': get_current_time,\n",
        "    'calculate': calculate\n",
        "}\n",
        "\n",
        "print(f\"\u2705 Defined {len(multi_functions)} functions for the LLM!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ValidationError: 1 validation error for FunctionCall\narguments\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\n"
          ]
        }
      ],
      "source": [
        "# Test with a query that needs multiple functions\n",
        "messages = [{\n",
        "    'role': 'user',\n",
        "    'content': 'What time is it and what is 15 * 23?'\n",
        "}]\n",
        "\n",
        "print(\"User: What time is it and what is 15 * 23?\\n\")\n",
        "\n",
        "responses = []\n",
        "for responses in llm.chat(\n",
        "    messages=messages,\n",
        "    functions=multi_functions,\n",
        "    stream=True,\n",
        "    extra_generate_cfg={'parallel_function_calls': True}\n",
        "):\n",
        "    pass\n",
        "\n",
        "# Execute any function calls\n",
        "fncall_msgs = [rsp for rsp in responses if rsp.get('function_call')]\n",
        "if fncall_msgs:\n",
        "    print(f\"LLM requested {len(fncall_msgs)} function call(s):\\n\")\n",
        "    \n",
        "    messages.extend(responses)\n",
        "    \n",
        "    for msg in fncall_msgs:\n",
        "        result = safe_execute_function(msg, available_functions)\n",
        "        print(f\"Function: {msg['function_call']['name']}\")\n",
        "        print(f\"Result: {result}\\n\")\n",
        "        \n",
        "        messages.append({\n",
        "            'role': 'function',\n",
        "            'name': msg['function_call']['name'],\n",
        "            'content': result\n",
        "        })\n",
        "    \n",
        "    # Get final answer\n",
        "    for final_responses in llm.chat(messages=messages, functions=multi_functions, stream=True):\n",
        "        pass\n",
        "    \n",
        "    print(\"Final Answer:\")\n",
        "    print(final_responses[-1].get('content', ''))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MULTI-FUNCTION EXAMPLE: Tool Selection\n",
        "print(\"=\"*70)\n",
        "print(\"MULTI-FUNCTION TOOL SELECTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Define functions\n",
        "def get_current_time(timezone='UTC'):\n",
        "    from datetime import datetime\n",
        "    return json.dumps({'timezone': timezone, 'time': datetime.now().isoformat()})\n",
        "\n",
        "def calculate(expression):\n",
        "    try:\n",
        "        result = eval(expression)  # In production, use safe math parser!\n",
        "        return json.dumps({'expression': expression, 'result': result})\n",
        "    except Exception as e:\n",
        "        return json.dumps({'error': str(e)})\n",
        "\n",
        "available_functions = {\n",
        "    'get_current_weather': get_current_weather,\n",
        "    'get_current_time': get_current_time,\n",
        "    'calculate': calculate\n",
        "}\n",
        "\n",
        "print(\"\\nAvailable functions:\")\n",
        "for name in available_functions.keys():\n",
        "    print(f\"  - {name}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEST 1: Time query\")\n",
        "print(\"=\"*60)\n",
        "print(\"User: What time is it?\")\n",
        "print(\"\\nLLM decides to call: get_current_time\")\n",
        "result = get_current_time('UTC')\n",
        "print(f\"Result: {result}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEST 2: Math query\")\n",
        "print(\"=\"*60)\n",
        "print(\"User: What is 15 * 23?\")\n",
        "print(\"\\nLLM decides to call: calculate\")\n",
        "result = calculate(\"15 * 23\")\n",
        "print(f\"Result: {result}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEST 3: Weather query\")\n",
        "print(\"=\"*60)\n",
        "print(\"User: Weather in Tokyo?\")\n",
        "print(\"\\nLLM decides to call: get_current_weather\")\n",
        "result = get_current_weather(\"Tokyo\")\n",
        "print(f\"Result: {result}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEST 4: Parallel query\")\n",
        "print(\"=\"*60)\n",
        "print(\"User: What time is it and what is 15 * 23?\")\n",
        "print(\"\\nLLM decides to call BOTH:\")\n",
        "print(\"  1. get_current_time()\")\n",
        "print(\"  2. calculate('15 * 23')\")\n",
        "result1 = get_current_time()\n",
        "result2 = calculate(\"15 * 23\")\n",
        "print(f\"Results:\")\n",
        "print(f\"  Time: {result1}\")\n",
        "print(f\"  Calculation: {result2}\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 KEY INSIGHT: LLM chooses function based on:\")\n",
        "print(\"   - Function descriptions\")\n",
        "print(\"   - User query keywords\")\n",
        "print(\"   - Context from conversation\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ValidationError: 1 validation error for FunctionCall\narguments\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\n"
          ]
        }
      ],
      "source": [
        "def function_calling_chat(user_query, functions, available_functions, max_turns=5):\n",
        "    \"\"\"\n",
        "    Complete function calling loop\n",
        "    \"\"\"\n",
        "    messages = [{'role': 'user', 'content': user_query}]\n",
        "    llm = get_chat_model(llm_cfg)\n",
        "    \n",
        "    for turn in range(max_turns):\n",
        "        print(f\"\\n--- Turn {turn + 1} ---\")\n",
        "        \n",
        "        # Call LLM\n",
        "        responses = []\n",
        "        for responses in llm.chat(\n",
        "            messages=messages,\n",
        "            functions=functions,\n",
        "            stream=True,\n",
        "            extra_generate_cfg={'parallel_function_calls': True}\n",
        "        ):\n",
        "            pass\n",
        "        \n",
        "        messages.extend(responses)\n",
        "        \n",
        "        # Check for function calls\n",
        "        fncall_msgs = [rsp for rsp in responses if rsp.get('function_call')]\n",
        "        \n",
        "        if not fncall_msgs:\n",
        "            # No function calls - we have the final answer\n",
        "            print(\"\u2705 Got final answer\")\n",
        "            return responses[-1].get('content', '')\n",
        "        \n",
        "        # Execute function calls\n",
        "        print(f\"Executing {len(fncall_msgs)} function(s)...\")\n",
        "        for msg in fncall_msgs:\n",
        "            fn_name = msg['function_call']['name']\n",
        "            print(f\"  - {fn_name}\")\n",
        "            \n",
        "            result = safe_execute_function(msg, available_functions)\n",
        "            messages.append({\n",
        "                'role': 'function',\n",
        "                'name': fn_name,\n",
        "                'content': result\n",
        "            })\n",
        "    \n",
        "    return \"Max turns reached\"\n",
        "\n",
        "# Test it!\n",
        "answer = function_calling_chat(\n",
        "    \"What's the weather in Paris and what is 100 divided by 4?\",\n",
        "    multi_functions,\n",
        "    available_functions\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Final Answer:\")\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 10: Practice Exercises\n",
        "\n",
        "Now it's your turn!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: Add a Translation Function\n",
        "\n",
        "Create a `translate_text` function that:\n",
        "- Takes `text` and `target_language` parameters\n",
        "- Returns a dummy translation\n",
        "- Define its schema\n",
        "- Test it with the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement translate_text function and schema\n",
        "# Hint: Follow the pattern of get_current_weather\n",
        "\n",
        "# Your code here:\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Handle Parallel Weather Queries\n",
        "\n",
        "Modify the weather function to handle 5+ cities in parallel.\n",
        "Test with: \"Compare weather in New York, London, Tokyo, Sydney, and Mumbai\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Test parallel weather queries\n",
        "# Your code here:\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3: Build a Unit Converter\n",
        "\n",
        "Create a function calling system with:\n",
        "- `convert_temperature(value, from_unit, to_unit)`\n",
        "- `convert_length(value, from_unit, to_unit)`\n",
        "- `convert_weight(value, from_unit, to_unit)`\n",
        "\n",
        "Test with: \"Convert 100 fahrenheit to celsius, 10 kilometers to miles, and 5 pounds to kilograms\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement unit converter system\n",
        "# Your code here:\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4: Error Recovery\n",
        "\n",
        "Create a function that sometimes fails, and implement error recovery:\n",
        "- If function returns error, add error to messages\n",
        "- Ask LLM to try again with different parameters\n",
        "- Limit retry attempts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement error recovery\n",
        "# Your code here:\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary: What You Learned Today\n",
        "\n",
        "### Core Concepts\n",
        "\n",
        "\u2705 **Function calling** - LLMs generate structured function requests\n",
        "\n",
        "\u2705 **Function schemas** - JSON Schema format for defining functions\n",
        "\n",
        "\u2705 **Direct LLM calling** - Function calling without agents (using llm.chat())\n",
        "\n",
        "\u2705 **fncall_prompt_type** - 'qwen' vs 'nous' formats\n",
        "\n",
        "\u2705 **function_choice** - Control when functions are called ('auto', 'none', or function name)\n",
        "\n",
        "\u2705 **Parallel function calls** - Multiple tools in one response\n",
        "\n",
        "\u2705 **Error handling** - Gracefully handling malformed calls and execution errors\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **LLMs don't execute functions** - they generate requests for you to execute\n",
        "2. **Function descriptions matter** - they teach the LLM when to use each tool\n",
        "3. **Always validate** - Parse JSON carefully and handle errors\n",
        "4. **Parallel calling is powerful** - Enable it for better UX\n",
        "5. **The loop is important** - LLM \u2192 Function \u2192 LLM (repeat as needed)\n",
        "\n",
        "### What's Next?\n",
        "\n",
        "**Tomorrow (Day 7)**: We'll learn **Custom Tools**!\n",
        "\n",
        "You'll learn:\n",
        "- Using @register_tool decorator\n",
        "- Parameter schema deep dive\n",
        "- Building reusable tools\n",
        "- Advanced tool patterns\n",
        "- Tool testing\n",
        "\n",
        "### Additional Resources\n",
        "\n",
        "- \ud83d\udcd6 Function Calling Example: `/examples/function_calling.py`\n",
        "- \ud83d\udd27 Parallel Calls Example: `/examples/function_calling_in_parallel.py`\n",
        "- \ud83d\udca1 OpenAI Function Calling Guide: https://platform.openai.com/docs/guides/function-calling\n",
        "\n",
        "---\n",
        "\n",
        "**Congratulations! \ud83c\udf89 You now understand how LLMs use tools through function calling!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}