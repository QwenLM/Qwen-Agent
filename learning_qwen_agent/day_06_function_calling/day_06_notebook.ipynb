{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6: Function Calling - Teaching LLMs to Use Tools\n",
    "\n",
    "## What You'll Learn Today\n",
    "\n",
    "Today we dive deep into **function calling** - the mechanism that allows LLMs to use tools!\n",
    "\n",
    "**Why Function Calling Matters:**\n",
    "- Without it: LLM can only talk about using a weather API\n",
    "- With it: LLM can actually CALL the weather API and get real data\n",
    "\n",
    "### Today's Learning Path:\n",
    "1. **Understand function calling** - How LLMs request tool use\n",
    "2. **Function schemas** - Defining functions for the LLM\n",
    "3. **Direct LLM function calling** - Without agents\n",
    "4. **fncall_prompt_type** - 'qwen' vs 'nous' formats\n",
    "5. **function_choice parameter** - Control when functions are called\n",
    "6. **Parallel function calls** - Multiple tools at once\n",
    "7. **Error handling** - Dealing with malformed calls\n",
    "\n",
    "Let's unlock the full power of LLMs! üîß"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Configure Our Environment\n",
    "\n",
    "Same Fireworks API setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================================================\n# FIREWORKS API CONFIGURATION  \n# ================================================\nimport os\nimport json\n\nos.environ['FIREWORKS_API_KEY'] = 'fw_3ZSpUnVR78vs38jJtyewjcWk'\n\nllm_cfg_fireworks = {\n    'model': 'accounts/fireworks/models/qwen3-235b-a22b-thinking-2507',\n    'model_server': 'https://api.fireworks.ai/inference/v1',\n    'api_key': os.environ['FIREWORKS_API_KEY'],\n    'generate_cfg': {\n        'max_tokens': 32768,\n        'temperature': 0.6,\n    }\n}\n\nllm_cfg = llm_cfg_fireworks\n\nprint('‚úÖ Configured for Fireworks API')\nprint(f'   Model: Qwen3-235B-A22B-Thinking-2507')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: What is Function Calling?\n",
    "\n",
    "### The Function Calling Flow\n",
    "\n",
    "Function calling is a **structured way for LLMs to request tool execution**:\n",
    "\n",
    "```\n",
    "1. User: \"What's the weather in San Francisco?\"\n",
    "   ‚Üì\n",
    "2. You provide:\n",
    "   - Message history\n",
    "   - Available functions (with schemas)\n",
    "   ‚Üì\n",
    "3. LLM responds with:\n",
    "   {\n",
    "     \"function_call\": {\n",
    "       \"name\": \"get_weather\",\n",
    "       \"arguments\": \"{\\\"location\\\": \\\"San Francisco\\\"}\"\n",
    "     }\n",
    "   }\n",
    "   ‚Üì\n",
    "4. You execute the function\n",
    "   ‚Üì\n",
    "5. You add the result to messages\n",
    "   ‚Üì\n",
    "6. Call LLM again to get final answer\n",
    "```\n",
    "\n",
    "**Key insight:** The LLM doesn't execute functions - it just generates **structured requests** for YOU to execute!\n",
    "\n",
    "### Function Schema Format\n",
    "\n",
    "Functions are defined using **JSON Schema**:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'name': 'function_name',           # Unique identifier\n",
    "    'description': 'What it does',     # How LLM knows when to use it\n",
    "    'parameters': {                    # JSON Schema for parameters\n",
    "        'type': 'object',\n",
    "        'properties': {\n",
    "            'param1': {\n",
    "                'type': 'string',\n",
    "                'description': 'What this parameter is for'\n",
    "            }\n",
    "        },\n",
    "        'required': ['param1']         # Which params are mandatory\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Direct LLM Function Calling\n",
    "\n",
    "### Example: Weather Function\n",
    "\n",
    "Let's implement the classic weather example from the official Qwen-Agent examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwen_agent.llm import get_chat_model\n",
    "import json\n",
    "\n",
    "# Step 1: Define a dummy function (in production, this would call a real API)\n",
    "def get_current_weather(location, unit='fahrenheit'):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    if 'tokyo' in location.lower():\n",
    "        return json.dumps({'location': 'Tokyo', 'temperature': '10', 'unit': 'celsius'})\n",
    "    elif 'san francisco' in location.lower():\n",
    "        return json.dumps({'location': 'San Francisco', 'temperature': '72', 'unit': 'fahrenheit'})\n",
    "    elif 'paris' in location.lower():\n",
    "        return json.dumps({'location': 'Paris', 'temperature': '22', 'unit': 'celsius'})\n",
    "    else:\n",
    "        return json.dumps({'location': location, 'temperature': 'unknown'})\n",
    "\n",
    "# Step 2: Define the function schema\n",
    "functions = [{\n",
    "    'name': 'get_current_weather',\n",
    "    'description': 'Get the current weather in a given location',\n",
    "    'parameters': {\n",
    "        'type': 'object',\n",
    "        'properties': {\n",
    "            'location': {\n",
    "                'type': 'string',\n",
    "                'description': 'The city and state, e.g. San Francisco, CA',\n",
    "            },\n",
    "            'unit': {\n",
    "                'type': 'string',\n",
    "                'enum': ['celsius', 'fahrenheit']\n",
    "            },\n",
    "        },\n",
    "        'required': ['location'],\n",
    "    },\n",
    "}]\n",
    "\n",
    "print(\"‚úÖ Function defined!\")\n",
    "print(f\"Function: {functions[0]['name']}\")\n",
    "print(f\"Description: {functions[0]['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create LLM client\n",
    "llm = get_chat_model(llm_cfg)\n",
    "\n",
    "# Step 4: Send user query with function definitions\n",
    "messages = [{'role': 'user', 'content': \"What's the weather like in San Francisco?\"}]\n",
    "\n",
    "print(\"User: What's the weather like in San Francisco?\\n\")\n",
    "print(\"Calling LLM with function definitions...\\n\")\n",
    "\n",
    "# Get LLM response\n",
    "responses = []\n",
    "for responses in llm.chat(\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    stream=True\n",
    "):\n",
    "    pass  # Just get the final response\n",
    "\n",
    "print(\"LLM Response:\")\n",
    "print(json.dumps(responses, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Response\n",
    "\n",
    "Notice the LLM response contains:\n",
    "- `role`: 'assistant'\n",
    "- `function_call`: A dict with `name` and `arguments`\n",
    "- `arguments`: A JSON string (not a dict!)\n",
    "\n",
    "Now let's execute the function and complete the interaction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Execute the function\n",
    "messages.extend(responses)  # Add LLM's function call to history\n",
    "\n",
    "last_response = messages[-1]\n",
    "if last_response.get('function_call'):\n",
    "    print(\"\\nüîß LLM requested function call!\\n\")\n",
    "    \n",
    "    # Parse the function call\n",
    "    function_name = last_response['function_call']['name']\n",
    "    function_args = json.loads(last_response['function_call']['arguments'])\n",
    "    \n",
    "    print(f\"Function: {function_name}\")\n",
    "    print(f\"Arguments: {function_args}\\n\")\n",
    "    \n",
    "    # Execute the function\n",
    "    available_functions = {\n",
    "        'get_current_weather': get_current_weather,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name]\n",
    "    function_response = function_to_call(\n",
    "        location=function_args.get('location'),\n",
    "        unit=function_args.get('unit', 'fahrenheit'),\n",
    "    )\n",
    "    \n",
    "    print(f\"Function Response: {function_response}\\n\")\n",
    "    \n",
    "    # Step 6: Add function result to messages\n",
    "    messages.append({\n",
    "        'role': 'function',\n",
    "        'name': function_name,\n",
    "        'content': function_response,\n",
    "    })\n",
    "    \n",
    "    # Step 7: Call LLM again to get final answer\n",
    "    print(\"Calling LLM again with function result...\\n\")\n",
    "    final_responses = []\n",
    "    for final_responses in llm.chat(\n",
    "        messages=messages,\n",
    "        functions=functions,\n",
    "        stream=True\n",
    "    ):\n",
    "        pass\n",
    "    \n",
    "    print(\"Final Answer:\")\n",
    "    print(final_responses[-1].get('content', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Function Call Prompt Types\n",
    "\n",
    "### What is fncall_prompt_type?\n",
    "\n",
    "Different models expect different function calling formats:\n",
    "\n",
    "| Type | Description | When to Use |\n",
    "|------|-------------|-------------|\n",
    "| **'qwen'** | Qwen's native format | Qwen models via DashScope |\n",
    "| **'nous'** | NousResearch format | Most OpenAI-compatible APIs |\n",
    "\n",
    "**For Fireworks API:** We should use **'nous'** format!\n",
    "\n",
    "Let's test both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with 'nous' format (recommended for Fireworks)\n",
    "llm_with_nous = get_chat_model({\n",
    "    **llm_cfg,\n",
    "    'generate_cfg': {\n",
    "        **llm_cfg.get('generate_cfg', {}),\n",
    "        'fncall_prompt_type': 'nous'\n",
    "    }\n",
    "})\n",
    "\n",
    "messages = [{'role': 'user', 'content': \"What's the weather in Paris?\"}]\n",
    "\n",
    "print(\"Testing with fncall_prompt_type='nous':\\n\")\n",
    "for responses in llm_with_nous.chat(messages=messages, functions=functions, stream=True):\n",
    "    pass\n",
    "\n",
    "if responses and responses[-1].get('function_call'):\n",
    "    print(\"‚úÖ Function call detected!\")\n",
    "    print(f\"   Function: {responses[-1]['function_call']['name']}\")\n",
    "    print(f\"   Args: {responses[-1]['function_call']['arguments']}\")\n",
    "else:\n",
    "    print(\"‚ùå No function call (might need different format)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Controlling Function Calls with function_choice\n",
    "\n",
    "### The function_choice Parameter\n",
    "\n",
    "You can control when the LLM uses functions:\n",
    "\n",
    "| Value | Behavior | Use Case |\n",
    "|-------|----------|----------|\n",
    "| **'auto'** (default) | LLM decides | Normal operation |\n",
    "| **'none'** | Never call functions | Force direct answer |\n",
    "| **function_name** | Force this function | Required tool use |\n",
    "\n",
    "Let's see examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Auto (default) - LLM decides\n",
    "print(\"Example 1: function_choice='auto'\\n\")\n",
    "messages = [{'role': 'user', 'content': \"What's the weather in Tokyo?\"}]\n",
    "\n",
    "for responses in llm.chat(\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    stream=True,\n",
    "    extra_generate_cfg={'function_choice': 'auto'}\n",
    "):\n",
    "    pass\n",
    "\n",
    "print(f\"LLM decision: {'Call function' if responses[-1].get('function_call') else 'Direct answer'}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Force function call\n",
    "print(\"Example 2: Forcing function call\\n\")\n",
    "messages = [{'role': 'user', 'content': \"Tell me about Tokyo\"}]\n",
    "\n",
    "for responses in llm.chat(\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    stream=True,\n",
    "    extra_generate_cfg={'function_choice': 'get_current_weather'}\n",
    "):\n",
    "    pass\n",
    "\n",
    "if responses[-1].get('function_call'):\n",
    "    print(\"‚úÖ Function was forced to be called!\")\n",
    "    print(f\"   Arguments: {responses[-1]['function_call']['arguments']}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Disable function calls\n",
    "print(\"Example 3: function_choice='none' (disable functions)\\n\")\n",
    "messages = [{'role': 'user', 'content': \"What's the weather in Paris?\"}]\n",
    "\n",
    "for responses in llm.chat(\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    stream=True,\n",
    "    extra_generate_cfg={'function_choice': 'none'}\n",
    "):\n",
    "    pass\n",
    "\n",
    "print(f\"Has function_call: {responses[-1].get('function_call') is not None}\")\n",
    "print(f\"Direct answer: {responses[-1].get('content', '')[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Parallel Function Calls\n",
    "\n",
    "### Calling Multiple Functions at Once\n",
    "\n",
    "When a user asks about multiple things, the LLM can call multiple functions in parallel!\n",
    "\n",
    "Example: \"What's the weather in San Francisco, Tokyo, and Paris?\"\n",
    "\n",
    "The LLM can generate 3 function calls in one response!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable parallel function calls\n",
    "messages = [{\n",
    "    'role': 'user',\n",
    "    'content': \"What's the weather like in San Francisco, Tokyo, and Paris?\"\n",
    "}]\n",
    "\n",
    "print(\"User: What's the weather in San Francisco, Tokyo, and Paris?\\n\")\n",
    "print(\"Calling LLM with parallel_function_calls=True...\\n\")\n",
    "\n",
    "responses = []\n",
    "for responses in llm.chat(\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    stream=True,\n",
    "    extra_generate_cfg={'parallel_function_calls': True}\n",
    "):\n",
    "    pass\n",
    "\n",
    "# Check if we got multiple function calls\n",
    "fncall_msgs = [rsp for rsp in responses if rsp.get('function_call')]\n",
    "print(f\"Number of function calls: {len(fncall_msgs)}\\n\")\n",
    "\n",
    "for i, msg in enumerate(fncall_msgs, 1):\n",
    "    print(f\"Call {i}:\")\n",
    "    print(f\"  Function: {msg['function_call']['name']}\")\n",
    "    print(f\"  Arguments: {msg['function_call']['arguments']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing Parallel Function Calls\n",
    "\n",
    "When you get multiple function calls, execute them and add ALL results back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.extend(responses)  # Add all function calls to history\n",
    "\n",
    "if fncall_msgs:\n",
    "    print(\"Executing all function calls...\\n\")\n",
    "    \n",
    "    available_functions = {\n",
    "        'get_current_weather': get_current_weather,\n",
    "    }\n",
    "    \n",
    "    # Execute each function call\n",
    "    for msg in fncall_msgs:\n",
    "        function_name = msg['function_call']['name']\n",
    "        function_args = json.loads(msg['function_call']['arguments'])\n",
    "        \n",
    "        function_to_call = available_functions[function_name]\n",
    "        function_response = function_to_call(\n",
    "            location=function_args.get('location'),\n",
    "            unit=function_args.get('unit', 'fahrenheit'),\n",
    "        )\n",
    "        \n",
    "        print(f\"Result for {function_args['location']}: {function_response}\")\n",
    "        \n",
    "        # Add function result to messages (in same order as calls!)\n",
    "        messages.append({\n",
    "            'role': 'function',\n",
    "            'name': function_name,\n",
    "            'content': function_response,\n",
    "        })\n",
    "    \n",
    "    print(\"\\nCalling LLM again with all results...\\n\")\n",
    "    \n",
    "    # Get final synthesized answer\n",
    "    final_responses = []\n",
    "    for final_responses in llm.chat(\n",
    "        messages=messages,\n",
    "        functions=functions,\n",
    "        stream=True,\n",
    "        extra_generate_cfg={'parallel_function_calls': True}\n",
    "    ):\n",
    "        pass\n",
    "    \n",
    "    print(\"Final Answer:\")\n",
    "    print(final_responses[-1].get('content', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Error Handling\n",
    "\n",
    "### Common Errors in Function Calling\n",
    "\n",
    "1. **Malformed JSON** - LLM generates invalid JSON arguments\n",
    "2. **Unknown function** - LLM tries to call a function you didn't define\n",
    "3. **Missing required parameters** - LLM omits required arguments\n",
    "4. **Type errors** - LLM provides wrong type (string instead of number)\n",
    "\n",
    "Let's handle these gracefully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_execute_function(function_call_msg, available_functions):\n",
    "    \"\"\"\n",
    "    Safely execute a function call with error handling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        function_name = function_call_msg['function_call']['name']\n",
    "        \n",
    "        # Check if function exists\n",
    "        if function_name not in available_functions:\n",
    "            return json.dumps({\n",
    "                'error': f\"Function '{function_name}' not found\",\n",
    "                'available_functions': list(available_functions.keys())\n",
    "            })\n",
    "        \n",
    "        # Parse arguments\n",
    "        try:\n",
    "            function_args = json.loads(function_call_msg['function_call']['arguments'])\n",
    "        except json.JSONDecodeError as e:\n",
    "            return json.dumps({\n",
    "                'error': f\"Invalid JSON in arguments: {str(e)}\",\n",
    "                'arguments': function_call_msg['function_call']['arguments']\n",
    "            })\n",
    "        \n",
    "        # Execute function\n",
    "        function_to_call = available_functions[function_name]\n",
    "        result = function_to_call(**function_args)\n",
    "        return result\n",
    "        \n",
    "    except TypeError as e:\n",
    "        return json.dumps({\n",
    "            'error': f\"TypeError: {str(e)}\",\n",
    "            'hint': 'Check if all required parameters are provided with correct types'\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return json.dumps({\n",
    "            'error': f\"Unexpected error: {str(e)}\",\n",
    "            'type': type(e).__name__\n",
    "        })\n",
    "\n",
    "# Test error handling\n",
    "print(\"Testing error handling...\\n\")\n",
    "\n",
    "# Simulate a malformed function call\n",
    "test_msg = {\n",
    "    'function_call': {\n",
    "        'name': 'unknown_function',\n",
    "        'arguments': '{\"invalid\": json}'\n",
    "    }\n",
    "}\n",
    "\n",
    "result = safe_execute_function(test_msg, {'get_current_weather': get_current_weather})\n",
    "print(f\"Error handling result:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Multiple Functions Example\n",
    "\n",
    "### Giving the LLM Multiple Tools\n",
    "\n",
    "Let's add more functions for the LLM to choose from!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple functions\n",
    "def get_current_time(timezone='UTC'):\n",
    "    \"\"\"Get the current time in a timezone\"\"\"\n",
    "    from datetime import datetime\n",
    "    return json.dumps({'timezone': timezone, 'time': datetime.now().isoformat()})\n",
    "\n",
    "def calculate(expression):\n",
    "    \"\"\"Calculate a mathematical expression\"\"\"\n",
    "    try:\n",
    "        result = eval(expression)  # In production, use a safe math parser!\n",
    "        return json.dumps({'expression': expression, 'result': result})\n",
    "    except Exception as e:\n",
    "        return json.dumps({'error': str(e)})\n",
    "\n",
    "# Define function schemas\n",
    "multi_functions = [\n",
    "    {\n",
    "        'name': 'get_current_weather',\n",
    "        'description': 'Get the current weather in a given location',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'location': {'type': 'string', 'description': 'The city name'},\n",
    "                'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}\n",
    "            },\n",
    "            'required': ['location']\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'get_current_time',\n",
    "        'description': 'Get the current time in a specific timezone',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'timezone': {'type': 'string', 'description': 'Timezone like UTC, EST, PST'}\n",
    "            },\n",
    "            'required': []\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'calculate',\n",
    "        'description': 'Calculate a mathematical expression',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'expression': {'type': 'string', 'description': 'Math expression like \"2+2\" or \"sqrt(16)\"'}\n",
    "            },\n",
    "            'required': ['expression']\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "available_functions = {\n",
    "    'get_current_weather': get_current_weather,\n",
    "    'get_current_time': get_current_time,\n",
    "    'calculate': calculate\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Defined {len(multi_functions)} functions for the LLM!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a query that needs multiple functions\n",
    "messages = [{\n",
    "    'role': 'user',\n",
    "    'content': 'What time is it and what is 15 * 23?'\n",
    "}]\n",
    "\n",
    "print(\"User: What time is it and what is 15 * 23?\\n\")\n",
    "\n",
    "responses = []\n",
    "for responses in llm.chat(\n",
    "    messages=messages,\n",
    "    functions=multi_functions,\n",
    "    stream=True,\n",
    "    extra_generate_cfg={'parallel_function_calls': True}\n",
    "):\n",
    "    pass\n",
    "\n",
    "# Execute any function calls\n",
    "fncall_msgs = [rsp for rsp in responses if rsp.get('function_call')]\n",
    "if fncall_msgs:\n",
    "    print(f\"LLM requested {len(fncall_msgs)} function call(s):\\n\")\n",
    "    \n",
    "    messages.extend(responses)\n",
    "    \n",
    "    for msg in fncall_msgs:\n",
    "        result = safe_execute_function(msg, available_functions)\n",
    "        print(f\"Function: {msg['function_call']['name']}\")\n",
    "        print(f\"Result: {result}\\n\")\n",
    "        \n",
    "        messages.append({\n",
    "            'role': 'function',\n",
    "            'name': msg['function_call']['name'],\n",
    "            'content': result\n",
    "        })\n",
    "    \n",
    "    # Get final answer\n",
    "    for final_responses in llm.chat(messages=messages, functions=multi_functions, stream=True):\n",
    "        pass\n",
    "    \n",
    "    print(\"Final Answer:\")\n",
    "    print(final_responses[-1].get('content', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 9: Complete Function Calling Example\n",
    "\n",
    "### Building a Complete Chat Loop\n",
    "\n",
    "Let's build a complete function-calling chat loop that handles everything automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_calling_chat(user_query, functions, available_functions, max_turns=5):\n",
    "    \"\"\"\n",
    "    Complete function calling loop\n",
    "    \"\"\"\n",
    "    messages = [{'role': 'user', 'content': user_query}]\n",
    "    llm = get_chat_model(llm_cfg)\n",
    "    \n",
    "    for turn in range(max_turns):\n",
    "        print(f\"\\n--- Turn {turn + 1} ---\")\n",
    "        \n",
    "        # Call LLM\n",
    "        responses = []\n",
    "        for responses in llm.chat(\n",
    "            messages=messages,\n",
    "            functions=functions,\n",
    "            stream=True,\n",
    "            extra_generate_cfg={'parallel_function_calls': True}\n",
    "        ):\n",
    "            pass\n",
    "        \n",
    "        messages.extend(responses)\n",
    "        \n",
    "        # Check for function calls\n",
    "        fncall_msgs = [rsp for rsp in responses if rsp.get('function_call')]\n",
    "        \n",
    "        if not fncall_msgs:\n",
    "            # No function calls - we have the final answer\n",
    "            print(\"‚úÖ Got final answer\")\n",
    "            return responses[-1].get('content', '')\n",
    "        \n",
    "        # Execute function calls\n",
    "        print(f\"Executing {len(fncall_msgs)} function(s)...\")\n",
    "        for msg in fncall_msgs:\n",
    "            fn_name = msg['function_call']['name']\n",
    "            print(f\"  - {fn_name}\")\n",
    "            \n",
    "            result = safe_execute_function(msg, available_functions)\n",
    "            messages.append({\n",
    "                'role': 'function',\n",
    "                'name': fn_name,\n",
    "                'content': result\n",
    "            })\n",
    "    \n",
    "    return \"Max turns reached\"\n",
    "\n",
    "# Test it!\n",
    "answer = function_calling_chat(\n",
    "    \"What's the weather in Paris and what is 100 divided by 4?\",\n",
    "    multi_functions,\n",
    "    available_functions\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Final Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 10: Practice Exercises\n",
    "\n",
    "Now it's your turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Add a Translation Function\n",
    "\n",
    "Create a `translate_text` function that:\n",
    "- Takes `text` and `target_language` parameters\n",
    "- Returns a dummy translation\n",
    "- Define its schema\n",
    "- Test it with the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement translate_text function and schema\n",
    "# Hint: Follow the pattern of get_current_weather\n",
    "\n",
    "# Your code here:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Handle Parallel Weather Queries\n",
    "\n",
    "Modify the weather function to handle 5+ cities in parallel.\n",
    "Test with: \"Compare weather in New York, London, Tokyo, Sydney, and Mumbai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test parallel weather queries\n",
    "# Your code here:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Build a Unit Converter\n",
    "\n",
    "Create a function calling system with:\n",
    "- `convert_temperature(value, from_unit, to_unit)`\n",
    "- `convert_length(value, from_unit, to_unit)`\n",
    "- `convert_weight(value, from_unit, to_unit)`\n",
    "\n",
    "Test with: \"Convert 100 fahrenheit to celsius, 10 kilometers to miles, and 5 pounds to kilograms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement unit converter system\n",
    "# Your code here:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Error Recovery\n",
    "\n",
    "Create a function that sometimes fails, and implement error recovery:\n",
    "- If function returns error, add error to messages\n",
    "- Ask LLM to try again with different parameters\n",
    "- Limit retry attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement error recovery\n",
    "# Your code here:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: What You Learned Today\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "‚úÖ **Function calling** - LLMs generate structured function requests\n",
    "\n",
    "‚úÖ **Function schemas** - JSON Schema format for defining functions\n",
    "\n",
    "‚úÖ **Direct LLM calling** - Function calling without agents (using llm.chat())\n",
    "\n",
    "‚úÖ **fncall_prompt_type** - 'qwen' vs 'nous' formats\n",
    "\n",
    "‚úÖ **function_choice** - Control when functions are called ('auto', 'none', or function name)\n",
    "\n",
    "‚úÖ **Parallel function calls** - Multiple tools in one response\n",
    "\n",
    "‚úÖ **Error handling** - Gracefully handling malformed calls and execution errors\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **LLMs don't execute functions** - they generate requests for you to execute\n",
    "2. **Function descriptions matter** - they teach the LLM when to use each tool\n",
    "3. **Always validate** - Parse JSON carefully and handle errors\n",
    "4. **Parallel calling is powerful** - Enable it for better UX\n",
    "5. **The loop is important** - LLM ‚Üí Function ‚Üí LLM (repeat as needed)\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Tomorrow (Day 7)**: We'll learn **Custom Tools**!\n",
    "\n",
    "You'll learn:\n",
    "- Using @register_tool decorator\n",
    "- Parameter schema deep dive\n",
    "- Building reusable tools\n",
    "- Advanced tool patterns\n",
    "- Tool testing\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- üìñ Function Calling Example: `/examples/function_calling.py`\n",
    "- üîß Parallel Calls Example: `/examples/function_calling_in_parallel.py`\n",
    "- üí° OpenAI Function Calling Guide: https://platform.openai.com/docs/guides/function-calling\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! üéâ You now understand how LLMs use tools through function calling!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}