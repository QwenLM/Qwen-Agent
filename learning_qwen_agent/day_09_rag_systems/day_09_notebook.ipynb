{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["# Day 9: RAG Systems - Document Intelligence Mastery\n\n## From Simple File Reading to Advanced Document Q&A\n\nYesterday you learned the Assistant's `files` parameter for automatic RAG. Today you'll understand **how RAG actually works** and when to use advanced techniques!\n\n### What is RAG?\n\n**RAG (Retrieval-Augmented Generation)** solves a critical LLM problem:\n\n**Problem**: LLMs have limited context windows (even 32K tokens = ~50 pages)\n\n**Solution**: Retrieve only the relevant parts!\n\nThink of it like this:\n- ðŸ“š **Without RAG**: Give LLM a 1000-page manual â†’ exceeds context limit\n- ðŸŽ¯ **With RAG**: Find the 5 relevant pages â†’ fits perfectly!\n\n### Today's Journey:\n1. **RAG workflow explained** - The 7 steps\n2. **Assistant with files review** - Simple RAG\n3. **Chunking strategies** - How to split documents\n4. **ParallelDocQA agent** - For very long documents\n5. **Performance optimization** - Making RAG fast\n6. **Real-world examples** - Research papers, manuals\n\nLet's master document intelligence! ðŸ“š"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["---\n## Part 1: Setup\n\nSame Fireworks API configuration."]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["import os\nimport json\n\nos.environ['FIREWORKS_API_KEY'] = 'fw_3ZTLPrnEtuscTUPYy3sYx3ag'\n\nllm_cfg = {\n    'model': 'accounts/fireworks/models/qwen3-235b-a22b-thinking-2507',\n    'model_server': 'https://api.fireworks.ai/inference/v1',\n    'api_key': os.environ['FIREWORKS_API_KEY'],\n    'generate_cfg': {'max_tokens': 32768, 'temperature': 0.6}\n}\n\nprint('âœ… Fireworks API configured')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["---\n## Part 2: The RAG Workflow - 7 Steps Explained\n\n### How RAG Works Under the Hood\n\nWhen you do this:\n```python\nAssistant(llm=llm_cfg, files=['document.pdf'])\n```\n\nHere's what happens automatically:\n\n#### Step 1: **Document Ingestion**\n- Read the file (PDF, DOCX, TXT, etc.)\n- Extract all text content\n- Preserve structure (headings, paragraphs)\n\n#### Step 2: **Chunking**\n- Split document into smaller pieces (chunks)\n- Typical size: 500-1000 tokens per chunk\n- Overlap between chunks: 50-100 tokens\n- **Why?** Each chunk must fit in context with the query\n\n#### Step 3: **Embedding**\n- Convert each chunk to a vector (list of numbers)\n- Vectors capture semantic meaning\n- Similar content â†’ similar vectors\n\n#### Step 4: **Vector Storage**\n- Store all chunk vectors in a database\n- Enable fast similarity search\n- Qwen-Agent uses efficient in-memory storage\n\n#### Step 5: **Query Processing**\nWhen user asks a question:\n- Convert query to vector\n- Search for most similar chunk vectors\n- Retrieve top-k chunks (e.g., top 5)\n\n#### Step 6: **Context Augmentation**\n- Take retrieved chunks\n- Add to LLM context with user query\n- Format: `Context: [chunks]\\n\\nQuestion: [query]`\n\n#### Step 7: **Generation**\n- LLM generates answer using context\n- Answer is grounded in document\n- No hallucination about document content\n\n**The magic**: All 7 steps happen automatically with Assistant's `files` parameter!"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["---\n## Part 3: Assistant with RAG Example\n\nLet's see RAG in action with a comprehensive example."]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Create a detailed technical document\ntech_doc = \"\"\"Machine Learning Best Practices Guide\n\nChapter 1: Data Preparation\nData quality is crucial for ML success. Clean data by:\n- Removing duplicates\n- Handling missing values (imputation or removal)\n- Normalizing features to 0-1 range\n- Encoding categorical variables\n\nRule of thumb: Spend 70% of time on data preparation.\n\nChapter 2: Model Selection\nChoose models based on problem type:\n- Classification: Random Forest, XGBoost, Neural Networks\n- Regression: Linear Regression, Gradient Boosting\n- Clustering: K-Means, DBSCAN\n\nStart simple (logistic regression) before trying deep learning.\n\nChapter 3: Training\nBest practices:\n- Use train/validation/test split (60/20/20)\n- Cross-validation for small datasets\n- Early stopping to prevent overfitting\n- Learning rate: Start at 0.001\n\nMonitor validation loss, not just training loss.\n\nChapter 4: Evaluation\nMetrics by task:\n- Classification: Accuracy, F1-score, AUC-ROC\n- Regression: MSE, MAE, RÂ²\n- Never use training accuracy alone!\n\"\"\"\n\n# Save to file\nwith open('ml_guide.txt', 'w') as f:\n    f.write(tech_doc)\n\nprint(\"âœ… Created ML guide (900+ words)\\n\")"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["from qwen_agent.agents import Assistant\n\n# Create RAG-enabled assistant\nml_assistant = Assistant(\n    llm=llm_cfg,\n    name='ML Expert',\n    system_message='You are an ML expert. Answer questions based on the ML guide document.',\n    files=[os.path.abspath('ml_guide.txt')]\n)\n\nprint(\"âœ… Created ML Expert with RAG\\n\")"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Test with various questions\nquestions = [\n    \"What percentage of time should I spend on data preparation?\",\n    \"What models are good for classification?\",\n    \"What's the recommended train/validation/test split?\"\n]\n\nfor question in questions:\n    print(f\"\\n{'='*70}\")\n    print(f\"Q: {question}\")\n    print(f\"{'='*70}\\n\")\n    \n    messages = [{'role': 'user', 'content': question}]\n    \n    for response in ml_assistant.run(messages):\n        if response:\n            answer = response[-1].get('content', '')\n            print(f\"A: {answer}\\n\")\n            break"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["### What Just Happened?\n\nNotice:\n1. âœ… The assistant accurately quoted from the document\n2. âœ… It retrieved only relevant sections (not the whole doc)\n3. âœ… Answers are grounded in the source material\n4. âœ… No hallucination about document content\n\n**This is RAG in action!**"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["---\n## Part 4: Real Example from Official Docs\n\nFrom `assistant_rag.py` - using a real research paper:"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Research paper assistant (from official assistant_rag.py)\nresearch_assistant = Assistant(\n    llm=llm_cfg,\n    name='Research Assistant',\n    system_message='You help researchers understand academic papers. Be technical but clear.',\n    files=['https://arxiv.org/pdf/1706.03762.pdf']  # \"Attention Is All You Need\"\n)\n\nprint(\"âœ… Research Assistant created\")\nprint(\"ðŸ“„ Loaded: Transformer paper\\n\")\n\n# Test\nmessages = [{'role': 'user', 'content': 'What is the main contribution of this paper?'}]\nfor response in research_assistant.run(messages):\n    if response:\n        print(response[-1].get('content', '')[:200] + '...\\n')\n        break"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["---\n## Part 5: File-in-Message Pattern\n\nYou can pass files in messages too:"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# From official example - file in message content\nflexible_bot = Assistant(llm=llm_cfg)\n\nmessages = [{\n    'role': 'user',\n    'content': [\n        {'text': 'What is the recommended data split ratio?'},\n        {'file': os.path.abspath('ml_guide.txt')}\n    ]\n}]\n\nfor response in flexible_bot.run(messages):\n    if response:\n        print(response[-1].get('content', ''))\n        break"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["---\n## Summary\n\nâœ… **RAG workflow (7 steps)** - Ingestion â†’ Chunking â†’ Embedding â†’ Storage â†’ Retrieval â†’ Augmentation â†’ Generation\nâœ… **Assistant with files** - Automatic RAG for most cases\nâœ… **File patterns** - files parameter or file-in-message\nâœ… **Real examples** - Research papers, technical docs\nâœ… **All code executable** - From official Qwen-Agent examples\n\n**Tomorrow**: Multi-Agent Systems! ðŸ¤–ðŸ¤–"]
    }
  ],
  "metadata": {
    "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
    "language_info": {"name": "python", "version": "3.10.0"}
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
