{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["# Day 11: Advanced Patterns - Reasoning and Thinking Models\n\n## Unlocking Advanced Capabilities\n\nToday you'll learn about advanced agent patterns including reasoning models!\n\n### What's Special About Reasoning Models?\n\n**Regular LLMs**: Think â†’ Answer (fast but less accurate)\n\n**Reasoning models** (QwQ, Qwen3 with thinking): Think â†’ Reason â†’ Verify â†’ Answer (slower but more accurate)\n\n**Example:**\n```\nRegular: \"What's 15 factorial?\"\nâ†’ \"1307674368000\" (guessed)\n\nReasoning: \"What's 15 factorial?\"\nâ†’ <think>I need to calculate 15! step by step...\n   15 * 14 = 210\n   210 * 13 = 2730...\n   Final: 1307674368000</think>\nâ†’ \"The answer is 1,307,674,368,000\" (verified)\n```\n\n### Today's Topics:\n1. **QwQ-32B reasoning model** - Deep thinking for complex tasks\n2. **enable_thinking parameter** - Control thinking mode\n3. **thought_in_content** - Handling thinking in different APIs\n4. **fncall_prompt_type** - Tool calling templates\n5. **Real examples** - From official assistant_qwq.py and assistant_qwen3.py\n\nLet's explore advanced patterns! ðŸ§ "]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["---\n## Part 1: Setup"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["import os\n\nos.environ['FIREWORKS_API_KEY'] = 'fw_3ZTLPrnEtuscTUPYy3sYx3ag'\n\nllm_cfg = {\n    'model': 'accounts/fireworks/models/qwen3-235b-a22b-thinking-2507',\n    'model_server': 'https://api.fireworks.ai/inference/v1',\n    'api_key': os.environ['FIREWORKS_API_KEY'],\n    'generate_cfg': {'max_tokens': 32768}\n}\n\nprint('âœ… Configured')"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["---\n## Part 2: QwQ-32B Reasoning Model\n\n### What is QwQ?\n\n**QwQ-32B** is a reasoning model that shows its thinking process.\n\nFrom official `assistant_qwq.py`:\n\n```python\nllm_cfg = {\n    'model': 'qwq-32b',\n    'model_type': 'qwen_dashscope',\n    'generate_cfg': {\n        'fncall_prompt_type': 'nous',  # Recommended for QwQ\n    }\n}\n```\n\n### Key Configuration for Reasoning Models\n\n1. **fncall_prompt_type: 'nous'**\n   - Better for Qwen2.5+ and QwQ\n   - Supports parallel function calls\n   - Default in newer versions\n\n2. **thought_in_content**\n   - Use when old vLLM doesn't support `reasoning_content` field\n   - When thinking is mixed with answer: `<think>...</think>answer`\n   - Don't use when thinking is separate field"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["from qwen_agent.agents import Assistant\n\n# Our Fireworks model supports reasoning\n# It's Qwen3-235B-A22B-Thinking-2507 which has thinking capability\n\nreasoning_bot = Assistant(\n    llm=llm_cfg,\n    name='Reasoning Assistant',\n    system_message='You solve problems step-by-step. Show your reasoning.'\n)\n\nprint(\"âœ… Created reasoning assistant\")"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Test with a problem that benefits from reasoning\nmessages = [{\n    'role': 'user',\n    'content': 'If Alice has twice as many apples as Bob, and Bob has 3 more apples than Charlie, and Charlie has 5 apples, how many apples does Alice have?'\n}]\n\nprint(\"Question: Complex word problem\\n\")\nfor response in reasoning_bot.run(messages):\n    if response:\n        answer = response[-1].get('content', '')\n        print(f\"Answer: {answer}\\n\")\n        break"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["---\n## Part 3: enable_thinking Parameter\n\n### Different API Approaches\n\nFrom official `assistant_qwen3.py`, there are 3 ways to configure thinking:\n\n#### Approach 1: DashScope Native API\n```python\nllm_cfg = {\n    'model': 'qwen3-235b-a22b',\n    'model_type': 'qwen_dashscope',\n    'generate_cfg': {\n        'enable_thinking': True,  # Enable thinking mode\n    }\n}\n```\n\n#### Approach 2: DashScope OpenAI-Compatible API\n```python\nllm_cfg = {\n    'model': 'qwen3-235b-a22b',\n    'model_server': 'https://dashscope.aliyuncs.com/compatible-mode/v1',\n    'generate_cfg': {\n        'extra_body': {\n            'enable_thinking': False  # Control via extra_body\n        }\n    }\n}\n```\n\n#### Approach 3: vLLM/SGLang Self-Hosted\n```python\nllm_cfg = {\n    'model': 'Qwen/Qwen3-32B',\n    'model_server': 'http://localhost:8000/v1',\n    'generate_cfg': {\n        'extra_body': {\n            'chat_template_kwargs': {'enable_thinking': False}\n        }\n    }\n}\n```"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Example: Using our Fireworks Qwen3 thinking model\n# It already has thinking enabled by default in the model name\n\nprint(\"Our model configuration:\")\nprint(f\"Model: {llm_cfg['model']}\")\nprint(\"\\nThis model has built-in thinking capability!\")\nprint(\"The '2507' in the name indicates it's a thinking-enhanced version.\")\nprint(\"\\nNo need to explicitly enable thinking - it's already optimized for reasoning tasks.\")"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["---\n## Part 4: thought_in_content Parameter\n\n### When to Use It?\n\nFrom `assistant_qwq.py` comments:\n\n```python\n'generate_cfg': {\n    # This parameter needs to be passed when:\n    # 1. Using reasoning model (e.g. qwq-32b)\n    # 2. Deployed with old vLLM that doesn't support reasoning_content field\n    # 3. Content format is: `<think>thought</think>answer`\n    \n    # 'thought_in_content': True,  # Uncomment if needed\n}\n```\n\n### Two Formats:\n\n**Format 1: Separate fields** (modern)\n```json\n{\n  \"reasoning_content\": \"Let me think...\",\n  \"content\": \"The answer is 42\"\n}\n```\nâ†’ Don't use `thought_in_content`\n\n**Format 2: Mixed in content** (old vLLM)\n```json\n{\n  \"content\": \"<think>Let me think...</think>The answer is 42\"\n}\n```\nâ†’ Use `thought_in_content: True`"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["---\n## Part 5: fncall_prompt_type - Tool Calling Templates\n\n### Available Templates\n\n1. **'nous'** (recommended for Qwen2.5+, QwQ)\n   - Better tool calling\n   - Parallel function calls\n   - Default in newer versions\n\n2. **'qwen'** (legacy)\n   - Original Qwen format\n   - Still works but 'nous' is better\n\nFrom official examples:\n```python\n# QwQ and Qwen3 examples use:\n'generate_cfg': {\n    'fncall_prompt_type': 'nous'\n}\n```"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Example: Assistant with tools using 'nous' template\ntool_bot = Assistant(\n    llm=llm_cfg,\n    function_list=['code_interpreter'],\n    system_message='You are a helpful coding assistant.'\n)\n\nprint(\"âœ… Created assistant with code_interpreter\")\nprint(\"   Using default fncall_prompt_type (optimized for Qwen3)\\n\")\n\n# Test it\nmessages = [{'role': 'user', 'content': 'Calculate the sum of squares from 1 to 10'}]\n\nfor response in tool_bot.run(messages):\n    for msg in response:\n        if msg.get('function_call'):\n            print(f\"ðŸ”§ Calling: {msg['function_call']['name']}\")\n        elif msg.get('content'):\n            print(f\"Result: {msg['content'][:150]}\\n\")\n            break"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["---\n## Part 6: Real Example from Official Code\n\n### QwQ Image Generation Demo\n\nFrom `assistant_qwq.py` - complete working example:"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Adapted from assistant_qwq.py\nimport urllib.parse\nimport json\nfrom qwen_agent.tools.base import BaseTool, register_tool\n\n# Register image generation tool (from official example)\n@register_tool('image_gen')\nclass ImageGen(BaseTool):\n    description = 'AI painting (image generation) service'\n    parameters = [{\n        'name': 'prompt',\n        'type': 'string',\n        'description': 'Image description in English',\n        'required': True\n    }]\n    \n    def call(self, params, **kwargs):\n        prompt = json.loads(params)['prompt']\n        prompt = urllib.parse.quote(prompt)\n        return json.dumps({'image_url': f'https://image.pollinations.ai/prompt/{prompt}'})\n\nprint(\"âœ… Registered image_gen tool\")"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Create QwQ-style assistant (adapted for Fireworks)\nqwq_bot = Assistant(\n    llm=llm_cfg,\n    function_list=['image_gen'],\n    name='Reasoning Image Generator',\n    description='I think through image generation requests carefully'\n)\n\nprint(\"âœ… Created reasoning image generator\\n\")\n\n# Test (from official example pattern)\nmessages = [{'role': 'user', 'content': 'Draw a cat and a dog playing together'}]\n\nfor response in qwq_bot.run(messages):\n    for msg in response:\n        if msg.get('function_call'):\n            print(f\"ðŸŽ¨ Generating image with: {msg['function_call']['name']}\")\n            print(f\"   Prompt: {msg['function_call']['arguments'][:100]}...\\n\")\n        elif msg.get('content'):\n            print(f\"Response: {msg['content'][:200]}\\n\")\n            break"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["---\n## Part 7: Qwen3 with MCP Tools\n\n### MCP Integration Example\n\nFrom `assistant_qwen3.py`:"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Example MCP configuration from assistant_qwen3.py\nmcp_config = {\n    'mcpServers': {\n        'time': {\n            'command': 'uvx',\n            'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n        },\n        'fetch': {\n            'command': 'uvx',\n            'args': ['mcp-server-fetch']\n        }\n    }\n}\n\nprint(\"MCP Tools Configuration (from official example):\")\nprint(json.dumps(mcp_config, indent=2))\nprint(\"\\nâœ… This shows how to integrate external MCP servers\")\nprint(\"   (Requires Node.js/Python MCP servers installed)\")\n\n# With MCP, you could create:\n# bot = Assistant(\n#     llm=llm_cfg,\n#     function_list=[mcp_config, 'code_interpreter']\n# )"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["---\n## Part 8: Comparison Table\n\n### When to Use What?\n\n| Feature | Regular Assistant | Reasoning Assistant |\n|---------|------------------|--------------------|\n| **Speed** | Fast | Slower |\n| **Accuracy** | Good | Excellent |\n| **Best for** | Simple tasks | Complex reasoning |\n| **Cost** | Lower | Higher |\n| **Thinking shown** | No | Yes (optional) |\n| **Use when** | Speed matters | Accuracy critical |\n\n### Configuration Quick Reference\n\n```python\n# For most tasks (fast)\nllm_cfg = {'model': 'qwen3-32b'}\n\n# For reasoning tasks (accurate)\nllm_cfg = {\n    'model': 'qwq-32b',\n    'generate_cfg': {'fncall_prompt_type': 'nous'}\n}\n\n# Enable thinking (DashScope)\nllm_cfg = {\n    'model': 'qwen3-235b',\n    'generate_cfg': {'enable_thinking': True}\n}\n```"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["---\n## Summary\n\nâœ… **QwQ-32B** - Reasoning model for complex tasks\nâœ… **enable_thinking** - Control thinking mode (3 API approaches)\nâœ… **thought_in_content** - For old vLLM deployments\nâœ… **fncall_prompt_type** - 'nous' recommended for Qwen2.5+/QwQ\nâœ… **All from official examples** - assistant_qwq.py & assistant_qwen3.py\nâœ… **MCP integration** - External tool servers\n\n### Key Takeaways:\n\n1. **Reasoning models think step-by-step** - Better for complex tasks\n2. **enable_thinking** - Different for each API (DashScope, OAI, vLLM)\n3. **fncall_prompt_type: 'nous'** - Use for modern models\n4. **thought_in_content** - Only for old vLLM\n5. **MCP tools** - Extend with external servers\n\n### From Official Examples:\n- `assistant_qwq.py` - QwQ reasoning model\n- `assistant_qwen3.py` - Qwen3 with MCP and thinking\n- All configurations are production-tested!\n\n**Tomorrow**: GUI Development with WebUI! ðŸŽ¨"]
    }
  ],
  "metadata": {
    "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
    "language_info": {"name": "python", "version": "3.10.0"}
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
